@online{12066471Causal,
  title = {[1206.6471] {{On Causal}} and {{Anticausal Learning}}},
  url = {https://arxiv.org/abs/1206.6471},
  urldate = {2019-07-01},
  file = {C:\Users\USEBPERP\Zotero\storage\H7WP45NW\1206.html}
}

@online{13126114AutoEncoding,
  title = {[1312.6114] {{Auto-Encoding Variational Bayes}}},
  url = {https://arxiv.org/abs/1312.6114},
  urldate = {2020-01-14},
  file = {C:\Users\USEBPERP\Zotero\storage\WGD8Z76L\1312.html}
}

@online{161207837SampleRNN,
  title = {[1612.07837] {{SampleRNN}}: {{An Unconditional End-to-End Neural Audio Generation Model}}},
  url = {https://arxiv.org/abs/1612.07837},
  urldate = {2019-10-21},
  file = {C:\Users\USEBPERP\Zotero\storage\F98UQJ56\1612.html}
}

@online{16AnomalyDetection,
  title = {(16) {{Anomaly Detection}} – {{Real-Time Anomaly Detection}} in {{Time Series Data}} | {{LinkedIn}}},
  url = {https://www.linkedin.com/pulse/anomaly-detection-real-time-time-series-data-saba-el-hilo/},
  urldate = {2023-02-21},
  file = {C:\Users\USEBPERP\Zotero\storage\65DYPVC9\anomaly-detection-real-time-time-series-data-saba-el-hilo.html}
}

@online{180308533Understanding,
  title = {[1803.08533] {{Understanding Measures}} of {{Uncertainty}} for {{Adversarial Example Detection}}},
  url = {https://arxiv.org/abs/1803.08533},
  urldate = {2019-03-11},
  file = {C:\Users\USEBPERP\Zotero\storage\23ZCPCEY\1803.html}
}

@online{180407612Revisiting,
  title = {[1804.07612] {{Revisiting Small Batch Training}} for {{Deep Neural Networks}}},
  url = {https://arxiv.org/abs/1804.07612},
  urldate = {2019-08-28},
  file = {C:\Users\USEBPERP\Zotero\storage\G6KPZ9W5\1804.html}
}

@online{180705520Deep,
  title = {[1807.05520] {{Deep Clustering}} for {{Unsupervised Learning}} of {{Visual Features}}},
  url = {https://arxiv.org/abs/1807.05520},
  urldate = {2019-05-03},
  file = {C:\Users\USEBPERP\Zotero\storage\5IU92CH7\1807.html}
}

@online{181206162Empirical,
  title = {[1812.06162] {{An Empirical Model}} of {{Large-Batch Training}}},
  url = {https://arxiv.org/abs/1812.06162},
  urldate = {2019-11-07}
}

@online{190602530Can,
  title = {[1906.02530] {{Can You Trust Your Model}}'s {{Uncertainty}}? {{Evaluating Predictive Uncertainty Under Dataset Shift}}},
  url = {https://arxiv.org/abs/1906.02530},
  urldate = {2019-06-13},
  file = {C:\Users\USEBPERP\Zotero\storage\QCPARX5H\1906.html}
}

@online{190904656v1Video,
  title = {[1909.04656v1] {{Video Representation Learning}} by {{Dense Predictive Coding}}},
  url = {https://arxiv.org/abs/1909.04656v1},
  urldate = {2019-09-12},
  file = {C:\Users\USEBPERP\Zotero\storage\3P7BEUZY\1909.html}
}

@online{200607038Learning,
  title = {[2006.07038] {{Learning Graph Models}} for {{Template-Free Retrosynthesis}}},
  url = {https://arxiv.org/abs/2006.07038},
  urldate = {2020-06-22},
  file = {C:\Users\USEBPERP\Zotero\storage\49CC86NY\2006.html}
}

@online{30CameraReadySubmissionStarter_draft,
  title = {30\textbackslash{{CameraReadySubmission}}\textbackslash starter\_draft (2).Pdf},
  url = {https://drive.google.com/file/d/1sw50rCqBWMC4QTOWFWZ5xMN75NTJWpQD/view?usp=sharing&usp=embed_facebook},
  urldate = {2020-02-11},
  organization = {Google Docs},
  file = {C:\Users\USEBPERP\Zotero\storage\XGKCBW3P\view.html}
}

@article{abadiDeepLearningDifferential2016,
  title = {Deep {{Learning}} with {{Differential Privacy}}},
  author = {Abadi, Martín and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  date = {2016-10-24},
  journaltitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
  eprint = {1607.00133},
  eprinttype = {arxiv},
  pages = {308--318},
  doi = {10.1145/2976749.2978318},
  url = {http://arxiv.org/abs/1607.00133},
  urldate = {2020-07-13},
  abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{aboodeAnomalyDetectionTime,
  title = {Anomaly {{Detection}} in {{Time Series Data Based}} on {{Holt-Winters Method}}},
  author = {Aboode, Adam},
  abstract = {In today’s world the amount of collected data increases every day, this is a trend which is likely to continue. At the same time the potential value of the data does also increase due to the constant development and improvement of hardware and software. However, in order to gain insights, make decisions or train accurate machine learning models we want to ensure that the data we collect is of good quality. There are many definitions of data quality, in this thesis we focus on the accuracy aspect.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\RV85KYDY\Aboode - Anomaly Detection in Time Series Data Based on Hol.pdf}
}

@online{AccessingNextcloudFiles,
  title = {Accessing {{Nextcloud}} Files Using {{WebDAV}} — {{Nextcloud}} Latest {{User Manual}} Latest Documentation},
  url = {https://docs.nextcloud.com/server/17/user_manual/files/access_webdav.html},
  urldate = {2020-01-14},
  file = {C:\Users\USEBPERP\Zotero\storage\FYTXYQSQ\access_webdav.html}
}

@online{ActiveLearningStrategies,
  title = {Active {{Learning Strategies}} for {{Phenotypic Profiling}} of {{High-Content Screens}} - {{Kevin Smith}}, {{Peter Horvath}}, 2014},
  url = {https://journals.sagepub.com/doi/full/10.1177/1087057114527313?url_ver=Z39.88-2003&rfr_id=ori%3Arid%3Acrossref.org&rfr_dat=cr_pub%3Dpubmed},
  urldate = {2019-07-16},
  file = {C:\Users\USEBPERP\Zotero\storage\9DFWQ83X\1087057114527313.html}
}

@online{ActivityrelevantSimilarityValues,
  title = {Activity-Relevant Similarity Values for Fingerprints and Implications for Similarity Searching - {{PMC}}},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830209/},
  urldate = {2023-09-04},
  file = {C:\Users\USEBPERP\Zotero\storage\DCXDRRIN\PMC4830209.html}
}

@online{ActivityrelevantSimilarityValuesa,
  title = {Activity-Relevant Similarity Values for Fingerprints and Implications for Similarity Searching - {{PubMed}}},
  url = {https://pubmed.ncbi.nlm.nih.gov/27127620/},
  urldate = {2023-09-04},
  file = {C:\Users\USEBPERP\Zotero\storage\DVNFSL57\27127620.html}
}

@online{adamsBayesianOnlineChangepoint2007,
  title = {Bayesian {{Online Changepoint Detection}}},
  author = {Adams, Ryan Prescott and MacKay, David J. C.},
  date = {2007-10-19},
  eprint = {0710.3742},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.0710.3742},
  url = {http://arxiv.org/abs/0710.3742},
  urldate = {2023-01-27},
  abstract = {Changepoints are abrupt variations in the generative parameters of a data sequence. Online detection of changepoints is useful in modelling and prediction of time series in application areas such as finance, biometrics, and robotics. While frequentist methods have yielded online filtering and prediction techniques, most Bayesian papers have focused on the retrospective segmentation problem. Here we examine the case where the model parameters before and after the changepoint are independent and we derive an online algorithm for exact inference of the most recent changepoint. We compute the probability distribution of the length of the current ``run,'' or time since the last changepoint, using a simple message-passing algorithm. Our implementation is highly modular so that the algorithm may be applied to a variety of types of data. We illustrate this modularity by demonstrating the algorithm on three different real-world data sets.},
  pubstate = {preprint},
  keywords = {Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\GZP9J6XX\\Adams and MacKay - 2007 - Bayesian Online Changepoint Detection.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BIES7IXW\\0710.html}
}

@article{adamsonMachineLearningHealth2018,
  title = {Machine {{Learning}} and {{Health Care Disparities}} in {{Dermatology}}},
  author = {Adamson, Adewole S. and Smith, Avery},
  date = {2018-11-01},
  journaltitle = {JAMA Dermatol},
  volume = {154},
  number = {11},
  pages = {1247--1248},
  publisher = {American Medical Association},
  issn = {2168-6068},
  doi = {10.1001/jamadermatol.2018.2348},
  url = {https://jamanetwork.com/journals/jamadermatology/fullarticle/2688587},
  urldate = {2020-06-23},
  abstract = {This viewpoint discusses the limitations of machine learning in diagnostics involving skin of color.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\WDMBF8F9\2688587.html}
}

@online{ahmadRealTimeAnomalyDetection2016,
  title = {Real-{{Time Anomaly Detection}} for {{Streaming Analytics}}},
  author = {Ahmad, Subutai and Purdy, Scott},
  date = {2016-07-08},
  eprint = {1607.02480},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1607.02480},
  urldate = {2023-01-27},
  abstract = {Much of the worlds data is streaming, time-series data, where anomalies give significant information in critical situations. Yet detecting anomalies in streaming data is a difficult task, requiring detectors to process data in real-time, and learn while simultaneously making predictions. We present a novel anomaly detection technique based on an on-line sequence memory algorithm called Hierarchical Temporal Memory (HTM). We show results from a live application that detects anomalies in financial metrics in real-time. We also test the algorithm on NAB, a published benchmark for real-time anomaly detection, where our algorithm achieves best-in-class results.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Electrical Engineering and Systems Science - Systems and Control},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\X4UPG9RT\\Ahmad and Purdy - 2016 - Real-Time Anomaly Detection for Streaming Analytic.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DBM3NKGQ\\1607.html}
}

@inproceedings{ahnLearningWhatDefer2020,
  title = {Learning {{What}} to {{Defer}} for {{Maximum Independent Sets}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Ahn, Sungsoo and Seo, Younggyo and Shin, Jinwoo},
  date = {2020-11-21},
  pages = {134--144},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/ahn20a.html},
  urldate = {2022-05-25},
  abstract = {Designing efficient algorithms for combinatorial optimization appears ubiquitously in various scientific fields. Recently, deep reinforcement learning (DRL) frameworks have gained considerable attention as a new approach: they can automate the design of a solver while relying less on sophisticated domain knowledge of the target problem. However, the existing DRL solvers determine the solution using a number of stages proportional to the number of elements in the solution, which severely limits their applicability to large-scale graphs. In this paper, we seek to resolve this issue by proposing a novel DRL scheme, coined learning what to defer (LwD), where the agent adaptively shrinks or stretch the number of stages by learning to distribute the element-wise decisions of the solution at each stage. We apply the proposed framework to the maximum independent set (MIS) problem, and demonstrate its significant improvement over the current state-of-the-art DRL scheme. We also show that LwD can outperform the conventional MIS solvers on large-scale graphs having millions of vertices, under a limited time budget.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BP685JN3\\Ahn et al. - 2020 - Learning What to Defer for Maximum Independent Set.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VQYJIT75\\Ahn et al. - 2020 - Learning What to Defer for Maximum Independent Set.pdf}
}

@article{aiAITransformationPlaybook,
  title = {{{AI Transformation Playbook}}},
  author = {Ai, Landing},
  pages = {12},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\MGDYF2S8\Ai_AI Transformation Playbook.pdf}
}

@unpublished{al-rfouCharacterLevelLanguageModeling2018,
  title = {Character-{{Level Language Modeling}} with {{Deeper Self-Attention}}},
  author = {Al-Rfou, Rami and Choe, Dokook and Constant, Noah and Guo, Mandy and Jones, Llion},
  date = {2018-08-09},
  eprint = {1808.04444},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1808.04444},
  urldate = {2019-10-21},
  abstract = {LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\9LUHSM7N\\Al-Rfou et al_2018_Character-Level Language Modeling with Deeper Self-Attention.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2WU4RH9D\\1808.html}
}

@article{alexandrovGluonTSProbabilisticNeural2020,
  title = {{{GluonTS}}: {{Probabilistic}} and {{Neural Time Series Modeling}} in {{Python}}},
  shorttitle = {{{GluonTS}}},
  author = {Alexandrov, Alexander and Benidis, Konstantinos and Bohlke-Schneider, Michael and Flunkert, Valentin and Gasthaus, Jan and Januschowski, Tim and Maddix, Danielle C. and Rangapuram, Syama and Salinas, David and Schulz, Jasper and Stella, Lorenzo and Türkmen, Ali Caner and Wang, Yuyang},
  date = {2020},
  journaltitle = {Journal of Machine Learning Research},
  volume = {21},
  number = {116},
  pages = {1--6},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v21/19-820.html},
  urldate = {2023-01-30},
  abstract = {We introduce the Gluon Time Series Toolkit (GluonTS), a Python library for deep learning based time series modeling for ubiquitous tasks, such as forecasting and anomaly detection. GluonTS simplifies the time series modeling pipeline by providing the necessary components and tools for quick model development, efficient experimentation and evaluation. In addition, it contains reference implementations of state-of-the-art time series models that enable simple benchmarking of new algorithms.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DS2ET668\\Alexandrov et al. - 2020 - GluonTS Probabilistic and Neural Time Series Mode.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8YJXVHXK\\gluonts.html}
}

@article{allisProofnumberSearch1994,
  title = {Proof-Number Search},
  author = {Allis, L. Victor and family=Meulen, given=Maarten, prefix=van der, useprefix=true and family=Herik, given=H. Jaap, prefix=van den, useprefix=true},
  date = {1994-03-01},
  journaltitle = {Artificial Intelligence},
  volume = {66},
  number = {1},
  pages = {91--124},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(94)90004-3},
  url = {http://www.sciencedirect.com/science/article/pii/0004370294900043},
  urldate = {2020-09-10},
  abstract = {Proof-number search (pn-search) is designed for finding the game-theoretical value in game trees. It is based on ideas derived from conspiracy-number search and its variants, such as applied cn-search and αβ-cn search. While in cn-search the purpose is to continue searching until it is unlikely that the minimax value of the root will change, pn-search aims at proving the true value of the root. Therefore, pn-search does not consider interim minimax values. Pn-search selects the next node to be expanded using two criteria: the potential range of subtree values and the number of nodes which must conspire to prove or disprove that range of potential values. These two criteria enable pn-search to treat efficiently game trees with a non-uniform branching factor. It is shown that in non-uniform trees pn-search outperforms other types of search, such as α-β iterative-deepening search, even when enhanced with transposition tables, move ordering for the full principal variation, etc. Pn-search has been used to establish the game-theoretical values of Connect-Four, Qubic, and Go-Moku. There pn-search was able to find a forced win for the player to move first. The experiments described here are in the domain of Awari, a game which has not yet been solved. The experiments are repeatable for other games with a non-uniform branching factor. This article describes the underlying principles of pn-search, presents an appropriate implementation, and provides an analysis of its strengths and weaknesses.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\IDTBXSKJ\0004370294900043.html}
}

@unpublished{alpersteinAllSMILESVariational2019,
  title = {All {{SMILES Variational Autoencoder}}},
  author = {Alperstein, Zaccary and Cherkasov, Artem and Rolfe, Jason Tyler},
  date = {2019-05-30},
  eprint = {1905.13343},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.13343},
  urldate = {2019-06-05},
  abstract = {Variational autoencoders (VAEs) defined over SMILES string and graph-based representations of molecules promise to improve the optimization of molecular properties, thereby revolutionizing the pharmaceuticals and materials industries. However, these VAEs are hindered by the non-unique nature of SMILES strings and the computational cost of graph convolutions. To efficiently pass messages along all paths through the molecular graph, we encode multiple SMILES strings of a single molecule using a set of stacked recurrent neural networks, pooling hidden representations of each atom between SMILES representations, and use attentional pooling to build a final fixed-length latent representation. By then decoding to a disjoint set of SMILES strings of the molecule, our All SMILES VAE learns an almost bijective mapping between molecules and latent representations near the high-probability-mass subspace of the prior. Our SMILES-derived but molecule-based latent representations significantly surpass the state-of-the-art in a variety of fully- and semi-supervised property regression and molecular property optimization tasks.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\SJLLNZQC\\Alperstein et al_2019_All SMILES Variational Autoencoder.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7HNURBK6\\1905.html}
}

@unpublished{altae-tranLowDataDrug2016,
  title = {Low {{Data Drug Discovery}} with {{One-shot Learning}}},
  author = {Altae-Tran, Han and Ramsundar, Bharath and Pappu, Aneesh S. and Pande, Vijay},
  date = {2016-11-10},
  eprint = {1611.03199},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.03199},
  urldate = {2018-09-17},
  abstract = {Recent advances in machine learning have made significant contributions to drug discovery. Deep neural networks in particular have been demonstrated to provide significant boosts in predictive power when inferring the properties and activities of small-molecule compounds. However, the applicability of these techniques has been limited by the requirement for large amounts of training data. In this work, we demonstrate how one-shot learning can be used to significantly lower the amounts of data required to make meaningful predictions in drug discovery applications. We introduce a new architecture, the residual LSTM embedding, that, when combined with graph convolutional neural networks, significantly improves the ability to learn meaningful distance metrics over small-molecules. We open source all models introduced in this work as part of DeepChem, an open-source framework for deep-learning in drug discovery.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\UKAK5BRQ\\Altae-Tran et al_2016_Low Data Drug Discovery with One-shot Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AUMJMKQP\\1611.html}
}

@incollection{andersonCormorantCovariantMolecular2019,
  title = {Cormorant: {{Covariant Molecular Neural Networks}}},
  shorttitle = {Cormorant},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Anderson, Brandon and Hy, Truong Son and Kondor, Risi},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Alché-Buc, given=F., prefix=d\textbackslash textquotesingle, useprefix=false and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {14510--14519},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/9596-cormorant-covariant-molecular-neural-networks.pdf},
  urldate = {2019-12-19},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\SJ6CBW8W\\Anderson et al_2019_Cormorant.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\F82EURGH\\9596-cormorant-covariant-molecular-neural-networks.html}
}

@online{andreasbenderComparisonStructureLigandBased,
  title = {Comparison of {{Structure-}} and {{Ligand-Based Scoring Functions}} for {{Deep Generative Models}}, {{A GPCR Case Study}}: {{Interview With}} the {{Author}} – {{DrugDiscovery}}.{{NET}} – {{AI}} in {{Drug Discovery}}},
  shorttitle = {Comparison of {{Structure-}} and {{Ligand-Based Scoring Functions}} for {{Deep Generative Models}}, {{A GPCR Case Study}}},
  author = {Andreasbender},
  url = {http://www.drugdiscovery.net/2021/05/20/comparison-of-structure-and-ligand-based-scoring-functions-for-deep-generative-models-a-gpcr-case-study-interview-with-the-author/},
  urldate = {2021-05-23},
  langid = {american},
  file = {C:\Users\USEBPERP\Zotero\storage\6XHD4C7X\comparison-of-structure-and-ligand-based-scoring-functions-for-deep-generative-models-a-gpcr-ca.html}
}

@incollection{angeliChemicalBiologicalDiversity2002,
  title = {Chemical and Biological Diversity in Drug Discovery},
  booktitle = {Pharmacochemistry {{Library}}},
  author = {Angeli, P. and Gaviraghi, G.},
  date = {2002},
  volume = {32},
  pages = {95--96},
  publisher = {Elsevier},
  doi = {10.1016/S0165-7208(02)80011-3},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0165720802800113},
  urldate = {2022-08-18},
  isbn = {978-0-444-50760-0},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\C3A63UIS\Angeli and Gaviraghi - 2002 - Chemical and biological diversity in drug discover.pdf}
}

@inproceedings{angiulliFastOutlierDetection2002,
  title = {Fast {{Outlier Detection}} in {{High Dimensional Spaces}}},
  booktitle = {Principles of {{Data Mining}} and {{Knowledge Discovery}}},
  author = {Angiulli, Fabrizio and Pizzuti, Clara},
  editor = {Elomaa, Tapio and Mannila, Heikki and Toivonen, Hannu},
  date = {2002},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {15--27},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-45681-3_2},
  abstract = {In this paper we propose a new definition of distance-based outlier that considers for each point the sum of the distances from its k nearest neighbors, called weight. Outliers are those points having the largest values of weight. In order to compute these weights, we find the k nearest neighbors of each point in a fast and efficient way by linearizing the search space through the Hilbert space filling curve. The algorithm consists of two phases, the first provides an approximated solution, within a small factor, after executing at most d + 1 scans of the data set with a low time complexity cost, where d is the number of dimensions of the data set. During each scan the number of points candidate to belong to the solution set is sensibly reduced. The second phase returns the exact solution by doing a single scan which examines further a little fraction of the data set. Experimental results show that the algorithm always finds the exact solution during the first phase after d- 《 d + 1 steps and it scales linearly both in the dimensionality and the size of the data set.},
  isbn = {978-3-540-45681-0},
  langid = {english},
  keywords = {High Dimensional Space,Hilbert Curve,Local Outlier,Outlier Detection,Point Feature},
  file = {C:\Users\USEBPERP\Zotero\storage\TUQ67872\Angiulli and Pizzuti - 2002 - Fast Outlier Detection in High Dimensional Spaces.pdf}
}

@online{AnomalyDetectionLow,
  title = {Anomaly Detection for Low Volume Metrics},
  url = {https://www.sinch.com/blog/anomaly-detection-for-low-volume-metrics/},
  urldate = {2023-01-05},
  abstract = {In this article:                      The quest for time-series anomaly detection at Sinch - part one                         Moving average                         Time warping                         Conclusions},
  langid = {english},
  organization = {Sinch},
  file = {C:\Users\USEBPERP\Zotero\storage\GUKTXQN2\anomaly-detection-for-low-volume-metrics.html}
}

@online{AnomalyDetectionSurvey,
  title = {Anomaly Detection: {{A}} Survey: {{ACM Computing Surveys}}: {{Vol}} 41, {{No}} 3},
  url = {https://dl.acm.org/doi/10.1145/1541880.1541882},
  urldate = {2023-01-06},
  file = {C:\Users\USEBPERP\Zotero\storage\NDHETXR2\1541880.html}
}

@online{AnomalyDetectionUnivariate,
  title = {Anomaly {{Detection}} in {{Univariate Time-series}}: {{A Survey}} on the {{Stateof-the-Art}} - {{Google Suche}}},
  url = {https://www.google.com/search?q=Anomaly+Detection+in+Univariate+Time-series%3A+A+Survey+on+the+Stateof-the-Art&rlz=1C5GCEM_enDE1028DE1028&sourceid=chrome&ie=UTF-8},
  urldate = {2023-01-06},
  file = {C:\Users\USEBPERP\Zotero\storage\ZI6KZH3N\search.html}
}

@article{anonymousDEFactorDifferentiableEdge2018,
  title = {{{DEFactor}}: {{Differentiable Edge Factorization-based Probabilistic Graph Generation}}},
  shorttitle = {{{DEFactor}}},
  author = {Anonymous},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=Bygre3R9Fm&noteId=rkeqNivN5m},
  urldate = {2018-10-09},
  abstract = {Generating novel molecules with optimal properties is a crucial step in many industries such as drug discovery. Recently, deep generative models have shown a promising way of performing de-novo...},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\APN6QTV4\\Anonymous_2018_DEFactor.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\76CTJN5C\\forum.html}
}

@article{anonymousHdetachModifyingLSTM2018,
  title = {H-Detach: {{Modifying}} the {{LSTM Gradient Towards Better Optimization}}},
  shorttitle = {H-Detach},
  author = {Anonymous},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=HJMXTsCqYQ&noteId=HJMXTsCqYQ},
  urldate = {2018-10-09},
  abstract = {Recurrent neural networks are known for their notorious exploding and vanishing gradient problem (EVGP). This problem becomes more evident in tasks where the information needed to correctly solve...},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\CLPNTWT8\\Anonymous_2018_h-detach.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QFDIJJN7\\forum.html}
}

@article{anonymousMolCycleGANGenerativeModel2018,
  title = {Mol-{{CycleGAN}} - a Generative Model for Molecular Optimization},
  author = {Anonymous},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=BklKFo09YX},
  urldate = {2018-09-28},
  abstract = {Designing a molecule with desired properties is one of the biggest challenges in drug development, as it requires optimization of chemical compound structures with respect to many complex...},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ZJSZLAZY\\Anonymous_2018_Mol-CycleGAN - a generative model for molecular optimization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\UA2YPKRS\\forum.html}
}

@online{ApplicationGenerativeAutoencoder,
  title = {Application of {{Generative Autoencoder}} in {{De Novo Molecular Design}} - {{Blaschke}} - 2018 - {{Molecular Informatics}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1002/minf.201700123},
  urldate = {2019-05-06},
  file = {C:\Users\USEBPERP\Zotero\storage\6AYNFYUL\minf.html}
}

@online{AppliedScientistAnomaly,
  title = {Applied {{Scientist}}, {{Anomaly Detection}} \&amp; {{Insights}}},
  url = {https://www.amazon.jobs/en/jobs/2105541/applied-scientist-anomaly-detection-insights},
  urldate = {2022-08-31},
  abstract = {Job summaryCome build the future of entertainment with us. Are you interested in shaping the future of movies and television? Do you want to define the next generation of how and what Amazon customers are watching?Prime Video is a premium streaming service that offers customers a vast collection of TV shows and movies - all with the ease of finding what they love to watch in one place. We offer customers thousands of popular movies and TV shows from Originals and Exclusive content to exciting live sports events. We also offer our members the opportunity to subscribe to add-on channels which they can cancel at anytime and to rent or buy new release movies and TV box sets on the Prime Video Store. Prime Video is a fast-paced, growth business - available in over 240 countries and territories worldwide. The team works in a dynamic environment where innovating on behalf of our customers is at the heart of everything we do. If this sounds exciting to you, please read on.You will have the opportunity to work with a variety of leading technologies, leveraging AWS. You will help shape the future of Amazon’s video platform by investigating new approaches to provide the best video user experience for customers. This will be balanced with sound engineering practices leveraging tools, data and machine learning. Your goal is to build the best video playback application for the world's most customer-centric company.We are looking for an Applied Scientist to join Prime Video. In this role you will leverage your strong background in Machine Learning to help build the next generation of Time Series Anomaly Detection systems. You will apply your deep knowledge of machine learning to concrete problems that have broad cross-organizational, global, and technology impact. You will work on large engineering efforts that solve significantly complex problems facing global customers. You will be trusted to operate with independence and are often assigned to focus on areas with significant impact on audience satisfaction. You must be equally comfortable digging in to customer requirements as you are drilling into design with development teams and developing production ready learning models. You consistently bring strong, data-driven business and technical judgment to decisions. The ideal candidate will have experience with machine learning models, and additionally, we are seeking candidates with strong rigor in applied sciences and engineering, creativity, curiosity, and great judgment.Key job responsibilitiesYou will work as part of one of our Agile teams, launching and growing new initiatives for Amazon's global business. As an applied scientist, you will be involved in every aspect of the process - from research, idea generation, and data analysis through to development and deployment across a variety of technologies - giving you true ownership on the future of our products. Together we take on hard technical problems, and build systems which meet high standards of performance and operate at massive scale.Amazon is a place where we use science and engineering to solve problems. We're looking for scientists and engineers who can delight customers by continually learning and inventing. From day one, you'll be working with experienced engineers, machine learning experts and other applied scientists who love what they do.A day in the lifeYou will work with internal and external stakeholders, cross-functional partners, and end-users around the world at all levels. Our team makes a big impact because nothing is more important to us than pleasing our customers, continually earning their trust, and thinking long term. You are empowered to bring new technologies and deep learning approaches to your solutions.We embrace the challenges of a fast paced market and evolving technologies, paving the way to universal availability of content. You will be encouraged to see the big picture, be innovative, and positively impact millions of customers.About the teamThe team is based in Amazon's engineering centre in London and consists of engineers and applied scientists with a variety of backgrounds, from seasoned Amazonian to newly hired; with industry experience and straight from college; a mix of ML experts, service experts, and generalists, but all of us learning and growing. We work closely with other Prime Video engineering teams, including teams based on the US west coast and India as well as in London.},
  langid = {english},
  organization = {amazon.jobs},
  file = {C:\Users\USEBPERP\Zotero\storage\2FFTSTIY\applied-scientist-anomaly-detection-insights.html}
}

@article{arjona-medinaRUDDERReturnDecomposition2018,
  title = {{{RUDDER}}: {{Return Decomposition}} for {{Delayed Rewards}}},
  shorttitle = {{{RUDDER}}},
  author = {Arjona-Medina, Jose A. and Gillhofer, Michael and Widrich, Michael and Unterthiner, Thomas and Brandstetter, Johannes and Hochreiter, Sepp},
  date = {2018-06-20},
  journaltitle = {Advances in Neural Information Processing Systems 32},
  eprint = {1806.07857},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1806.07857},
  urldate = {2019-10-01},
  abstract = {We propose RUDDER, a novel reinforcement learning approach for delayed rewards in finite Markov decision processes (MDPs). In MDPs the Q-values are equal to the expected immediate reward plus the expected future rewards. The latter are related to bias problems in temporal difference (TD) learning and to high variance problems in Monte Carlo (MC) learning. Both problems are even more severe when rewards are delayed. RUDDER aims at making the expected future rewards zero, which simplifies Q-value estimation to computing the mean of the immediate reward. We propose the following two new concepts to push the expected future rewards toward zero. (i) Reward redistribution that leads to return-equivalent decision processes with the same optimal policies and, when optimal, zero expected future rewards. (ii) Return decomposition via contribution analysis which transforms the reinforcement learning task into a regression task at which deep learning excels. On artificial tasks with delayed rewards, RUDDER is significantly faster than MC and exponentially faster than Monte Carlo Tree Search (MCTS), TD(\{\textbackslash lambda\}), and reward shaping approaches. At Atari games, RUDDER on top of a Proximal Policy Optimization (PPO) baseline improves the scores, which is most prominent at games with delayed rewards. Source code is available at \textbackslash url\{https://github.com/ml-jku/rudder\} and demonstration videos at \textbackslash url\{https://goo.gl/EQerZV\}.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\WESUDWEB\\Arjona-Medina et al_2018_RUDDER.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BRYLZEH4\\1806.html}
}

@unpublished{arjovskyInvariantRiskMinimization2020,
  title = {Invariant {{Risk Minimization}}},
  author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
  date = {2020-03-27},
  eprint = {1907.02893},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.02893},
  urldate = {2020-06-23},
  abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\G3WLFFMJ\\Arjovsky et al_2020_Invariant Risk Minimization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JRB96L2B\\1907.html}
}

@unpublished{arpitCloserLookMemorization2017,
  ids = {arpitCloserLookMemorization2017a},
  title = {A {{Closer Look}} at {{Memorization}} in {{Deep Networks}}},
  author = {Arpit, Devansh and Jastrzębski, Stanisław and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S. and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and Lacoste-Julien, Simon},
  date = {2017-06-16},
  eprint = {1706.05394},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.05394},
  urldate = {2018-09-04},
  abstract = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2IRM5CPG\\Arpit et al_2017_A Closer Look at Memorization in Deep Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7ASK8KK4\\Arpit et al_2017_A Closer Look at Memorization in Deep Networks2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\J34ZXUUL\\1706.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZS6J4RLD\\1706.html}
}

@online{ArtificialIntelligenceModern,
  title = {Artificial {{Intelligence}}: {{A Modern Approach}}, 4th {{US}} Ed.},
  url = {https://aima.cs.berkeley.edu/},
  urldate = {2023-01-05},
  file = {C:\Users\USEBPERP\Zotero\storage\4WAA9MJS\aima.cs.berkeley.edu.html}
}

@article{arus-pousExploringGDB13Chemical2019,
  ids = {arus-pousExploringGDB13Chemical2019a},
  title = {Exploring the {{GDB-13}} Chemical Space Using Deep Generative Models},
  author = {Arús-Pous, Josep and Blaschke, Thomas and Ulander, Silas and Reymond, Jean-Louis and Chen, Hongming and Engkvist, Ola},
  date = {2019-03-12},
  journaltitle = {Journal of Cheminformatics},
  volume = {11},
  number = {1},
  pages = {20},
  issn = {1758-2946},
  doi = {10.1186/s13321-019-0341-z},
  url = {https://doi.org/10.1186/s13321-019-0341-z},
  urldate = {2019-05-06},
  abstract = {Recent applications of recurrent neural networks (RNN) enable training models that sample the chemical space. In this study we train RNN with molecular string representations (SMILES) with a subset of the enumerated database GDB-13 (975 million molecules). We show that a model trained with 1 million structures (0.1\% of the database) reproduces 68.9\% of the entire database after training, when sampling 2 billion molecules. We also developed a method to assess the quality of the training process using negative log-likelihood plots. Furthermore, we use a mathematical model based on the “coupon collector problem” that compares the trained model to an upper bound and thus we are able to quantify how much it has learned. We also suggest that this method can be used as a tool to benchmark the learning capabilities of any molecular generative model architecture. Additionally, an analysis of the generated chemical space was performed, which shows that, mostly due to the syntax of SMILES, complex molecules with many rings and heteroatoms are more difficult to sample.},
  keywords = {Chemical databases,Chemical space exploration,Deep generative models,Deep learning,Recurrent neural networks},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\J9LDM3MT\\Arús-Pous et al_2019_Exploring the GDB-13 chemical space using deep generative models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TTMP9A9U\\Arús-Pous et al. - 2019 - Exploring the GDB-13 chemical space using deep gen.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YL87PJFD\\s13321-019-0341-z.html}
}

@unpublished{asanoSurprisingEffectivenessFewImage2019,
  title = {Surprising {{Effectiveness}} of {{Few-Image Unsupervised Feature Learning}}},
  author = {Asano, Yuki M. and Rupprecht, Christian and Vedaldi, Andrea},
  date = {2019-04-30},
  eprint = {1904.13132},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1904.13132},
  urldate = {2019-05-03},
  abstract = {State-of-the-art methods for unsupervised representation learning can train well the first few layers of standard convolutional neural networks, but they are not as good as supervised learning for deeper layers. This is likely due to the generic and relatively simple nature of shallow layers; and yet, these approaches are applied to millions of images, scalability being advertised as their major advantage since unlabelled data is cheap to collect. In this paper we question this practice and ask whether so many images are actually needed to learn the layers for which unsupervised learning works best. Our main result is that a few or even a single image together with strong data augmentation are sufficient to nearly saturate performance. Specifically, we provide an analysis for three different self-supervised feature learning methods (BiGAN, RotNet, DeepCluster) vs number of training images (1, 10, 1000) and show that we can top the accuracy for the first two convolutional layers of common networks using just a single unlabelled training image and obtain competitive results for other layers. We further study and visualize the learned representation as a function of which (single) image is used for training. Our results are also suggestive of which type of information may be captured by shallow layers in deep networks.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C:\Users\USEBPERP\Zotero\storage\GS2AACVB\Asano et al_2019_Surprising Effectiveness of Few-Image Unsupervised Feature Learning.pdf}
}

@article{ashtonIdentificationDiverseDatabase2002,
  title = {Identification of {{Diverse Database Subsets}} Using {{Property-Based}} and {{Fragment-Based Molecular Descriptions}}},
  author = {Ashton, Mark and Barnard, John and Casset, Florence and Charlton, Michael and Downs, Geoffrey and Gorse, Dominique and Holliday, John and Lahana, Roger and Willett, Peter},
  date = {2002},
  journaltitle = {Quantitative Structure-Activity Relationships},
  volume = {21},
  number = {6},
  pages = {598--604},
  issn = {1521-3838},
  doi = {10.1002/qsar.200290002},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qsar.200290002},
  urldate = {2022-06-07},
  abstract = {This paper reports a comparison of calculated molecular properties and of 2D fragment bit-strings when used for the selection of structurally diverse subsets of a file of 44295 compounds. MaxMin dissimilarity-based selection and k-means cluster-based selection are used to select subsets containing between 1\% and 20\% of the file. Investigation of the numbers of bioactive molecules in the selected subsets suggest: that the MaxMin subsets are noticeably superior to the k-means subsets; that the property-based descriptors are marginally superior to the fragment-based descriptors; and that both approaches are noticeably superior to random selection.},
  langid = {english},
  keywords = {Diversity,Molecular diversity analysis,Structural diversity,Subset selection},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\QMTFAGK8\\Ashton et al. - 2002 - Identification of Diverse Database Subsets using P.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZM8PTTF9\\qsar.html}
}

@unpublished{assouelDEFactorDifferentiableEdge2018,
  title = {{{DEFactor}}: {{Differentiable Edge Factorization-based Probabilistic Graph Generation}}},
  shorttitle = {{{DEFactor}}},
  author = {Assouel, Rim and Ahmed, Mohamed and Segler, Marwin H. and Saffari, Amir and Bengio, Yoshua},
  date = {2018-11-24},
  eprint = {1811.09766},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1811.09766},
  urldate = {2020-04-07},
  abstract = {Generating novel molecules with optimal properties is a crucial step in many industries such as drug discovery. Recently, deep generative models have shown a promising way of performing de-novo molecular design. Although graph generative models are currently available they either have a graph size dependency in their number of parameters, limiting their use to only very small graphs or are formulated as a sequence of discrete actions needed to construct a graph, making the output graph non-differentiable w.r.t the model parameters, therefore preventing them to be used in scenarios such as conditional graph generation. In this work we propose a model for conditional graph generation that is computationally efficient and enables direct optimisation of the graph. We demonstrate favourable performance of our model on prototype-based molecular graph conditional generation tasks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\SMIEVBBR\\Assouel et al_2018_DEFactor.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\X7MCHWQ5\\1811.html}
}

@article{atanasovDiscoveryResupplyPharmacologically2015,
  title = {Discovery and Resupply of Pharmacologically Active Plant-Derived Natural Products: {{A}} Review},
  shorttitle = {Discovery and Resupply of Pharmacologically Active Plant-Derived Natural Products},
  author = {Atanasov, Atanas G. and Waltenberger, Birgit and Pferschy-Wenzig, Eva-Maria and Linder, Thomas and Wawrosch, Christoph and Uhrin, Pavel and Temml, Veronika and Wang, Limei and Schwaiger, Stefan and Heiss, Elke H. and Rollinger, Judith M. and Schuster, Daniela and Breuss, Johannes M. and Bochkov, Valery and Mihovilovic, Marko D. and Kopp, Brigitte and Bauer, Rudolf and Dirsch, Verena M. and Stuppner, Hermann},
  date = {2015-12},
  journaltitle = {Biotechnol Adv},
  volume = {33},
  number = {8},
  eprint = {26281720},
  eprinttype = {pmid},
  pages = {1582--1614},
  issn = {0734-9750},
  doi = {10.1016/j.biotechadv.2015.08.001},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4748402/},
  urldate = {2018-05-20},
  abstract = {Medicinal plants have historically proven their value as a source of molecules with therapeutic potential, and nowadays still represent an important pool for the identification of novel drug leads. In the past decades, pharmaceutical industry focused mainly on libraries of synthetic compounds as drug discovery source. They are comparably easy to produce and resupply, and demonstrate good compatibility with established high throughput screening (HTS) platforms. However, at the same time there has been a declining trend in the number of new drugs reaching the market, raising renewed scientific interest in drug discovery from natural sources, despite of its known challenges. In this survey, a brief outline of historical development is provided together with a comprehensive overview of used approaches and recent developments relevant to plant-derived natural product drug discovery. Associated challenges and major strengths of natural product-based drug discovery are critically discussed. A snapshot of the advanced plant-derived natural products that are currently in actively recruiting clinical trials is also presented. Importantly, the transition of a natural compound from a “screening hit” through a “drug lead” to a “marketed drug” is associated with increasingly challenging demands for compound amount, which often cannot be met by re-isolation from the respective plant sources. In this regard, existing alternatives for resupply are also discussed, including different biotechnology approaches and total organic synthesis., While the intrinsic complexity of natural product-based drug discovery necessitates highly integrated interdisciplinary approaches, the reviewed scientific developments, recent technological advances, and research trends clearly indicate that natural products will be among the most important sources of new drugs also in the future.},
  pmcid = {PMC4748402},
  file = {C:\Users\USEBPERP\Zotero\storage\C498R383\Atanasov et al_2015_Discovery and resupply of pharmacologically active plant-derived natural.pdf}
}

@article{atanceNovoDrugDesign2022,
  title = {De {{Novo Drug Design Using Reinforcement Learning}} with {{Graph-Based Deep Generative Models}}},
  author = {Atance, Sara Romeo and Diez, Juan Viguera and Engkvist, Ola and Olsson, Simon and Mercado, Rocío},
  date = {2022-10-24},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {62},
  number = {20},
  pages = {4863--4872},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.2c00838},
  url = {https://doi.org/10.1021/acs.jcim.2c00838},
  urldate = {2023-10-29},
  abstract = {Machine learning provides effective computational tools for exploring the chemical space via deep generative models. Here, we propose a new reinforcement learning scheme to fine-tune graph-based deep generative models for de novo molecular design tasks. We show how our computational framework can successfully guide a pretrained generative model toward the generation of molecules with a specific property profile, even when such molecules are not present in the training set and unlikely to be generated by the pretrained model. We explored the following tasks: generating molecules of decreasing/increasing size, increasing drug-likeness, and increasing bioactivity. Using the proposed approach, we achieve a model which generates diverse compounds with predicted DRD2 activity for 95\% of sampled molecules, outperforming previously reported methods on this metric.}
}

@online{AutomaticChemicalDesign,
  title = {Automatic {{Chemical Design Using}} a {{Data-Driven Continuous Representation}} of {{Molecules}} - {{ACS Central Science}} ({{ACS Publications}})},
  url = {https://pubs.acs.org/doi/10.1021/acscentsci.7b00572},
  urldate = {2018-10-11},
  file = {C:\Users\USEBPERP\Zotero\storage\2ZT7HHG6\acscentsci.html}
}

@article{awadMoralMachineExperiment2018,
  title = {The {{Moral Machine}} Experiment},
  author = {Awad, Edmond and Dsouza, Sohan and Kim, Richard and Schulz, Jonathan and Henrich, Joseph and Shariff, Azim and Bonnefon, Jean-François and Rahwan, Iyad},
  date = {2018-11},
  journaltitle = {Nature},
  volume = {563},
  number = {7729},
  pages = {59--64},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-018-0637-6},
  url = {http://www.nature.com/articles/s41586-018-0637-6},
  urldate = {2018-11-06},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\82HS6ZFB\Awad et al_2018_The Moral Machine experiment.pdf}
}

@online{ayedAnomalyDetectionScale2020,
  title = {Anomaly {{Detection}} at {{Scale}}: {{The Case}} for {{Deep Distributional Time Series Models}}},
  shorttitle = {Anomaly {{Detection}} at {{Scale}}},
  author = {Ayed, Fadhel and Stella, Lorenzo and Januschowski, Tim and Gasthaus, Jan},
  date = {2020-07-30},
  eprint = {2007.15541},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.15541},
  urldate = {2023-01-06},
  abstract = {This paper introduces a new methodology for detecting anomalies in time series data, with a primary application to monitoring the health of (micro-) services and cloud resources. The main novelty in our approach is that instead of modeling time series consisting of real values or vectors of real values, we model time series of probability distributions over real values (or vectors). This extension to time series of probability distributions allows the technique to be applied to the common scenario where the data is generated by requests coming in to a service, which is then aggregated at a fixed temporal frequency. Our method is amenable to streaming anomaly detection and scales to monitoring for anomalies on millions of time series. We show the superior accuracy of our method on synthetic and public real-world data. On the Yahoo Webscope data set, we outperform the state of the art in 3 out of 4 data sets and we show that we outperform popular open-source anomaly detection tools by up to 17\% average improvement for a real-world data set.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ZESLJ7SS\\Ayed et al. - 2020 - Anomaly Detection at Scale The Case for Deep Dist.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ADUJRM6T\\2007.html}
}

@unpublished{aytarPlayingHardExploration2018,
  title = {Playing Hard Exploration Games by Watching {{YouTube}}},
  author = {Aytar, Yusuf and Pfaff, Tobias and Budden, David and Paine, Tom Le and Wang, Ziyu and family=Freitas, given=Nando, prefix=de, useprefix=true},
  date = {2018-05-29},
  eprint = {1805.11592},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.11592},
  urldate = {2019-02-19},
  abstract = {Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games Montezuma's Revenge, Pitfall! and Private Eye for the first time, even if the agent is not presented with any environment rewards.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\WVXARTJX\\Aytar et al_2018_Playing hard exploration games by watching YouTube.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6HTEAM4C\\1805.html}
}

@unpublished{azarWorldDiscoveryModels2019,
  title = {World {{Discovery Models}}},
  author = {Azar, Mohammad Gheshlaghi and Piot, Bilal and Pires, Bernardo Avila and Grill, Jean-Bastien and Altché, Florent and Munos, Rémi},
  date = {2019-02-20},
  eprint = {1902.07685},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.07685},
  urldate = {2019-06-06},
  abstract = {As humans we are driven by a strong desire for seeking novelty in our world. Also upon observing a novel pattern we are capable of refining our understanding of the world based on the new information---humans can discover their world. The outstanding ability of the human mind for discovery has led to many breakthroughs in science, art and technology. Here we investigate the possibility of building an agent capable of discovering its world using the modern AI technology. In particular we introduce NDIGO, Neural Differential Information Gain Optimisation, a self-supervised discovery model that aims at seeking new information to construct a global view of its world from partial and noisy observations. Our experiments on some controlled 2-D navigation tasks show that NDIGO outperforms state-of-the-art information-seeking methods in terms of the quality of the learned representation. The improvement in performance is particularly significant in the presence of white or structured noise where other information-seeking methods follow the noise instead of discovering their world.},
  keywords = {Computer Science - Artificial Intelligence,Statistics - Applications,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\44IMKL6X\\Azar et al_2019_World Discovery Models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\A9XS2EZB\\1902.html}
}

@article{badowskiSynergyExpertMachineLearning2020,
  title = {Synergy {{Between Expert}} and {{Machine-Learning Approaches Allows}} for {{Improved Retrosynthetic Planning}}},
  author = {Badowski, Tomasz and Gajewska, Ewa P. and Molga, Karol and Grzybowski, Bartosz A.},
  date = {2020},
  journaltitle = {Angewandte Chemie International Edition},
  volume = {59},
  number = {2},
  pages = {725--730},
  issn = {1521-3773},
  doi = {10.1002/anie.201912083},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.201912083},
  urldate = {2020-11-19},
  abstract = {When computers plan multistep syntheses, they can rely either on expert knowledge or information machine-extracted from large reaction repositories. Both approaches suffer from imperfect functions evaluating reaction choices: expert functions are heuristics based on chemical intuition, whereas machine learning (ML) relies on neural networks (NNs) that can make meaningful predictions only about popular reaction types. This paper shows that expert and ML approaches can be synergistic—specifically, when NNs are trained on literature data matched onto high-quality, expert-coded reaction rules, they achieve higher synthetic accuracy than either of the methods alone and, importantly, can also handle rare/specialized reaction types.},
  langid = {english},
  keywords = {artificial intelligence,computer-aided retrosynthesis,expert systems,neural networks},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\B29L8BTP\\Badowski et al. - 2020 - Synergy Between Expert and Machine-Learning Approa.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\X7BNLRLK\\anie.html}
}

@article{bajorathDualityActivityCliffs2019,
  title = {Duality of Activity Cliffs in Drug Discovery},
  author = {Bajorath, Jürgen},
  date = {2019-06-03},
  journaltitle = {Expert Opinion on Drug Discovery},
  volume = {14},
  number = {6},
  eprint = {30882260},
  eprinttype = {pmid},
  pages = {517--520},
  issn = {1746-0441},
  doi = {10.1080/17460441.2019.1593371},
  url = {https://doi.org/10.1080/17460441.2019.1593371},
  urldate = {2019-05-20},
  keywords = {Activity cliffs,analogs,chemical similarity,compound activity data,interpretability,lead optimization,potency value distributions,structure-activity relationships},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7DVX62YQ\\Bajorath_2019_Duality of activity cliffs in drug discovery.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XY3BV2IJ\\17460441.2019.html}
}

@unpublished{baLayerNormalization2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  date = {2016-07-21},
  eprint = {1607.06450},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1607.06450},
  urldate = {2021-04-08},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\XC7MTJ4P\\Ba et al. - 2016 - Layer Normalization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\NTJ5ALY8\\1607.html}
}

@article{baldiDropoutLearningAlgorithm2014,
  title = {The Dropout Learning Algorithm},
  author = {Baldi, Pierre and Sadowski, Peter},
  date = {2014-05-01},
  journaltitle = {Artificial Intelligence},
  volume = {210},
  pages = {78--122},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2014.02.004},
  url = {http://www.sciencedirect.com/science/article/pii/S0004370214000216},
  urldate = {2019-04-03},
  abstract = {Dropout is a recently introduced algorithm for training neural networks by randomly dropping units during training to prevent their co-adaptation. A mathematical analysis of some of the static and dynamic properties of dropout is provided using Bernoulli gating variables, general enough to accommodate dropout on units or connections, and with variable rates. The framework allows a complete analysis of the ensemble averaging properties of dropout in linear networks, which is useful to understand the non-linear case. The ensemble averaging properties of dropout in non-linear logistic networks result from three fundamental equations: (1) the approximation of the expectations of logistic functions by normalized geometric means, for which bounds and estimates are derived; (2) the algebraic equality between normalized geometric means of logistic functions with the logistic of the means, which mathematically characterizes logistic functions; and (3) the linearity of the means with respect to sums, as well as products of independent variables. The results are also extended to other classes of transfer functions, including rectified linear functions. Approximation errors tend to cancel each other and do not accumulate. Dropout can also be connected to stochastic neurons and used to predict firing rates, and to backpropagation by viewing the backward propagation as ensemble averaging in a dropout linear network. Moreover, the convergence properties of dropout can be understood in terms of stochastic gradient descent. Finally, for the regularization properties of dropout, the expectation of the dropout gradient is the gradient of the corresponding approximation ensemble, regularized by an adaptive weight decay term with a propensity for self-consistent variance minimization and sparse representations.},
  keywords = {Backpropagation,Ensemble,Geometric mean,Machine learning,Neural networks,Regularization,Sparse representations,Stochastic gradient descent,Stochastic neurons,Variance minimization},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4MIIGAQI\\Baldi_Sadowski_2014_The dropout learning algorithm.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\UGC4YA89\\S0004370214000216.html}
}

@incollection{baldiUnderstandingDropout2013,
  title = {Understanding {{Dropout}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26},
  author = {Baldi, Pierre and Sadowski, Peter J},
  editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
  date = {2013},
  pages = {2814--2822},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/4878-understanding-dropout.pdf},
  urldate = {2019-03-28},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\UXHUY729\\Baldi_Sadowski_2013_Understanding Dropout.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TS4XAZVT\\4878-understanding-dropout.html}
}

@article{bansalCanWeGain,
  title = {Can {{We Gain More}} from {{Orthogonality Regularizations}} in {{Training Deep CNNs}}?},
  author = {Bansal, Nitin and Chen, Xiaohan and Wang, Zhangyang},
  pages = {11},
  abstract = {This paper seeks to answer the question: as the (near-) orthogonality of weights is found to be a favorable property for training deep convolutional neural networks, how can we enforce it in more effective and easy-to-use ways? We develop novel orthogonality regularizations on training deep CNNs, utilizing various advanced analytical tools such as mutual coherence and restricted isometry property. These plug-and-play regularizations can be conveniently incorporated into training almost any CNN without extra hassle. We then benchmark their effects on state-of-the-art models: ResNet, WideResNet, and ResNeXt, on several most popular computer vision datasets: CIFAR-10, CIFAR-100, SVHN and ImageNet. We observe consistent performance gains after applying those proposed regularizations, in terms of both the final accuracies achieved, and faster and more stable convergences. We have made our codes and pre-trained models publicly available.1.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\MYD8GVE2\Bansal et al_Can We Gain More from Orthogonality Regularizations in Training Deep CNNs.pdf}
}

@online{BayesianMolecularDesign,
  title = {Bayesian Molecular Design with a Chemical Language Model | {{SpringerLink}}},
  url = {https://link.springer.com/article/10.1007/s10822-016-0008-z},
  urldate = {2019-05-06},
  file = {C:\Users\USEBPERP\Zotero\storage\3Z4HR2NT\s10822-016-0008-z.html}
}

@article{baylonEnhancingRetrosyntheticReaction2019,
  ids = {baylon2019enhancinga,baylonEnhancingRetrosyntheticReaction2019a},
  title = {Enhancing {{Retrosynthetic Reaction Prediction}} with {{Deep Learning}} Using {{Multiscale Reaction Classification}}},
  author = {Baylon, Javier L. and Cilfone, Nicholas A. and Gulcher, Jeffrey R. and Chittenden, Thomas W.},
  date = {2019-01-14},
  journaltitle = {J. Chem. Inf. Model.},
  number = {2},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.8b00801},
  url = {https://doi.org/10.1021/acs.jcim.8b00801},
  urldate = {2019-01-23},
  abstract = {Chemical synthesis planning is a key aspect in many fields of chemistry, especially drug discovery. Recent implementations of machine learning and artificial intelligence techniques for retrosynthetic analysis have shown great potential to improve computational methods for synthesis planning. Herein, we present a multiscale, data-driven approach for retrosynthetic analysis with deep highway networks (DHN). We automatically extracted reaction rules (i.e., ways in which a molecule is produced) from a dataset consisting of chemical reactions derived from U.S. patents. We performed the retrosynthetic reaction prediction task in two steps: first, we built a DHN model to predict which group of reactions (consisting of chemically similar reaction rules) was employed to produce a molecule. Once a reaction group was identified, a DHN trained on the subset of reactions within the identified reaction group, was employed to predict the transformation rule used to produce a molecule. To validate our approach, we predicted the first retrosynthetic reaction step for 40 approved drugs using our multiscale model and compared its predictive performance with a conventional model trained on all machine-extracted reaction rules employed as a control. Our multiscale approach showed a success rate of 82.9\% at generating valid reactants from retrosynthetic reaction predictions. Comparatively, the control model trained on all machine-extracted reaction rules yielded a success rate of 58.5\% on the validation set of 40 pharmaceutical molecules, indicating a significant statistical improvement with our approach to match known first synthetic reaction of the tested drugs in this study. While our multiscale approach was unable to outperform state-of-the-art rule-based systems curated by expert chemists, multiscale classification represents a marked enhancement in retrosynthetic analysis and can be easily adapted for use in a range of artificial intelligence strategies.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\75MBQH7R\\Baylon et al_2019_Enhancing Retrosynthetic Reaction Prediction with Deep Learning using.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZHPV2UU4\\Baylon et al. - 2019 - Enhancing Retrosynthetic Reaction Prediction with .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\NYPAJF5T\\acs.jcim.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\XZRQRIDJ\\acs.jcim.html}
}

@unpublished{becigneulOptimalTransportGraph2021,
  title = {Optimal {{Transport Graph Neural Networks}}},
  author = {Bécigneul, Gary and Ganea, Octavian-Eugen and Chen, Benson and Barzilay, Regina and Jaakkola, Tommi},
  date = {2021-02-13},
  eprint = {2006.04804},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.04804},
  urldate = {2021-04-21},
  abstract = {Current graph neural network (GNN) architectures naively average or sum node embeddings into an aggregated graph representation -- potentially losing structural or semantic information. We here introduce OT-GNN, a model that computes graph embeddings using parametric prototypes that highlight key facets of different graph aspects. Towards this goal, we are (to our knowledge) the first to successfully combine optimal transport (OT) with parametric graph models. Graph representations are obtained from Wasserstein distances between the set of GNN node embeddings and "prototype" point clouds as free parameters. We theoretically prove that, unlike traditional sum aggregation, our function class on point clouds satisfies a fundamental universal approximation theorem. Empirically, we address an inherent collapse optimization issue by proposing a noise contrastive regularizer to steer the model towards truly exploiting the optimal transport geometry. Finally, we consistently report better generalization performance on several molecular property prediction tasks, while exhibiting smoother graph representations.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@report{bedoyaPrivacyCivilRight2020,
  type = {SSRN Scholarly Paper},
  title = {Privacy as {{Civil Right}}},
  author = {Bedoya, Alvaro},
  date = {2020-05-12},
  number = {ID 3599201},
  institution = {Social Science Research Network},
  location = {Rochester, NY},
  url = {https://papers.ssrn.com/abstract=3599201},
  urldate = {2020-06-24},
  abstract = {As the first U.S.-born Hispanic senator, Senator Dennis Chávez of New Mexico left a rich legacy of advocacy for civil rights and civil liberties. In this lecture, the fourth U.S. Senator Dennis Chávez Endowed Lecture on Law and Civil Rights, I explore an idea at the intersection of those two bodies of law: the right to privacy. In 2020, the hallmark of surveillance is its ubiquity; “everyone is watched.” Unfortunately, this discourse erases the fact that, across American history, the burdens of government surveillance have fallen overwhelmingly on the shoulders of immigrants, heretics, people of color, the poor, and anyone else considered “other.” Inspired by the legacy of “El Senador,” I trace that history from the English Puritans we now know as Pilgrims to the immigrant children detained at the southern U.S. border. I go on to argue that if we acknowledge the “color of surveillance,” we must reckon with its consequence. If surveillance is a tool used to threaten the vulnerable, we must understand privacy not just as a civil liberty, but also a civil right: A shield that allows the unpopular and persecuted to survive and thrive.},
  langid = {english},
  keywords = {civil liberties,civil rights,privacy,senate},
  file = {C:\Users\USEBPERP\Zotero\storage\KUG6NHLC\papers.html}
}

@article{begleyRaiseStandardsPreclinical2012,
  title = {Raise Standards for Preclinical Cancer Research},
  author = {Begley, C. Glenn and Ellis, Lee M.},
  date = {2012-03},
  journaltitle = {Nature},
  volume = {483},
  number = {7391},
  pages = {531--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/483531a},
  url = {https://www.nature.com/articles/483531a},
  urldate = {2020-07-09},
  abstract = {C. Glenn Begley and Lee M. Ellis propose how methods, publications and incentives must change if patients are to benefit.},
  issue = {7391},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\EBLDB28U\\Begley_Ellis_2012_Raise standards for preclinical cancer research.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SAIS8PYP\\Begley and Ellis - 2012 - Raise standards for preclinical cancer research.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\974HU585\\483531a.html}
}

@incollection{belkinOverfittingPerfectFitting2018,
  title = {Overfitting or Perfect Fitting? {{Risk}} Bounds for Classification and Regression Rules That Interpolate},
  shorttitle = {Overfitting or Perfect Fitting?},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Belkin, Mikhail and Hsu, Daniel J and Mitra, Partha},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {2300--2311},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/7498-overfitting-or-perfect-fitting-risk-bounds-for-classification-and-regression-rules-that-interpolate.pdf},
  urldate = {2019-09-24},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4UGRM4QM\\Belkin et al_2018_Overfitting or perfect fitting.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FWCDQYB3\\7498-overfitting-or-perfect-fitting-risk-bounds-for-classification-and-regression-rules-that-in.html}
}

@article{bemisPropertiesKnownDrugs1996,
  title = {The {{Properties}} of {{Known Drugs}}. 1. {{Molecular Frameworks}}},
  author = {Bemis, Guy W. and Murcko, Mark A.},
  date = {1996-01-01},
  journaltitle = {J. Med. Chem.},
  volume = {39},
  number = {15},
  pages = {2887--2893},
  publisher = {American Chemical Society},
  issn = {0022-2623},
  doi = {10.1021/jm9602928},
  url = {https://doi.org/10.1021/jm9602928},
  urldate = {2023-11-20},
  abstract = {In order to better understand the common features present in drug molecules, we use shape description methods to analyze a database of commercially available drugs and prepare a list of common drug shapes. A useful way of organizing this structural data is to group the atoms of each drug molecule into ring, linker, framework, and side chain atoms. On the basis of the two-dimensional molecular structures (without regard to atom type, hybridization, and bond order), there are 1179 different frameworks among the 5120 compounds analyzed. However, the shapes of half of the drugs in the database are described by the 32 most frequently occurring frameworks. This suggests that the diversity of shapes in the set of known drugs is extremely low. In our second method of analysis, in which atom type, hybridization, and bond order are considered, more diversity is seen; there are 2506 different frameworks among the 5120 compounds in the database, and the most frequently occurring 42 frameworks account for only one-fourth of the drugs. We discuss the possible interpretations of these findings and the way they may be used to guide future drug discovery research.}
}

@article{ben-davidLearnabilityCanBe2019,
  title = {Learnability Can Be Undecidable},
  author = {Ben-David, Shai and Hrubeš, Pavel and Moran, Shay and Shpilka, Amir and Yehudayoff, Amir},
  date = {2019-01},
  journaltitle = {Nature Machine Intelligence},
  volume = {1},
  number = {1},
  pages = {44},
  issn = {2522-5839},
  doi = {10.1038/s42256-018-0002-3},
  url = {https://www.nature.com/articles/s42256-018-0002-3},
  urldate = {2019-01-16},
  abstract = {Not all mathematical questions can be resolved, according to Gödel’s famous incompleteness theorems. It turns out that machine learning can be vulnerable to undecidability too, as is illustrated with an example problem where learnability cannot be proved nor refuted.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\KZ9EXS5U\\Ben-David et al_2019_Learnability can be undecidable.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AVZV7TRJ\\s42256-018-0002-3.html}
}

@article{benderEvaluationGuidelinesMachine2022,
  title = {Evaluation Guidelines for Machine Learning Tools in the Chemical Sciences},
  author = {Bender, Andreas and Schneider, Nadine and Segler, Marwin and Patrick Walters, W. and Engkvist, Ola and Rodrigues, Tiago},
  date = {2022-06},
  journaltitle = {Nat Rev Chem},
  volume = {6},
  number = {6},
  eprint = {37117429},
  eprinttype = {pmid},
  pages = {428--442},
  issn = {2397-3358},
  doi = {10.1038/s41570-022-00391-9},
  abstract = {Machine learning (ML) promises to tackle the grand challenges in chemistry and speed up the generation, improvement and/or ordering of research hypotheses. Despite the overarching applicability of ML workflows, one usually finds diverse evaluation study designs. The current heterogeneity in evaluation techniques and metrics leads to difficulty in (or the impossibility of) comparing and assessing the relevance of new algorithms. Ultimately, this may delay the digitalization of chemistry at scale and confuse method developers, experimentalists, reviewers and journal editors. In this Perspective, we critically discuss a set of method development and evaluation guidelines for different types of ML-based publications, emphasizing supervised learning. We provide a diverse collection of examples from various authors and disciplines in chemistry. While taking into account varying accessibility across research groups, our recommendations focus on reporting completeness and standardizing comparisons between tools. We aim to further contribute to improved ML transparency and credibility by suggesting a checklist of retro-/prospective tests and dissecting their importance. We envisage that the wide adoption and continuous update of best practices will encourage an informed use of ML on real-world problems related to the chemical sciences.},
  langid = {english}
}

@book{benderMolecularSimilarityKey2004,
  title = {Molecular {{Similarity}}: {{A Key Technique}} in {{Molecular Informatics}}.},
  shorttitle = {Molecular {{Similarity}}},
  author = {Bender, Andreas and Glen, Robert},
  date = {2004-12-01},
  volume = {2},
  doi = {10.1039/B409813G},
  abstract = {Molecular Informatics utilises many ideas and concepts to find relationships between molecules. The concept of similarity, where molecules may be grouped according to their biological effects or physicochemical properties has found extensive use in drug discovery. Some areas of particular interest have been in lead discovery and compound optimisation. For example, in designing libraries of compounds for lead generation, one approach is to design sets of compounds "similar" to known active compounds in the hope that alternative molecular structures are found that maintain the properties required while enhancing e.g. patentability, medicinal chemistry opportunities or even in achieving optimised pharmacokinetic profiles. Thus the practical importance of the concept of molecular similarity has grown dramatically in recent years. The predominant users are pharmaceutical companies, employing similarity methods in a wide range of applications e.g. virtual screening, estimation of absorption, distribution, metabolism, excretion and toxicity (ADME/Tox) and prediction of physicochemical properties (solubility, partitioning etc.). In this perspective, we discuss the representation of molecular structure (descriptors), methods of comparing structures and how these relate to measured properties. This leads to the concept of molecular similarity, its various definitions and uses and how these have evolved in recent years. Here, we wish to evaluate and in some cases challenge accepted views and uses of molecular similarity. Molecular similarity, as a paradigm, contains many implicit and explicit assumptions in particular with respect to the prediction of the binding and efficacy of molecules at biological receptors. The fundamental observation is that molecular similarity has a context which both defines and limits its use. The key issues of solvation effects, heterogeneity of binding sites and the fundamental problem of the form of similarity measure to use are addressed.},
  pagetotal = {3204},
  file = {C:\Users\USEBPERP\Zotero\storage\2GD7U2XE\Bender_Glen_2004_Molecular Similarity.pdf}
}

@online{bengioFlowNetworkBased2021,
  title = {Flow {{Network}} Based {{Generative Models}} for {{Non-Iterative Diverse Candidate Generation}}},
  author = {Bengio, Emmanuel and Jain, Moksh and Korablyov, Maksym and Precup, Doina and Bengio, Yoshua},
  date = {2021-11-19},
  eprint = {2106.04399},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.04399},
  url = {http://arxiv.org/abs/2106.04399},
  urldate = {2022-07-05},
  abstract = {This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While MCMC methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation. Using insights from Temporal Difference learning, we propose GFlowNet, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of GFlowNet on a simple domain where there are many modes to the reward function, and on a molecule synthesis task.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\EDUJYC9Y\\Bengio et al. - 2021 - Flow Network based Generative Models for Non-Itera.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YHKDH6BW\\Bengio et al. - 2021 - Flow Network based Generative Models for Non-Itera.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\75H8JLX4\\2106.html}
}

@article{bengioLearningLongtermDependencies1994,
  title = {Learning Long-Term Dependencies with Gradient Descent Is Difficult},
  author = {Bengio, Y. and Simard, P. and Frasconi, P.},
  date = {1994-03},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {5},
  number = {2},
  pages = {157--166},
  issn = {1045-9227},
  doi = {10.1109/72.279181},
  abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.{$<<$}ETX{$>>$}},
  keywords = {Computer networks,Cost function,Delay effects,Discrete transforms,Displays,efficient learning,gradient descent,input/output sequence mapping,Intelligent networks,learning (artificial intelligence),long-term dependencies,Neural networks,Neurofeedback,numerical analysis,prediction problems,Production,production problems,recognition,recurrent neural nets,recurrent neural network training,Recurrent neural networks,temporal contingencies},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\HDKZ676D\\Bengio et al_1994_Learning long-term dependencies with gradient descent is difficult.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\4HXBR8Y7\\279181.html}
}

@unpublished{bengioRepresentationLearningReview2014,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  shorttitle = {Representation {{Learning}}},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  date = {2014-04-23},
  eprint = {1206.5538},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1206.5538},
  urldate = {2019-12-23},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\9S52PIXX\\Bengio et al_2014_Representation Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QGIQUKYR\\1206.html}
}

@unpublished{benhendaChemGANChallengeDrug2017,
  title = {{{ChemGAN}} Challenge for Drug Discovery: Can {{AI}} Reproduce Natural Chemical Diversity?},
  shorttitle = {{{ChemGAN}} Challenge for Drug Discovery},
  author = {Benhenda, Mostapha},
  date = {2017-08-28},
  eprint = {1708.08227},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1708.08227},
  urldate = {2018-05-24},
  abstract = {Generating molecules with desired chemical properties is important for drug discovery. The use of generative neural networks is promising for this task. However, from visual inspection, it often appears that generated samples lack diversity. In this paper, we quantify this internal chemical diversity, and we raise the following challenge: can a nontrivial AI model reproduce natural chemical diversity for desired molecules? To illustrate this question, we consider two generative models: a Reinforcement Learning model and the recently introduced ORGAN. Both fail at this challenge. We hope this challenge will stimulate research in this direction.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\9PS9YJV9\\Benhenda_2017_ChemGAN challenge for drug discovery.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\Y24NBMTI\\1708.html}
}

@article{bennholdBodyBagsEnemy2020,
  entrysubtype = {newspaper},
  title = {Body {{Bags}} and {{Enemy Lists}}: {{How Far-Right Police Officers}} and {{Ex-Soldiers Planned}} for ‘{{Day X}}’},
  shorttitle = {Body {{Bags}} and {{Enemy Lists}}},
  author = {Bennhold, Katrin},
  date = {2020-08-01T05:00:24-04:00},
  journaltitle = {The New York Times},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2020/08/01/world/europe/germany-nazi-infiltration.html},
  urldate = {2020-08-02},
  abstract = {Germany has woken up to a problem of far-right extremism in its elite special forces. But the threat of neo-Nazi infiltration of state institutions is much broader.},
  journalsubtitle = {World},
  langid = {american},
  keywords = {Alternative for Germany,Fringe Groups and Movements,Germany,Gross Marko,Murders Attempted Murders and Homicides,Neo Nazi Groups,Nordkreuz}
}

@article{bentoChEMBLBioactivityDatabase2014,
  title = {The {{ChEMBL}} Bioactivity Database: An Update},
  shorttitle = {The {{ChEMBL}} Bioactivity Database},
  author = {Bento, A. Patrícia and Gaulton, Anna and Hersey, Anne and Bellis, Louisa J. and Chambers, Jon and Davies, Mark and Krüger, Felix A. and Light, Yvonne and Mak, Lora and McGlinchey, Shaun and Nowotka, Michal and Papadatos, George and Santos, Rita and Overington, John P.},
  date = {2014-01-01},
  journaltitle = {Nucleic Acids Res},
  volume = {42},
  number = {D1},
  pages = {D1083-D1090},
  issn = {0305-1048},
  doi = {10.1093/nar/gkt1031},
  url = {https://academic.oup.com/nar/article/42/D1/D1083/1043509},
  urldate = {2018-02-24},
  abstract = {ChEMBL is an open large-scale bioactivity database (https://www.ebi.ac.uk/chembl), previously described in the 2012 Nucleic Acids Research Database Issue. Since then, a variety of new data sources and improvements in functionality have contributed to the growth and utility of the resource. In particular, more comprehensive tracking of compounds from research stages through clinical development to market is provided through the inclusion of data from United States Adopted Name applications; a new richer data model for representing drug targets has been developed; and a number of methods have been put in place to allow users to more easily identify reliable data. Finally, access to ChEMBL is now available via a new Resource Description Framework format, in addition to the web-based interface, data downloads and web services.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\VKEFB3FT\\Bento et al_2014_The ChEMBL bioactivity database.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\THZ6IESX\\1043509.html}
}

@article{bernoulliExpositionNewTheory1954,
  title = {Exposition of a {{New Theory}} on the {{Measurement}} of {{Risk}}},
  author = {Bernoulli, Daniel},
  date = {1954},
  journaltitle = {Econometrica},
  volume = {22},
  number = {1},
  eprint = {1909829},
  eprinttype = {jstor},
  pages = {23--36},
  issn = {0012-9682},
  doi = {10.2307/1909829},
  url = {https://www.jstor.org/stable/1909829},
  urldate = {2019-09-18}
}

@unpublished{berradaDeepFrankWolfeNeural2018,
  title = {Deep {{Frank-Wolfe For Neural Network Optimization}}},
  author = {Berrada, Leonard and Zisserman, Andrew and Kumar, M. Pawan},
  date = {2018-11-19},
  eprint = {1811.07591},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.07591},
  urldate = {2018-11-21},
  abstract = {Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent (SGD) algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe (FW) algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while converging faster. The code is publicly available at https://github.com/oval-group/dfw.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6GQ8M8C4\\Berrada et al_2018_Deep Frank-Wolfe For Neural Network Optimization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FNC9UZ5U\\1811.html}
}

@unpublished{berthelotMixMatchHolisticApproach2019,
  title = {{{MixMatch}}: {{A Holistic Approach}} to {{Semi-Supervised Learning}}},
  shorttitle = {{{MixMatch}}},
  author = {Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin},
  date = {2019-05-06},
  eprint = {1905.02249},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.02249},
  urldate = {2019-05-27},
  abstract = {Semi-supervised learning has proven to be a powerful paradigm for leveraging unlabeled data to mitigate the reliance on large labeled datasets. In this work, we unify the current dominant approaches for semi-supervised learning to produce a new algorithm, MixMatch, that works by guessing low-entropy labels for data-augmented unlabeled examples and mixing labeled and unlabeled data using MixUp. We show that MixMatch obtains state-of-the-art results by a large margin across many datasets and labeled data amounts. For example, on CIFAR-10 with 250 labels, we reduce error rate by a factor of 4 (from 38\% to 11\%) and by a factor of 2 on STL-10. We also demonstrate how MixMatch can help achieve a dramatically better accuracy-privacy trade-off for differential privacy. Finally, we perform an ablation study to tease apart which components of MixMatch are most important for its success.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ED8ALPPN\\Berthelot et al_2019_MixMatch.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\EIUQEL4P\\1905.html}
}

@article{bickertonQuantifyingChemicalBeauty2012,
  ids = {bickertonQuantifyingChemicalBeauty2012a},
  title = {Quantifying the Chemical Beauty of Drugs},
  author = {Bickerton, G. Richard and Paolini, Gaia V. and Besnard, Jérémy and Muresan, Sorel and Hopkins, Andrew L.},
  date = {2012-01-24},
  journaltitle = {Nat Chem},
  volume = {4},
  number = {2},
  eprint = {22270643},
  eprinttype = {pmid},
  pages = {90--98},
  issn = {1755-4349},
  doi = {10.1038/nchem.1243},
  abstract = {Drug-likeness is a key consideration when selecting compounds during the early stages of drug discovery. However, evaluation of drug-likeness in absolute terms does not reflect adequately the whole spectrum of compound quality. More worryingly, widely used rules may inadvertently foster undesirable molecular property inflation as they permit the encroachment of rule-compliant compounds towards their boundaries. We propose a measure of drug-likeness based on the concept of desirability called the quantitative estimate of drug-likeness (QED). The empirical rationale of QED reflects the underlying distribution of molecular properties. QED is intuitive, transparent, straightforward to implement in many practical settings and allows compounds to be ranked by their relative merit. We extended the utility of QED by applying it to the problem of molecular target druggability assessment by prioritizing a large set of published bioactive compounds. The measure may also capture the abstract notion of aesthetics in medicinal chemistry.},
  langid = {english},
  pmcid = {PMC3524573},
  keywords = {Empirical Research,Pharmaceutical Preparations},
  file = {C:\Users\USEBPERP\Zotero\storage\EEUSZPC3\Bickerton et al_2012_Quantifying the chemical beauty of drugs.pdf}
}

@article{bilodeauGenerativeModelsMolecular2022,
  title = {Generative Models for Molecular Discovery: {{Recent}} Advances and Challenges},
  shorttitle = {Generative Models for Molecular Discovery},
  author = {Bilodeau, Camille and Jin, Wengong and Jaakkola, Tommi and Barzilay, Regina and Jensen, Klavs F.},
  date = {2022},
  journaltitle = {WIREs Computational Molecular Science},
  volume = {12},
  number = {5},
  pages = {e1608},
  issn = {1759-0884},
  doi = {10.1002/wcms.1608},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcms.1608},
  urldate = {2024-01-05},
  abstract = {Development of new products often relies on the discovery of novel molecules. While conventional molecular design involves using human expertise to propose, synthesize, and test new molecules, this process can be cost and time intensive, limiting the number of molecules that can be reasonably tested. Generative modeling provides an alternative approach to molecular discovery by reformulating molecular design as an inverse design problem. Here, we review the recent advances in the state-of-the-art of generative molecular design and discusses the considerations for integrating these models into real molecular discovery campaigns. We first review the model design choices required to develop and train a generative model including common 1D, 2D, and 3D representations of molecules and typical generative modeling neural network architectures. We then describe different problem statements for molecular discovery applications and explore the benchmarks used to evaluate models based on those problem statements. Finally, we discuss the important factors that play a role in integrating generative models into experimental workflows. Our aim is that this review will equip the reader with the information and context necessary to utilize generative modeling within their domain. This article is categorized under: Data Science {$>$} Artificial Intelligence/Machine Learning},
  langid = {english},
  keywords = {generative adversarial networks,generative models,molecular representation,normalizing flow models,variational autoencoders},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\CLKLARRG\\Bilodeau et al. - 2022 - Generative models for molecular discovery Recent .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WZWD7MR8\\wcms.html}
}

@article{bilodeauGenerativeModelsMolecular2022a,
  title = {Generative Models for Molecular Discovery: {{Recent}} Advances and Challenges},
  shorttitle = {Generative Models for Molecular Discovery},
  author = {Bilodeau, Camille and Jin, Wengong and Jaakkola, Tommi and Barzilay, Regina and Jensen, Klavs F.},
  date = {2022},
  journaltitle = {WIREs Computational Molecular Science},
  volume = {12},
  number = {5},
  pages = {e1608},
  issn = {1759-0884},
  doi = {10.1002/wcms.1608},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcms.1608},
  urldate = {2023-10-10},
  abstract = {Development of new products often relies on the discovery of novel molecules. While conventional molecular design involves using human expertise to propose, synthesize, and test new molecules, this process can be cost and time intensive, limiting the number of molecules that can be reasonably tested. Generative modeling provides an alternative approach to molecular discovery by reformulating molecular design as an inverse design problem. Here, we review the recent advances in the state-of-the-art of generative molecular design and discusses the considerations for integrating these models into real molecular discovery campaigns. We first review the model design choices required to develop and train a generative model including common 1D, 2D, and 3D representations of molecules and typical generative modeling neural network architectures. We then describe different problem statements for molecular discovery applications and explore the benchmarks used to evaluate models based on those problem statements. Finally, we discuss the important factors that play a role in integrating generative models into experimental workflows. Our aim is that this review will equip the reader with the information and context necessary to utilize generative modeling within their domain. This article is categorized under: Data Science {$>$} Artificial Intelligence/Machine Learning},
  langid = {english},
  keywords = {generative adversarial networks,generative models,molecular representation,normalizing flow models,variational autoencoders},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2FYRZEEH\\Bilodeau et al. - 2022 - Generative models for molecular discovery Recent .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XBBHH9FV\\wcms.html}
}

@unpublished{biNonautoregressiveElectronFlow2020,
  title = {Non-Autoregressive Electron Flow Generation for Reaction Prediction},
  author = {Bi, Hangrui and Wang, Hengyi and Shi, Chence and Tang, Jian},
  date = {2020-12-16},
  eprint = {2012.12124},
  eprinttype = {arxiv},
  eprintclass = {physics},
  url = {http://arxiv.org/abs/2012.12124},
  urldate = {2021-04-18},
  abstract = {Reaction prediction is a fundamental problem in computational chemistry. Existing approaches typically generate a chemical reaction by sampling tokens or graph edits sequentially, conditioning on previously generated outputs. These autoregressive generating methods impose an arbitrary ordering of outputs and prevent parallel decoding during inference. We devise a novel decoder that avoids such sequential generating and predicts the reaction in a Non-Autoregressive manner. Inspired by physical-chemistry insights, we represent edge edits in a molecule graph as electron flows, which can then be predicted in parallel. To capture the uncertainty of reactions, we introduce latent variables to generate multi-modal outputs. Following previous works, we evaluate our model on USPTO MIT dataset. Our model achieves both an order of magnitude lower inference latency, with state-of-the-art top-1 accuracy and comparable performance on Top-K sampling.},
  version = {1},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics}
}

@book{bishopPatternRecognitionMachine2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher},
  date = {2006},
  series = {Information {{Science}} and {{Statistics}}},
  publisher = {Springer-Verlag},
  location = {New York},
  url = {https://www.springer.com/gp/book/9780387310732},
  urldate = {2019-09-18},
  abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook "Neural Networks for Pattern Recognition" has been widely adopted. Coming soon: *For students, worked solutions to a subset of exercises available on a public web site (for exercises marked "www" in the text) *For instructors, worked solutions to remaining exercises from the Springer web site *Lecture slides to accompany each chapter *Data sets available for download},
  isbn = {978-0-387-31073-2},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\85MVVC3Z\9780387310732.html}
}

@article{bjerrumArtificialApplicabilityLabels2020,
  ids = {bjerrum2020artificialb},
  title = {Artificial Applicability Labels for Improving Policies in Retrosynthesis Prediction},
  author = {Bjerrum, Esben Jannik and Thakkar, Amol and Engkvist, Ola},
  date = {2020-12},
  journaltitle = {Mach. Learn.: Sci. Technol.},
  volume = {2},
  number = {1},
  pages = {017001},
  publisher = {IOP Publishing},
  issn = {2632-2153},
  doi = {10.1088/2632-2153/abcf90},
  url = {https://doi.org/10.1088/2632-2153/abcf90},
  urldate = {2021-04-08},
  abstract = {Automated retrosynthetic planning algorithms are a research area of increasing importance. Automated reaction-template extraction from large datasets, in conjunction with neural-network-enhanced tree-search algorithms, can find plausible routes to target compounds in seconds. However, the current method for training neural networks to predict suitable templates for a given target product leads to many predictions that are not applicable in silico. Most templates in the top 50 suggested templates cannot be applied to the target molecule to perform the virtual reaction. Here, we describe how to generate data and train a neural network policy that predicts whether templates are applicable or not. First, we generate a massive training dataset by applying each retrosynthetic template to each product from our reaction database. Second, we train a neural network to perform near-perfect prediction of the applicability labels on a held-out test set. The trained network is then joined with a policy model trained to predict and prioritize templates using the labels from the original dataset. The combined model was found to outperform the policy model in a route-finding task using 1700 compounds from our internal drug-discovery projects.},
  issue = {1},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\83BMWH3L\Bjerrum et al. - 2020 - Artificial applicability labels for improving poli.pdf}
}

@article{bjerrumArtificialApplicabilityLabels2020a,
  title = {Artificial {{Applicability Labels}} for {{Improving Policies}} in {{Retrosynthesis Prediction}}},
  author = {Bjerrum, Esben Jannik and Thakkar, Amol and Engkvist, Ola},
  date = {2020-05-11},
  publisher = {ChemRxiv},
  doi = {10.26434/chemrxiv.12249458.v1},
  url = {/articles/preprint/Artificial_Applicability_Labels_for_Improving_Policies_in_Retrosynthesis_Prediction/12249458/1},
  urldate = {2021-01-20},
  abstract = {Automated retrosynthetic planning algorithms are a research area of increased importance. Automated reaction template extraction from large datasets in conjunction with neural network enhanced tree search algorithms can find plausible routes to target compounds in seconds. However, the current way of training the neural networks to predict suitable templates for a given target product, leads to many predictions which are not applicable in silico. Most templates in the top-50 suggested templates can’t be applied to the target molecule to perform the virtual reaction. Here we describe how to generate data and train a neural network policy that predicts if templates are applicable or not. First, we generate a massive training dataset by applying each retrosynthetic template to each product from our reaction database. Second, we train a neural network to near perfect prediction of the applicability labels on a held-out test set. The trained network is then joined with a policy model trained to predict and prioritize templates using the labels from the original dataset. The combined model was found to outperform the policy model in a route-finding task using 1700 compounds from our internal drug discovery projects.},
  langid = {english}
}

@article{bjerrumFasterMoreDiverse2023,
  title = {Faster and More Diverse de Novo Molecular Optimization with Double-Loop Reinforcement Learning Using Augmented {{SMILES}}},
  author = {Bjerrum, Esben Jannik and Margreitter, Christian and Blaschke, Thomas and family=Castro, given=Raquel López-Ríos, prefix=de, useprefix=true},
  date = {2023-08},
  journaltitle = {J Comput Aided Mol Des},
  volume = {37},
  number = {8},
  eprint = {2210.12458},
  eprinttype = {arxiv},
  eprintclass = {physics},
  pages = {373--394},
  issn = {0920-654X, 1573-4951},
  doi = {10.1007/s10822-023-00512-6},
  url = {http://arxiv.org/abs/2210.12458},
  urldate = {2023-07-21},
  abstract = {Using generative deep learning models and reinforcement learning together can effectively generate new molecules with desired properties. By employing a multi-objective scoring function, thousands of high-scoring molecules can be generated, making this approach useful for drug discovery and material science. However, the application of these methods can be hindered by computationally expensive or time-consuming scoring procedures, particularly when a large number of function calls are required as feedback in the reinforcement learning optimization. Here, we propose the use of double-loop reinforcement learning with simplified molecular line entry system (SMILES) augmentation to improve the efficiency and speed of the optimization. By adding an inner loop that augments the generated SMILES strings to non-canonical SMILES for use in additional reinforcement learning rounds, we can both reuse the scoring calculations on the molecular level, thereby speeding up the learning process, as well as offer additional protection against mode collapse. We find that employing between 5 and 10 augmentation repetitions is optimal for the scoring functions tested and is further associated with an increased diversity in the generated compounds, improved reproducibility of the sampling runs and the generation of molecules of higher similarity to known ligands.},
  keywords = {68T07,Computer Science - Machine Learning,I.2.1,J.3,Physics - Chemical Physics},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\C4EP7H4U\\Bjerrum et al. - 2023 - Faster and more diverse de novo molecular optimiza.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\5SNP6Q4X\\2210.html}
}

@article{bjerrumImprovingChemicalAutoencoder2018,
  title = {Improving {{Chemical Autoencoder Latent Space}} and {{Molecular De Novo Generation Diversity}} with {{Heteroencoders}}},
  author = {Bjerrum, Esben Jannik and Sattarov, Boris},
  date = {2018-10-30},
  journaltitle = {Biomolecules},
  volume = {8},
  number = {4},
  eprint = {30380783},
  eprinttype = {pmid},
  issn = {2218-273X},
  doi = {10.3390/biom8040131},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6316879/},
  urldate = {2019-05-06},
  abstract = {Chemical autoencoders are attractive models as they combine chemical space navigation with possibilities for de novo molecule generation in areas of interest. This enables them to produce focused chemical libraries around a single lead compound for employment early in a drug discovery project. Here, it is shown that the choice of chemical representation, such as strings from the simplified molecular-input line-entry system (SMILES), has a large influence on the properties of the latent space. It is further explored to what extent translating between different chemical representations influences the latent space similarity to the SMILES strings or circular fingerprints. By employing SMILES enumeration for either the encoder or decoder, it is found that the decoder has the largest influence on the properties of the latent space. Training a sequence to sequence heteroencoder based on recurrent neural networks (RNNs) with long short-term memory cells (LSTM) to predict different enumerated SMILES strings from the same canonical SMILES string gives the largest similarity between latent space distance and molecular similarity measured as circular fingerprints similarity. Using the output from the code layer in quantitative structure activity relationship (QSAR) of five molecular datasets shows that heteroencoder derived vectors markedly outperforms autoencoder derived vectors as well as models built using ECFP4 fingerprints, underlining the increased chemical relevance of the latent space. However, the use of enumeration during training of the decoder leads to a marked increase in the rate of decoding to different molecules than encoded, a tendency that can be counteracted with more complex network architectures.},
  pmcid = {PMC6316879},
  file = {C:\Users\USEBPERP\Zotero\storage\JEIXZRBB\Bjerrum_Sattarov_2018_Improving Chemical Autoencoder Latent Space and Molecular De Novo Generation.pdf}
}

@unpublished{bjerrumMolecularGenerationRecurrent2017,
  title = {Molecular {{Generation}} with {{Recurrent Neural Networks}} ({{RNNs}})},
  author = {Bjerrum, Esben Jannik and Threlfall, Richard},
  date = {2017-05-12},
  eprint = {1705.04612},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/1705.04612},
  urldate = {2018-09-24},
  abstract = {The potential number of drug like small molecules is estimated to be between 10\textasciicircum 23 and 10\textasciicircum 60 while current databases of known compounds are orders of magnitude smaller with approximately 10\textasciicircum 8 compounds. This discrepancy has led to an interest in generating virtual libraries using hand crafted chemical rules and fragment based methods to cover a larger area of chemical space and generate chemical libraries for use in in silico drug discovery endeavors. Here it is explored to what extent a recurrent neural network with long short term memory cells can figure out sensible chemical rules and generate synthesizable molecules by being trained on existing compounds encoded as SMILES. The networks can to a high extent generate novel, but chemically sensible molecules. The properties of the molecules are tuned by training on two different datasets consisting of fragment like molecules and drug like molecules. The produced molecules and the training databases have very similar distributions of molar weight, predicted logP, number of hydrogen bond acceptors and donors, number of rotatable bonds and topological polar surface area when compared to their respective training sets. The compounds are for the most cases synthesizable as assessed with SA score and Wiley ChemPlanner.},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\I9HAPM9Y\\Bjerrum_Threlfall_2017_Molecular Generation with Recurrent Neural Networks (RNNs)2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QCF4ZNVI\\Bjerrum_Threlfall_2017_Molecular Generation with Recurrent Neural Networks (RNNs).pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DFCYTNUP\\1705.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\F4R2T62K\\1705.html}
}

@article{blaschkeApplicationGenerativeAutoencoder2018,
  ids = {blaschke2017Application},
  title = {Application of {{Generative Autoencoder}} in {{De Novo Molecular Design}}},
  author = {Blaschke, Thomas and Olivecrona, Marcus and Engkvist, Ola and Bajorath, Jürgen and Chen, Hongming},
  date = {2018},
  journaltitle = {Molecular Informatics},
  volume = {37},
  number = {1-2},
  eprint = {1711.07839},
  eprinttype = {arxiv},
  pages = {1700123},
  issn = {1868-1751},
  doi = {10.1002/minf.201700123},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/minf.201700123},
  urldate = {2020-04-20},
  abstract = {A major challenge in computational chemistry is the generation of novel molecular structures with desirable pharmacological and physiochemical properties. In this work, we investigate the potential use of autoencoder, a deep learning methodology, for de novo molecular design. Various generative autoencoders were used to map molecule structures into a continuous latent space and vice versa and their performance as structure generator was assessed. Our results show that the latent space preserves chemical similarity principle and thus can be used for the generation of analogue structures. Furthermore, the latent space created by autoencoders were searched systematically to generate novel compounds with predicted activity against dopamine receptor type 2 and compounds similar to known active compounds not included in the trainings set were identified.},
  langid = {english},
  keywords = {Autoencoder,chemoinformatics,Computer Science - Learning,Computer Science - Machine Learning,de novo molecular design,deep learning,inverse QSAR,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\8DPADUXW\\Blaschke et al_2017_Application of generative autoencoder in de novo molecular design2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GM9S4M68\\Blaschke et al_2017_Application of generative autoencoder in de novo molecular design.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\K6FMPKL3\\Blaschke et al_2018_Application of Generative Autoencoder in De Novo Molecular Design.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VKKFKLGP\\Blaschke et al_2018_Application of Generative Autoencoder in De Novo Molecular Design.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6TXZT5LT\\minf.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\9WTNKP7J\\minf.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\FATHGJ8G\\1711.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\J93MLTAW\\1711.html}
}

@article{blaschkeMemoryassistedReinforcementLearning2020,
  title = {Memory-Assisted Reinforcement Learning for Diverse Molecular de Novo Design},
  author = {Blaschke, Thomas and Engkvist, Ola and Bajorath, Jürgen and Chen, Hongming},
  date = {2020-12},
  journaltitle = {J Cheminform},
  volume = {12},
  number = {1},
  pages = {68},
  issn = {1758-2946},
  doi = {10.1186/s13321-020-00473-0},
  url = {https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00473-0},
  urldate = {2022-05-11},
  abstract = {In de novo molecular design, recurrent neural networks (RNN) have been shown to be effective methods for sampling and generating novel chemical structures. Using a technique called reinforcement learning (RL), an RNN can be tuned to target a particular section of chemical space with optimized desirable properties using a scoring function. However, ligands generated by current RL methods so far tend to have relatively low diversity, and sometimes even result in duplicate structures when optimizing towards desired properties. Here, we propose a new method to address the low diversity issue in RL for molecular design. Memory-assisted RL is an extension of the known RL, with the introduction of a so-called memory unit. As proof of concept, we applied our method to generate structures with a desired AlogP value. In a second case study, we applied our method to design ligands for the dopamine type 2 receptor and the 5-hydroxytryptamine type 1A receptor. For both receptors, a machine learning model was developed to predict whether generated molecules were active or not for the receptor. In both case studies, it was found that memoryassisted RL led to the generation of more compounds predicted to be active having higher chemical diversity, thus achieving better coverage of chemical space of known ligands compared to established RL methods.},
  langid = {english},
  keywords = {De Novo design,Deep learning applications,Exploration strategy,Recurrent neural networks,Reinforcement learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4H8YJ7SE\\Blaschke et al. - 2020 - Memory-assisted reinforcement learning for diverse.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BWFDGS9Z\\Blaschke et al. - 2020 - Memory-assisted reinforcement learning for diverse.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JZ5DGL7I\\Blaschke et al. - 2020 - Memory-assisted reinforcement learning for diverse.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\EXH5TRBQ\\s13321-020-00473-0.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\UE2IRLM9\\s13321-020-00473-0.html}
}

@article{blaschkeREINVENTAITool2020,
  title = {{{REINVENT}} 2.0: {{An AI Tool}} for {{De Novo Drug Design}}},
  shorttitle = {{{REINVENT}} 2.0},
  author = {Blaschke, Thomas and Arús-Pous, Josep and Chen, Hongming and Margreitter, Christian and Tyrchan, Christian and Engkvist, Ola and Papadopoulos, Kostas and Patronov, Atanas},
  date = {2020-12-28},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {60},
  number = {12},
  pages = {5918--5922},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.0c00915},
  url = {https://doi.org/10.1021/acs.jcim.0c00915},
  urldate = {2024-01-05},
  abstract = {In the past few years, we have witnessed a renaissance of the field of molecular de novo drug design. The advancements in deep learning and artificial intelligence (AI) have triggered an avalanche of ideas on how to translate such techniques to a variety of domains including the field of drug design. A range of architectures have been devised to find the optimal way of generating chemical compounds by using either graph- or string (SMILES)-based representations. With this application note, we aim to offer the community a production-ready tool for de novo design, called REINVENT. It can be effectively applied on drug discovery projects that are striving to resolve either exploration or exploitation problems while navigating the chemical space. It can facilitate the idea generation process by bringing to the researcher’s attention the most promising compounds. REINVENT’s code is publicly available at https://github.com/MolecularAI/Reinvent.},
  file = {C:\Users\USEBPERP\Zotero\storage\WIN2AJ7Z\Blaschke et al. - 2020 - REINVENT 2.0 An AI Tool for De Novo Drug Design.pdf}
}

@article{blaschkeREINVENTAITool2020a,
  title = {{{REINVENT}} 2.0: {{An AI Tool}} for {{De Novo Drug Design}}},
  shorttitle = {{{REINVENT}} 2.0},
  author = {Blaschke, Thomas and Arús-Pous, Josep and Chen, Hongming and Margreitter, Christian and Tyrchan, Christian and Engkvist, Ola and Papadopoulos, Kostas and Patronov, Atanas},
  date = {2020-10-29},
  journaltitle = {Journal of Chemical Information and Modeling},
  publisher = {American Chemical Society},
  doi = {10.1021/acs.jcim.0c00915},
  url = {https://pubs.acs.org/doi/pdf/10.1021/acs.jcim.0c00915},
  urldate = {2023-07-18},
  abstract = {In the past few years, we have witnessed a renaissance of the field of molecular de novo drug design. The advancements in deep learning and artificial intelligence (AI) have triggered an avalanche of ideas on how to translate such techniques to a variety of domains including the field of drug design. A range of architectures have been devised to find the optimal way of generating chemical compounds by using either graph- or string (SMILES)-based representations. With this application note, we aim to offer the community a production-ready tool for de novo design, called REINVENT. It can be effectively applied on drug discovery projects that are striving to resolve either exploration or exploitation problems while navigating the chemical space. It can facilitate the idea generation process by bringing to the researcher’s attention the most promising compounds. REINVENT’s code is publicly available at https://github.com/MolecularAI/Reinvent.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\L9X4PTAT\Blaschke et al. - 2020 - REINVENT 2.0 An AI Tool for De Novo Drug Design.pdf}
}

@article{blastlandFiveRulesEvidence2020,
  title = {Five Rules for Evidence Communication},
  author = {Blastland, Michael and Freeman, Alexandra L. J. and family=Linden, given=Sander, prefix=van der, useprefix=false and Marteau, Theresa M. and Spiegelhalter, David},
  date = {2020-11},
  journaltitle = {Nature},
  volume = {587},
  number = {7834},
  pages = {362--364},
  publisher = {Nature Publishing Group},
  doi = {10.1038/d41586-020-03189-1},
  url = {https://www.nature.com/articles/d41586-020-03189-1},
  urldate = {2020-12-19},
  abstract = {Avoid unwarranted certainty, neat narratives and partisan presentation; strive to inform, not persuade.},
  issue = {7834},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2ZJAMIEZ\\Blastland et al. - 2020 - Five rules for evidence communication.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\3D52ZWW8\\d41586-020-03189-1.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\7KCQ9RPN\\d41586-020-03189-1.html}
}

@online{blazquez-garciaReviewOutlierAnomaly2020,
  title = {A Review on Outlier/Anomaly Detection in Time Series Data},
  author = {Blázquez-García, Ane and Conde, Angel and Mori, Usue and Lozano, Jose A.},
  date = {2020-02-11},
  eprint = {2002.04236},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2002.04236},
  url = {http://arxiv.org/abs/2002.04236},
  urldate = {2023-01-06},
  abstract = {Recent advances in technology have brought major breakthroughs in data collection, enabling a large amount of data to be gathered over time and thus generating time series. Mining this data has become an important task for researchers and practitioners in the past few years, including the detection of outliers or anomalies that may represent errors or events of interest. This review aims to provide a structured and comprehensive state-of-the-art on outlier detection techniques in the context of time series. To this end, a taxonomy is presented based on the main aspects that characterize an outlier detection technique.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\TDJDNIJM\\Blázquez-García et al. - 2020 - A review on outlieranomaly detection in time seri.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AGHEEEVS\\2002.html}
}

@article{blumFoundationsDataScience,
  title = {Foundations of {{Data Science}}},
  author = {Blum, Avrim and Hopcroft, John and Kannan, Ravindran},
  pages = {479},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\K52TM9Q5\Blum et al_Foundations of Data Science.pdf}
}

@online{bockVerschluesselungEUParlamentarierTeils2020,
  title = {Verschlüsselung: EU-Parlamentarier teils vehement gegen Vorstoß},
  shorttitle = {Verschlüsselung},
  author = {{bock} and {ORF.at} and Brüssel, aus},
  date = {2020-11-09},
  url = {https://orf.at/stories/3189145/},
  urldate = {2020-11-09},
  langid = {ngerman},
  organization = {news.ORF.at},
  file = {C:\Users\USEBPERP\Zotero\storage\MF7KECWB\3189145.html}
}

@article{bogevigRouteDesign21st2015,
  title = {Route {{Design}} in the 21st {{Century}}: {{The ICSYNTH Software Tool}} as an {{Idea Generator}} for {{Synthesis Prediction}}},
  shorttitle = {Route {{Design}} in the 21st {{Century}}},
  author = {Bøgevig, Anders and Federsel, Hans-Jürgen and Huerta, Fernando and Hutchings, Michael G. and Kraut, Hans and Langer, Thomas and Löw, Peter and Oppawsky, Christoph and Rein, Tobias and Saller, Heinz},
  date = {2015-02-20},
  journaltitle = {Org. Process Res. Dev.},
  volume = {19},
  number = {2},
  pages = {357--368},
  issn = {1083-6160},
  doi = {10.1021/op500373e},
  url = {https://doi.org/10.1021/op500373e},
  urldate = {2019-01-15},
  abstract = {The new computer-aided synthesis design tool ICSYNTH has been evaluated by comparing its performance in predicting new ideas for route design to that of historical brainstorm results on a series of commercial pharmaceutical targets, as well as literature data. Examples of its output as an idea generator are described, and the conclusion is that it adds appreciable value to the performance of the professional drug research and development chemist team.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\MIBR6WW9\\Bøgevig et al_2015_Route Design in the 21st Century.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GIFFPWQP\\op500373e.html}
}

@misc{boghogDrugDiscoveryCycle2015,
  title = {Drug Discovery Cycle},
  author = {Boghog},
  date = {2015-06-22},
  url = {https://commons.wikimedia.org/wiki/File:Drug_discovery_cycle.svg}
}

@article{boitreaudOptiMolOptimizationBinding2020,
  title = {{{OptiMol}} : {{Optimization}} of {{Binding Affinities}} in {{Chemical Space}} for {{Drug Discovery}}},
  shorttitle = {{{OptiMol}}},
  author = {Boitreaud, Jacques and Mallet, Vincent and Oliver, Carlos and Waldispühl, Jérôme},
  date = {2020-12-28},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {60},
  number = {12},
  pages = {5658--5666},
  issn = {1549-9596, 1549-960X},
  doi = {10.1021/acs.jcim.0c00833},
  url = {https://pubs.acs.org/doi/10.1021/acs.jcim.0c00833},
  urldate = {2022-07-12},
  abstract = {Ligand-based drug design has recently benefited from the development of deep generative models. These models enable extensive explorations of the chemical space and provide a platform for molecular optimization. However, the vast majority of current methods does not leverage the structure of the binding target, which potentiates the binding of small molecules and plays a key role in the interaction. We propose an optimization pipeline that leverages complementary structure-based and ligand-based methods. Instead of performing docking on a fixed chemical library, we iteratively select promising compounds in the full chemical space using a ligand-centered generative model. Molecular docking is then used as an oracle to guide compound optimization. This allows for iterative generation of compounds that fit the target structure better and better, without prior knowledge about bioactives. For this purpose, we introduce a new graph to Selfies Variational Autoencoder (VAE) which benefits from an 18-fold faster decoding than the graph to graph state of the art, while achieving a similar performance. We then successfully optimize the generation of molecules toward high docking scores, enabling a 10-fold enrichment of high-scoring compounds found with a fixed computational cost.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ET5W4QNF\\Boitreaud et al. - 2020 - OptiMol Optimization of Binding Affinities in Che.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TIUAKTEQ\\Boitreaud et al. - 2020 - OptiMol  Optimization of Binding Affinities in Ch.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XYQDDKGB\\Boitreaud et al. - 2020 - OptiMol  Optimization of Binding Affinities in Ch.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\B2JDCAG5\\acs.jcim.html}
}

@article{boniolUnsupervisedScalableSubsequence2021,
  title = {Unsupervised and Scalable Subsequence Anomaly Detection in Large Data Series},
  author = {Boniol, Paul and Linardi, Michele and Roncallo, Federico and Palpanas, Themis and Meftah, Mohammed and Remy, Emmanuel},
  date = {2021-11-01},
  journaltitle = {The VLDB Journal},
  volume = {30},
  number = {6},
  pages = {909--931},
  issn = {0949-877X},
  doi = {10.1007/s00778-021-00655-8},
  url = {https://doi.org/10.1007/s00778-021-00655-8},
  urldate = {2023-01-30},
  abstract = {Subsequence anomaly (or outlier) detection in long sequences is an important problem with applications in a wide range of domains. However, the approaches that have been proposed so far in the literature have severe limitations: they either require prior domain knowledge or become cumbersome and expensive to use in situations with recurrent anomalies of the same type. In this work, we address these problems and propose NormA, a novel approach, suitable for domain-agnostic anomaly detection. NormA is based on a new data series primitive, which permits to detect anomalies based on their (dis)similarity to a model that represents normal behavior. The experimental results on several real datasets demonstrate that the proposed approach correctly identifies all single and recurrent anomalies of various types, with no prior knowledge of the characteristics of these anomalies (except for their length). Moreover, it outperforms by a large margin the current state-of-the art algorithms in terms of accuracy, while being orders of magnitude faster.},
  langid = {english}
}

@article{boppanaApproximatingMaximumIndependent1992,
  title = {Approximating Maximum Independent Sets by Excluding Subgraphs},
  author = {Boppana, Ravi and Halldórsson, Magnús M.},
  date = {1992-06},
  journaltitle = {BIT},
  volume = {32},
  number = {2},
  pages = {180--196},
  issn = {0006-3835, 1572-9125},
  doi = {10.1007/BF01994876},
  url = {http://link.springer.com/10.1007/BF01994876},
  urldate = {2022-07-28},
  langid = {english}
}

@incollection{borgesAnomalyDetectionTime2021,
  title = {Anomaly {{Detection}} in {{Time Series}}},
  booktitle = {Transactions on {{Large-Scale Data-}} and {{Knowledge-Centered Systems L}}},
  author = {Borges, Heraldo and Akbarinia, Reza and Masseglia, Florent},
  editor = {Hameurlain, Abdelkader and Tjoa, A Min},
  date = {2021},
  volume = {12930},
  pages = {46--62},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-64553-6_3},
  url = {https://link.springer.com/10.1007/978-3-662-64553-6_3},
  urldate = {2023-01-19},
  abstract = {Data mining has become an important task for researchers in the past few years, including detecting anomalies that may represent events of interest. The problem of anomaly detection refers to discovering the patterns that do not conform to expected behavior. This paper analyzes recent studies on the detection of anomalies in time series. The goal is to provide an introduction to anomaly detection and a survey of recent research and challenges. The article is divided into three main parts. First, the main concepts are presented. Then, the anomaly detection task is defined. Afterward, the main approaches and strategies to solve the problem are presented.},
  isbn = {978-3-662-64552-9 978-3-662-64553-6},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\6ZMCPZGX\Borges et al. - 2021 - Anomaly Detection in Time Series.pdf}
}

@article{bostromStuckRutOld2016,
  title = {Stuck in a Rut with Old Chemistry},
  author = {Boström, Jonas and Brown, Dean G.},
  date = {2016-05-01},
  journaltitle = {Drug Discovery Today},
  volume = {21},
  number = {5},
  pages = {701--703},
  issn = {1359-6446},
  doi = {10.1016/j.drudis.2016.02.017},
  url = {http://www.sciencedirect.com/science/article/pii/S1359644616300678},
  urldate = {2019-10-24},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\5ZNXKADB\\Boström_Brown_2016_Stuck in a rut with old chemistry.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HGQRVIBH\\S1359644616300678.html}
}

@article{bottouCounterfactualReasoningLearning,
  title = {Counterfactual {{Reasoning}} and {{Learning Systems}}: {{The Example}} of {{Computational Advertising}}},
  author = {Bottou, Léon and Peters, Jonas and Quiñonero-Candela, Joaquin and Charles, Denis X and Chickering, D Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
  pages = {54},
  abstract = {This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\M7MW8V3A\Bottou et al_Counterfactual Reasoning and Learning Systems.pdf}
}

@inproceedings{bottouCuriouslyFastConvergence2009,
  title = {Curiously {{Fast Convergence}} of Some {{Stochastic Gradient Descent Algorithms}}},
  author = {Bottou, Léon},
  date = {2009},
  abstract = {1 Context Given a finite set of m examples z 1 ,. .. , z m and a strictly convex differen-tiable loss function ℓ(z, θ) defined on a parameter vector θ ∈ R d , we are interested in minimizing the cost function min θ C(θ) = 1 m m i=1 ℓ(z i , θ). One way to perform such a minimization is to use a stochastic gradient algorithm. Starting from some initial value θ[1], iteration t consists in picking an example z[t] and applying the stochastic gradient update θ[t + 1] = θ[t] − η t ∂ℓ ∂θ ℓ(z[t], θ[t]) , where the sequence of positive scalars η t satisfies the well known Robbins-Monro conditions t η t = ∞ and t η 2 t {$<$} ∞. We consider three ways to pick the example z[t] at each iteration: • Random Examples are drawn uniformly from the training set at each iteration. • Cycle Examples are picked sequentially from the randomly shuffled training set, that is, z[km + t] = z σ(t) , where σ is a random permuta-• Shuffle Examples are still picked sequentially but the training set is shuffled before each pass, that is, z[km + t] = z σ k (t) , where the σ k are random permutations of \{1, .\vphantom\}},
  keywords = {Algorithm,Convex function,Iteration,Loss function,Maxima and minima,Randomness,Stochastic gradient descent,Test set,Whole Earth 'Lectronic Link},
  file = {C:\Users\USEBPERP\Zotero\storage\KNN9U3JH\Bottou_2009_Curiously Fast Convergence of some Stochastic Gradient Descent Algorithms.pdf}
}

@unpublished{bottouOptimizationMethodsLargeScale2016,
  title = {Optimization {{Methods}} for {{Large-Scale Machine Learning}}},
  author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  date = {2016-06-15},
  eprint = {1606.04838},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1606.04838},
  urldate = {2019-08-26},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\L3KX93NU\\Bottou et al_2016_Optimization Methods for Large-Scale Machine Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GWA7WW3Y\\1606.html}
}

@article{botvinickReinforcementLearningFast2019,
  title = {Reinforcement {{Learning}}, {{Fast}} and {{Slow}}},
  author = {Botvinick, Matthew and Ritter, Sam and Wang, Jane X. and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis},
  date = {2019-04},
  journaltitle = {Trends in Cognitive Sciences},
  pages = {S1364661319300610},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.02.006},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661319300610},
  urldate = {2019-05-03},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\D9IVWHLD\Botvinick et al_2019_Reinforcement Learning, Fast and Slow.pdf}
}

@unpublished{bradburyQuasiRecurrentNeuralNetworks2016,
  title = {Quasi-{{Recurrent Neural Networks}}},
  author = {Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
  date = {2016-11-04},
  eprint = {1611.01576},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.01576},
  urldate = {2019-10-21},
  abstract = {Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep's computation on the previous timestep's output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\H7DULDEP\\Bradbury et al_2016_Quasi-Recurrent Neural Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6TK7PD2Q\\1611.html}
}

@unpublished{bradshawGenerativeModelElectron2018,
  ids = {bradshaw2019generative},
  title = {A {{Generative Model For Electron Paths}}},
  author = {Bradshaw, John and Kusner, Matt J. and Paige, Brooks and Segler, Marwin H. S. and Hernández-Lobato, José Miguel},
  date = {2018-09-27},
  eprint = {1805.10970},
  eprinttype = {arxiv},
  url = {https://openreview.net/forum?id=r1x4BnCqKX},
  urldate = {2019-10-01},
  abstract = {Chemical reactions can be described as the stepwise redistribution of electrons in molecules. As such, reactions are often depicted using},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DCEA7IWZ\\Bradshaw et al. - 2019 - A GENERATIVE MODEL FOR ELECTRON PATHS.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VTPP4RGV\\Bradshaw et al_2018_A Generative Model For Electron Paths.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VP5K6RKY\\forum.html}
}

@online{braeiAnomalyDetectionUnivariate2020,
  title = {Anomaly {{Detection}} in {{Univariate Time-series}}: {{A Survey}} on the {{State-of-the-Art}}},
  shorttitle = {Anomaly {{Detection}} in {{Univariate Time-series}}},
  author = {Braei, Mohammad and Wagner, Sebastian},
  date = {2020-04-01},
  eprint = {2004.00433},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2004.00433},
  url = {http://arxiv.org/abs/2004.00433},
  urldate = {2023-01-06},
  abstract = {Anomaly detection for time-series data has been an important research field for a long time. Seminal work on anomaly detection methods has been focussing on statistical approaches. In recent years an increasing number of machine learning algorithms have been developed to detect anomalies on time-series. Subsequently, researchers tried to improve these techniques using (deep) neural networks. In the light of the increasing number of anomaly detection methods, the body of research lacks a broad comparative evaluation of statistical, machine learning and deep learning methods. This paper studies 20 univariate anomaly detection methods from the all three categories. The evaluation is conducted on publicly available datasets, which serve as benchmarks for time-series anomaly detection. By analyzing the accuracy of each method as well as the computation time of the algorithms, we provide a thorough insight about the performance of these anomaly detection approaches, alongside some general notion of which method is suited for a certain type of data.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ZAP9PQK2\\Braei and Wagner - 2020 - Anomaly Detection in Univariate Time-series A Sur.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8H5MGA29\\2004.html}
}

@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  date = {2001-10-01},
  journaltitle = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  issn = {1573-0565},
  doi = {10.1023/A:1010933404324},
  url = {https://doi.org/10.1023/A:1010933404324},
  urldate = {2019-09-19},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  langid = {english},
  keywords = {classification,ensemble,regression},
  file = {C:\Users\USEBPERP\Zotero\storage\6L7D2NVM\Breiman_2001_Random Forests.pdf}
}

@article{brierVerificationForecastsExpressed1950,
  title = {Verification of Forecasts Expressed in Terms of Probability},
  author = {Brier, Glenn W.},
  date = {1950-01-01},
  journaltitle = {Mon. Wea. Rev.},
  volume = {78},
  number = {1},
  pages = {1--3},
  issn = {0027-0644},
  doi = {10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2},
  url = {https://journals.ametsoc.org/doi/abs/10.1175/1520-0493%281950%29078%3C0001%3AVOFEIT%3E2.0.CO%3B2},
  urldate = {2019-05-06},
  abstract = {No Abstract Available.},
  file = {C:\Users\USEBPERP\Zotero\storage\986EV7JU\1520-0493(1950)0780001VOFEIT2.0.html}
}

@article{brownAnalysisPresentSynthetic2016,
  title = {Analysis of {{Past}} and {{Present Synthetic Methodologies}} on {{Medicinal Chemistry}}: {{Where Have All}} the {{New Reactions Gone}}?},
  shorttitle = {Analysis of {{Past}} and {{Present Synthetic Methodologies}} on {{Medicinal Chemistry}}},
  author = {Brown, Dean G. and Boström, Jonas},
  date = {2016-05-26},
  journaltitle = {J. Med. Chem.},
  volume = {59},
  number = {10},
  pages = {4443--4458},
  issn = {0022-2623},
  doi = {10.1021/acs.jmedchem.5b01409},
  url = {https://doi.org/10.1021/acs.jmedchem.5b01409},
  urldate = {2019-10-24},
  abstract = {An analysis of chemical reactions used in current medicinal chemistry (2014), three decades ago (1984), and in natural product total synthesis has been conducted. The analysis revealed that of the current most frequently used synthetic reactions, none were discovered within the past 20 years and only two in the 1980s and 1990s (Suzuki–Miyaura and Buchwald–Hartwig). This suggests an inherent high bar of impact for new synthetic reactions in drug discovery. The most frequently used reactions were amide bond formation, Suzuki–Miyaura coupling, and SNAr reactions, most likely due to commercial availability of reagents, high chemoselectivity, and a pressure on delivery. We show that these practices result in overpopulation of certain types of molecular shapes to the exclusion of others using simple PMI plots. We hope that these results will help catalyze improvements in integration of new synthetic methodologies as well as new library design.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NPAQ3YV5\\Brown_Boström_2016_Analysis of Past and Present Synthetic Methodologies on Medicinal Chemistry.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\96DQVMFY\\acs.jmedchem.html}
}

@article{brownDescriptorsDiversityAnalysis1996,
  title = {Descriptors for Diversity Analysis},
  author = {Brown, Robert D.},
  date = {1996-12-01},
  journaltitle = {Perspectives in Drug Discovery and Design},
  volume = {7},
  number = {1},
  pages = {31--49},
  issn = {1573-9023},
  doi = {10.1007/BF03380180},
  url = {https://doi.org/10.1007/BF03380180},
  urldate = {2022-08-22},
  abstract = {All diversity analysis methods rely on molecular descriptors to characterize the compound set or library under consideration. Descriptors based on 2D and 3D structure and physical properties have been developed. This paper considers the issues involved in the design of a descriptor, the types of descriptor currently in use for diversity analysis, and methods by which any descriptor may be validated to prove that it is able to distinguish biologically different molecules.},
  langid = {english},
  keywords = {diversity analysis,molecular descriptors,molecular dissimilarity,molecular similarity},
  file = {C:\Users\USEBPERP\Zotero\storage\SMRHR4YZ\Brown - 1996 - Descriptors for diversity analysis.pdf}
}

@article{browneSurveyMonteCarlo2012,
  title = {A {{Survey}} of {{Monte Carlo Tree Search Methods}}},
  author = {Browne, C. B. and Powley, E. and Whitehouse, D. and Lucas, S. M. and Cowling, P. I. and Rohlfshagen, P. and Tavener, S. and Perez, D. and Samothrakis, S. and Colton, S.},
  date = {2012-03},
  journaltitle = {IEEE Transactions on Computational Intelligence and AI in Games},
  volume = {4},
  number = {1},
  pages = {1--43},
  doi = {10.1109/TCIAIG.2012.2186810},
  abstract = {Monte Carlo tree search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarize the results from the key game and nongame domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
  keywords = {Artificial intelligence,Artificial intelligence (AI),bandit-based methods,computer Go,Computers,Decision theory,game search,game theory,Game theory,Games,key game,Markov processes,MCTS research,Monte Carlo methods,Monte Carlo tree search (MCTS),Monte carlo tree search methods,nongame domains,random sampling generality,tree searching,upper confidence bounds (UCB),upper confidence bounds for trees (UCT)},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\J9NQ5Q2V\\Browne et al_2012_A Survey of Monte Carlo Tree Search Methods.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\G9ZXFIF9\\6145622.html}
}

@article{brownGuacaMolBenchmarkingModels2019,
  title = {{{GuacaMol}}: {{Benchmarking Models}} for de {{Novo Molecular Design}}},
  shorttitle = {{{GuacaMol}}},
  author = {Brown, Nathan and Fiscato, Marco and Segler, Marwin H.S. and Vaucher, Alain C.},
  date = {2019-03-25},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {59},
  number = {3},
  pages = {1096--1108},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.8b00839},
  url = {https://doi.org/10.1021/acs.jcim.8b00839},
  urldate = {2019-09-13},
  abstract = {De novo design seeks to generate molecules with required property profiles by virtual design-make-test cycles. With the emergence of deep learning and neural generative models in many application areas, models for molecular design based on neural networks appeared recently and show promising results. However, the new models have not been profiled on consistent tasks, and comparative studies to well-established algorithms have only seldom been performed. To standardize the assessment of both classical and neural models for de novo molecular design, we propose an evaluation framework, GuacaMol, based on a suite of standardized benchmarks. The benchmark tasks encompass measuring the fidelity of the models to reproduce the property distribution of the training sets, the ability to generate novel molecules, the exploration and exploitation of chemical space, and a variety of single and multiobjective optimization tasks. The benchmarking open-source Python code and a leaderboard can be found on https://benevolent.ai/guacamol.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\5QWQD23C\\Brown et al_2019_GuacaMol.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\KB8KCRMA\\acs.jcim.html}
}

@article{brownUseStructureActivity1996,
  title = {Use of {{Structure}}−{{Activity Data To Compare Structure-Based Clustering Methods}} and {{Descriptors}} for {{Use}} in {{Compound Selection}}},
  author = {Brown, Robert D. and Martin, Yvonne C.},
  date = {1996-01-01},
  journaltitle = {J. Chem. Inf. Comput. Sci.},
  volume = {36},
  number = {3},
  pages = {572--584},
  issn = {0095-2338},
  doi = {10.1021/ci9501047},
  url = {https://pubs.acs.org/doi/10.1021/ci9501047},
  urldate = {2022-08-22},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DQST922X\\Brown and Martin - 1996 - Use of Structure−Activity Data To Compare Structur.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LFR36XRD\\Brown and Martin - 1996 - Use of Structure−Activity Data To Compare Structur.pdf}
}

@unpublished{bubeckAdversarialExamplesComputational2018,
  title = {Adversarial Examples from Computational Constraints},
  author = {Bubeck, Sébastien and Price, Eric and Razenshteyn, Ilya},
  date = {2018-05-25},
  eprint = {1805.10204},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.10204},
  urldate = {2019-06-13},
  abstract = {Why are classifiers in high dimension vulnerable to "adversarial" perturbations? We show that it is likely not due to information theoretic limitations, but rather it could be due to computational constraints. First we prove that, for a broad set of classification tasks, the mere existence of a robust classifier implies that it can be found by a possibly exponential-time algorithm with relatively few training examples. Then we give a particular classification task where learning a robust classifier is computationally intractable. More precisely we construct a binary classification task in high dimensional space which is (i) information theoretically easy to learn robustly for large perturbations, (ii) efficiently learnable (non-robustly) by a simple linear separator, (iii) yet is not efficiently robustly learnable, even for small perturbations, by any algorithm in the statistical query (SQ) model. This example gives an exponential separation between classical learning and robust learning in the statistical query model. It suggests that adversarial examples may be an unavoidable byproduct of computational limitations of learning algorithms.},
  keywords = {Computer Science - Computational Complexity,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\G6V5VZ77\\Bubeck et al_2018_Adversarial examples from computational constraints.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7L5KHRZV\\1805.html}
}

@article{buesingWOULDACOULDASHOULDA2019,
  title = {{{WOULDA}}, {{COULDA}}, {{SHOULDA}}: {{COUNTERFACTUALLY-GUIDED POLICY SEARCH}}},
  author = {Buesing, Lars and Weber, Theophane and Zwols, Yori and Racaniere, Sebastien and Guez, Arthur and Lespiau, Jean-Baptiste and Heess, Nicolas},
  date = {2019},
  pages = {15},
  abstract = {Learning policies on data synthesized by models can in principle quench the thirst of reinforcement learning algorithms for large amounts of real experience, which is often costly to acquire. However, simulating plausible experience de novo is a hard problem for many complex environments, often resulting in biases for modelbased policy evaluation and search. Instead of de novo synthesis of data, here we assume logged, real experience and model alternative outcomes of this experience under counterfactual actions, i.e. actions that were not actually taken. Based on this, we propose the Counterfactually-Guided Policy Search (CF-GPS) algorithm for learning policies in POMDPs from off-policy experience. It leverages structural causal models for counterfactual evaluation of arbitrary policies on individual off-policy episodes. CF-GPS can improve on vanilla model-based RL algorithms by making use of available logged data to de-bias model predictions. In contrast to off-policy algorithms based on Importance Sampling which re-weight data, CF-GPS leverages a model to explicitly consider alternative outcomes, allowing the algorithm to make better use of experience data. We find empirically that these advantages translate into improved policy evaluation and search results on a non-trivial grid-world task. Finally, we show that CF-GPS generalizes the previously proposed Guided Policy Search and that reparameterization-based algorithms such Stochastic Value Gradient can be interpreted as counterfactual methods.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\DLJS7MV8\Buesing et al. - 2019 - WOULDA, COULDA, SHOULDA COUNTERFACTUALLY-GUIDED P.pdf}
}

@article{buranyiStaggeringlyProfitableBusiness2017,
  entrysubtype = {newspaper},
  title = {Is the Staggeringly Profitable Business of Scientific Publishing Bad for Science?},
  author = {Buranyi, Stephen},
  date = {2017-06-27T05:00:21},
  journaltitle = {The Guardian},
  issn = {0261-3077},
  url = {https://www.theguardian.com/science/2017/jun/27/profitable-business-scientific-publishing-bad-for-science},
  urldate = {2020-08-05},
  abstract = {The long read: It is an industry like no other, with profit margins to rival Google – and it was created by one of Britain’s most notorious tycoons: Robert Maxwell},
  journalsubtitle = {Science},
  langid = {british},
  keywords = {Books,Peer review and scientific publishing,Publishing,Research,Research funding,Science,Science and nature books},
  file = {C:\Users\USEBPERP\Zotero\storage\Z3PLNFFU\profitable-business-scientific-publishing-bad-for-science.html}
}

@article{butinaUnsupervisedDataBase1999,
  title = {Unsupervised {{Data Base Clustering Based}} on {{Daylight}}'s {{Fingerprint}} and {{Tanimoto Similarity}}: {{A Fast}} and {{Automated Way To Cluster Small}} and {{Large Data Sets}}},
  shorttitle = {Unsupervised {{Data Base Clustering Based}} on {{Daylight}}'s {{Fingerprint}} and {{Tanimoto Similarity}}},
  author = {Butina, Darko},
  date = {1999-07-26},
  journaltitle = {J. Chem. Inf. Comput. Sci.},
  volume = {39},
  number = {4},
  pages = {747--750},
  issn = {0095-2338},
  doi = {10.1021/ci9803381},
  url = {https://pubs.acs.org/doi/10.1021/ci9803381},
  urldate = {2022-06-07},
  abstract = {A new clustering algorithm has been developed, which has the following features: (a) generation of good quality clusters where set similarity within a cluster reflects the Tanimoto level used for clustering and is the only input needed by the clustering algorithm; (b) no requirement for visual inspection of resulting clusters as a quality control, since the combination of standard Daylight fingerprints and similarity at high Tanimoto index produces a very reliable way of grouping together similar molecules; (c) orderindependent algorithm which identifies and sorts by potential cluster centroids; (d) can be fully automated and presented in a user-friendly manner; (e) main clustering program is written in ANSI C and as such is independent of Daylight software and can be used to cluster any type of binary fingerprints on the fastest Unix workstation available.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\PL8UD7RF\Butina - 1999 - Unsupervised Data Base Clustering Based on Dayligh.pdf}
}

@article{butlerMachineLearningMolecular2018,
  title = {Machine Learning for Molecular and Materials Science},
  author = {Butler, Keith T. and Davies, Daniel W. and Cartwright, Hugh and Isayev, Olexandr and Walsh, Aron},
  date = {2018-07},
  journaltitle = {Nature},
  volume = {559},
  number = {7715},
  pages = {547--555},
  issn = {1476-4687},
  doi = {10.1038/s41586-018-0337-2},
  url = {https://www.nature.com/articles/s41586-018-0337-2},
  urldate = {2018-09-24},
  abstract = {Recent progress in machine learning in the chemical sciences and future directions in this field are discussed.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\42UI2BGU\\Butler et al_2018_Machine learning for molecular and materials science.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\CTI3EHAJ\\s41586-018-0337-2.html}
}

@article{buttonAutomatedNovoMolecular2019,
  ids = {button2019automateda},
  title = {Automated de Novo Molecular Design by Hybrid Machine Intelligence and Rule-Driven Chemical Synthesis},
  author = {Button, Alexander and Merk, Daniel and Hiss, Jan A. and Schneider, Gisbert},
  date = {2019-07},
  journaltitle = {Nature Machine Intelligence},
  volume = {1},
  number = {7},
  pages = {307--315},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0067-7},
  url = {https://www.nature.com/articles/s42256-019-0067-7},
  urldate = {2020-05-29},
  abstract = {Chemical creativity in the design of new synthetic chemical entities (NCEs) with drug-like properties has been the domain of medicinal chemists. Here, we explore the capability of a chemistry-savvy machine intelligence to generate synthetically accessible molecules. DINGOS (design of innovative NCEs generated by optimization strategies) is a virtual assembly method that combines a rule-based approach with a machine learning model trained on successful synthetic routes described in chemical patent literature. This unique combination enables a balance between ligand-similarity-based generation of innovative compounds by scaffold hopping and the forward-synthetic feasibility of the designs. In a prospective proof-of-concept application, DINGOS successfully produced sets of de novo designs for four approved drugs that were in agreement with the desired structural and physicochemical properties. Target prediction indicated more than 50\% of the designs to be biologically active. Four selected computer-generated compounds were successfully synthesized in accordance with the synthetic route proposed by DINGOS. The results of this study demonstrate the capability of machine learning models to capture implicit chemical knowledge from chemical reaction data and suggest feasible syntheses of new chemical matter.},
  issue = {7},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\T2HFMBTM\\Button et al_2019_Automated de novo molecular design by hybrid machine intelligence and.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\CW9IRN5L\\s42256-019-0067-7.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\EFRGRFJT\\s42256-019-0067-7.html}
}

@online{buttWhyDidBush,
  title = {Why Did {{Bush}} Go to War in {{Iraq}}?},
  author = {Butt, Ahsan I.},
  url = {https://www.aljazeera.com/opinions/2019/3/20/why-did-bush-go-to-war-in-iraq/},
  urldate = {2020-11-11},
  abstract = {No, it wasn’t because of WMDs, democracy or Iraqi oil. The real reason is much more sinister than that.},
  langid = {english}
}

@article{c.dowsonFrechetDistanceMultivariate1982,
  title = {The {{Fréchet Distance}} between {{Multivariate Normal Distributions}}},
  author = {C. Dowson, D and V. Landau, B},
  date = {1982-09-01},
  journaltitle = {Journal of Multivariate Analysis},
  volume = {12},
  pages = {450--455},
  doi = {10.1016/0047-259X(82)90077-X},
  abstract = {The Fréchet distance between two multivariate normal distributions having means [mu]X, [mu]Y and covariance matrices [Sigma]X, [Sigma]Y is shown to be given by d2 = [mu]X - [mu]Y2 + tr([Sigma]X + [Sigma]Y - 2([Sigma]X[Sigma]Y)1/2). The quantity d0 given by d02 = tr([Sigma]X + [Sigma]Y - 2([Sigma]X[Sigma]Y)1/2) is a natural metric on the space of real covariance matrices of given order.}
}

@article{calikusNoFreeLunch2020,
  title = {No {{Free Lunch But A Cheaper Supper}}: {{A General Framework}} for {{Streaming Anomaly Detection}}},
  shorttitle = {No {{Free Lunch But A Cheaper Supper}}},
  author = {Calikus, Ece and Nowaczyk, Slawomir and Sant'Anna, Anita and Dikmen, Onur},
  date = {2020-10},
  journaltitle = {Expert Systems with Applications},
  volume = {155},
  eprint = {1909.06927},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {113453},
  issn = {09574174},
  doi = {10.1016/j.eswa.2020.113453},
  url = {http://arxiv.org/abs/1909.06927},
  urldate = {2023-02-14},
  abstract = {In recent years, there has been increased research interest in detecting anomalies in temporal streaming data. A variety of algorithms have been developed in the data mining community, which can be divided into two categories (i.e., general and ad hoc). In most cases, general approaches assume the one-size-fits-all solution model where a single anomaly detector can detect all anomalies in any domain. To date, there exists no single general method that has been shown to outperform the others across different anomaly types, use cases and datasets. In this paper, we propose SAFARI, a general framework formulated by abstracting and unifying the fundamental tasks in streaming anomaly detection, which provides a flexible and extensible anomaly detection procedure to overcome the limitations of one-size-fits-all solutions. SAFARI helps to facilitate more elaborate algorithm comparisons by allowing us to isolate the effects of shared and unique characteristics of different algorithms on detection performance. Using SAFARI, we have implemented various anomaly detectors and identified a research gap that motivates us to propose a novel learning strategy in this work. We conducted an extensive evaluation study of 20 detectors that are composed using SAFARI and compared their performances using real-world benchmark datasets with different properties. The results indicate that there is no single superior detector that works well for every case, proving our hypothesis that "there is no free lunch" in the streaming anomaly detection world. Finally, we discuss the benefits and drawbacks of each method in-depth and draw a set of conclusions to guide future users of SAFARI.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DZNHVTXS\\Calikus et al. - 2020 - No Free Lunch But A Cheaper Supper A General Fram.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8FX8JR5F\\1909.html}
}

@unpublished{carliniSecretSharerEvaluating2019,
  ids = {carlini2019secreta},
  title = {The {{Secret Sharer}}: {{Evaluating}} and {{Testing Unintended Memorization}} in {{Neural Networks}}},
  shorttitle = {The {{Secret Sharer}}},
  author = {Carlini, Nicholas and Liu, Chang and Erlingsson, Úlfar and Kos, Jernej and Song, Dawn},
  date = {2019-07-16},
  eprint = {1802.08232},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.08232},
  urldate = {2020-07-11},
  abstract = {This paper describes a testing methodology for quantitatively assessing the risk that rare or unique training-data sequences are unintentionally memorized by generative sequence models---a common type of machine-learning model. Because such models are sometimes trained on sensitive data (e.g., the text of users' private messages), this methodology can benefit privacy by allowing deep-learning practitioners to select means of training that minimize such memorization. In experiments, we show that unintended memorization is a persistent, hard-to-avoid issue that can have serious consequences. Specifically, for models trained without consideration of memorization, we describe new, efficient procedures that can extract unique, secret sequences, such as credit card numbers. We show that our testing strategy is a practical and easy-to-use first line of defense, e.g., by describing its application to quantitatively limit data exposure in Google's Smart Compose, a commercial text-completion neural network trained on millions of users' email messages.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\8K9BECNE\\Carlini et al_2019_The Secret Sharer.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DB94MD6V\\Carlini et al. - 2019 - The Secret Sharer Evaluating and Testing Unintend.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XSHWA2YT\\1802.html}
}

@article{carrenoAnalyzingRareEvent2020,
  title = {Analyzing Rare Event, Anomaly, Novelty and Outlier Detection Terms under the Supervised Classification Framework},
  author = {Carreño, Ander and Inza, Iñaki and Lozano, Jose A.},
  date = {2020-06-01},
  journaltitle = {Artif. Intell. Rev.},
  volume = {53},
  number = {5},
  pages = {3575--3594},
  issn = {0269-2821},
  doi = {10.1007/s10462-019-09771-y},
  url = {https://doi.org/10.1007/s10462-019-09771-y},
  urldate = {2023-01-05},
  abstract = {In recent years, a variety of research areas have contributed to a set of related problems with rare event, anomaly, novelty and outlier detection terms as the main actors. These multiple research areas have created a mix-up between terminology and problems. In some research, similar problems have been named differently; while in some other works, the same term has been used to describe different problems. This confusion between terms and problems causes the repetition of research and hinders the advance of the field. Therefore, a standardization is imperative. The goal of this paper is to underline the differences between each term, and organize the area by looking at all these terms under the umbrella of supervised classification. Therefore, a one-to-one assignment of terms to learning scenarios is proposed. In fact, each learning scenario is associated with the term most frequently used in the literature. In order to validate this proposal, a set of experiments retrieving papers from Google Scholar, ACM Digital Library and IEEE Xplore has been carried out.},
  keywords = {Anomaly detection,Novelty detection,Outlier detection,Rare event detection,Supervised classification}
}

@book{casellaStatisticalInference2002,
  title = {Statistical Inference},
  author = {Casella, George and Berger, Roger L.},
  date = {2002},
  edition = {2nd ed},
  publisher = {Thomson Learning},
  location = {Australia ; Pacific Grove, CA},
  isbn = {978-0-534-24312-8},
  langid = {english},
  pagetotal = {660},
  keywords = {Mathematical statistics,Probabilities},
  file = {C:\Users\USEBPERP\Zotero\storage\H7S97Q42\Casella_Berger_2002_Statistical inference.pdf}
}

@online{chalapathyDeepLearningAnomaly2019,
  title = {Deep {{Learning}} for {{Anomaly Detection}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} for {{Anomaly Detection}}},
  author = {Chalapathy, Raghavendra and Chawla, Sanjay},
  date = {2019-01-23},
  eprint = {1901.03407},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1901.03407},
  url = {http://arxiv.org/abs/1901.03407},
  urldate = {2022-08-16},
  abstract = {Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection. Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness. We have grouped state-of-the-art research techniques into different categories based on the underlying assumptions and approach adopted. Within each category we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior. For each category, we present we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains. Finally, we outline open issues in research and challenges faced while adopting these techniques.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\9SR3BWKA\\Chalapathy and Chawla - 2019 - Deep Learning for Anomaly Detection A Survey.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GYTEPU5S\\Chalapathy and Chawla - 2019 - Deep Learning for Anomaly Detection A Survey.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VWTPYCUX\\1901.html}
}

@article{chamberlinMethodMultipleWorking1965,
  title = {The {{Method}} of {{Multiple Working Hypotheses}}: {{With}} This Method the Dangers of Parental Affection for a Favorite Theory Can Be Circumvented},
  shorttitle = {The {{Method}} of {{Multiple Working Hypotheses}}},
  author = {Chamberlin, T. C.},
  date = {1965-05-07},
  journaltitle = {Science},
  volume = {148},
  number = {3671},
  pages = {754--759},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.148.3671.754},
  url = {http://www.sciencemag.org/cgi/doi/10.1126/science.148.3671.754},
  urldate = {2019-10-07},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\9BMLEYLU\Chamberlin - 1965 - The Method of Multiple Working Hypotheses With th.pdf}
}

@unpublished{chandakLearningActionRepresentations2019,
  title = {Learning {{Action Representations}} for {{Reinforcement Learning}}},
  author = {Chandak, Yash and Theocharous, Georgios and Kostas, James and Jordan, Scott and Thomas, Philip S.},
  date = {2019-05-14},
  eprint = {1902.00183},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.00183},
  urldate = {2020-11-06},
  abstract = {Most model-free reinforcement learning methods leverage state representations (embeddings) for generalization, but either ignore structure in the space of actions or assume the structure is provided a priori. We show how a policy can be decomposed into a component that acts in a low-dimensional space of action representations and a component that transforms these representations into actual actions. These representations improve generalization over large, finite action sets by allowing the agent to infer the outcomes of actions similar to actions already taken. We provide an algorithm to both learn and use action representations and provide conditions for its convergence. The efficacy of the proposed method is demonstrated on large-scale real-world problems.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7KMVCZ63\\Chandak et al_2019_Learning Action Representations for Reinforcement Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\KTS3RNP7\\1902.html}
}

@article{chandolaAnomalyDetectionDiscrete2012,
  title = {Anomaly {{Detection}} for {{Discrete Sequences}}: {{A Survey}}},
  shorttitle = {Anomaly {{Detection}} for {{Discrete Sequences}}},
  author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  date = {2012-05},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {24},
  number = {5},
  pages = {823--839},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2010.235},
  abstract = {This survey attempts to provide a comprehensive and structured overview of the existing research for the problem of detecting anomalies in discrete/symbolic sequences. The objective is to provide a global understanding of the sequence anomaly detection problem and how existing techniques relate to each other. The key contribution of this survey is the classification of the existing research into three distinct categories, based on the problem formulation that they are trying to solve. These problem formulations are: 1) identifying anomalous sequences with respect to a database of normal sequences; 2) identifying an anomalous subsequence within a long sequence; and 3) identifying a pattern in a sequence whose frequency of occurrence is anomalous. We show how each of these problem formulations is characteristically distinct from each other and discuss their relevance in various application domains. We review techniques from many disparate and disconnected application domains that address each of these formulations. Within each problem formulation, we group techniques into categories based on the nature of the underlying algorithm. For each category, we provide a basic anomaly detection technique, and show how the existing techniques are variants of the basic technique. This approach shows how different techniques within a category are related or different from each other. Our categorization reveals new variants and combinations that have not been investigated before for anomaly detection. We also provide a discussion of relative strengths and weaknesses of different techniques. We show how techniques developed for one problem formulation can be adapted to solve a different formulation, thereby providing several novel adaptations to solve the different problem formulations. We also highlight the applicability of the techniques that handle discrete sequences to other related areas such as online anomaly detection and time series anomaly detection.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  keywords = {anomaly detection.,Computers,Databases,Discrete sequences,Hidden Markov models,Operating systems,Postal services,Probabilistic logic,Training},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\3KKQKECS\\Chandola et al. - 2012 - Anomaly Detection for Discrete Sequences A Survey.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\NEU2F3LK\\5645624.html}
}

@article{chandolaAnomalyDetectionSurvey,
  title = {Anomaly {{Detection}} : {{A Survey}}},
  author = {Chandola, Varun},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\IVTJ6BPL\Chandola - Anomaly Detection  A Survey.pdf}
}

@inproceedings{changComputingNearMaximumIndependent2017,
  title = {Computing {{A Near-Maximum Independent Set}} in {{Linear Time}} by {{Reducing-Peeling}}},
  booktitle = {Proceedings of the 2017 {{ACM International Conference}} on {{Management}} of {{Data}}},
  author = {Chang, Lijun and Li, Wei and Zhang, Wenjie},
  date = {2017-05-09},
  pages = {1181--1196},
  publisher = {ACM},
  location = {Chicago Illinois USA},
  doi = {10.1145/3035918.3035939},
  url = {https://dl.acm.org/doi/10.1145/3035918.3035939},
  urldate = {2023-09-02},
  abstract = {This paper studies the problem of e ciently computing a maximum independent set from a large graph, a fundamental problem in graph analysis. Due to the hardness results of computing an exact maximum independent set or an approximate maximum independent set with accuracy guarantee, the existing algorithms resort to heuristic techniques for approximately computing a maximum independent set with good performance in practice but no accuracy guarantee theoretically. Observing that the existing techniques have various limits, in this paper, we aim to develop e cient algorithms (with linear or near-linear time complexity) that can generate a highquality (large-size) independent set from a graph in practice. In particular, firstly we develop a Reducing-Peeling framework which iteratively reduces the graph size by applying reduction rules on vertices with very low degrees (Reducing) and temporarily removing the vertex with the highest degree (Peeling) if the reduction rules cannot be applied. Secondly, based on our framework we design two baseline algorithms, BDOne and BDTwo, by utilizing the existing reduction rules for handling degree-one and degree-two vertices, respectively. Both algorithms can generate higher-quality (larger-size) independent sets than the existing algorithms. Thirdly, we propose a linear-time algorithm, LinearTime, and a near-linear time algorithm, NearLinear, by designing new reduction rules and developing techniques for e ciently and incrementally applying reduction rules. In practice, LinearTime takes similar time and space to BDOne but computes a higher quality independent set, similar in size to that of an independent set generated by BDTwo. Moreover, in practice NearLinear has a good chance to generate a maximum independent set and it often generates near-maximum independent sets. Fourthly, we extend our techniques to accelerate the existing iterated local search algorithms. Extensive empirical studies show that all our algorithms output much larger independent sets than the existing linear-time algorithms while having a similar running time, as well as achieve significant speedup against the existing iterated local search algorithms.},
  eventtitle = {{{SIGMOD}}/{{PODS}}'17: {{International Conference}} on {{Management}} of {{Data}}},
  isbn = {978-1-4503-4197-4},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\8A8PARE2\Chang et al. - 2017 - Computing A Near-Maximum Independent Set in Linear.pdf}
}

@unpublished{chaudhariStochasticGradientDescent2017,
  title = {Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks},
  author = {Chaudhari, Pratik and Soatto, Stefano},
  date = {2017-10-30},
  eprint = {1710.11029},
  eprinttype = {arxiv},
  eprintclass = {cond-mat, stat},
  url = {http://arxiv.org/abs/1710.11029},
  urldate = {2019-08-26},
  abstract = {Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such "out-of-equilibrium" behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1\% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Statistical Mechanics,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7MBCKZ75\\Chaudhari_Soatto_2017_Stochastic gradient descent performs variational inference, converges to limit.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\R285WC2S\\1710.html}
}

@unpublished{chengUniversalDensityMatrix2019,
  title = {A {{Universal Density Matrix Functional}} from {{Molecular Orbital-Based Machine Learning}}: {{Transferability}} across {{Organic Molecules}}},
  shorttitle = {A {{Universal Density Matrix Functional}} from {{Molecular Orbital-Based Machine Learning}}},
  author = {Cheng, Lixue and Welborn, Matthew and Miller III, Thomas F.},
  date = {2019-01-10},
  eprint = {1901.03309},
  eprinttype = {arxiv},
  eprintclass = {physics},
  url = {http://arxiv.org/abs/1901.03309},
  urldate = {2019-01-16},
  abstract = {We address the degree to which machine learning can be used to accurately and transferably predict post-Hartree-Fock correlation energies. After presenting refined strategies for feature design and selection, the molecular-orbital-based machine learning (MOB-ML) method is first applied to benchmark test systems. It is shown that the total electronic energy for a set of 1000 randomized geometries of water can be described to within 1 millihartree using a model that is trained at the level of MP2, CCSD, or CCSD(T) using only a single reference calculation at a randomized geometry. To explore the breadth of chemical diversity that can be described, the MOB-ML method is then applied a set of 7211 organic models with up to seven heavy atoms. It is shown that MP2 calculations on only 90 molecules are needed to train a model that predicts MP2 energies to within 2 millihartree accuracy for the remaining 7121 molecules; likewise, CCSD(T) calculations on only 150 molecules are needed to train a model that predicts CCSD(T) energies for the remaining molecules to within 2 millihartree accuracy. The MP2 model, trained with only 90 reference calculations on seven-heavy-atom molecules, is then applied to a diverse set of 1000 thirteen-heavy-atom organic molecules, demonstrating transferable preservation of chemical accuracy.},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4C66NPM9\\Cheng et al_2019_A Universal Density Matrix Functional from Molecular Orbital-Based Machine.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\S9D7MYZA\\1901.html}
}

@article{chenImbalancedDatasetbasedEcho2020,
  title = {Imbalanced Dataset-Based Echo State Networks for Anomaly Detection},
  author = {Chen, Qing and Zhang, Anguo and Huang, Tingwen and He, Qianping and Song, Yongduan},
  date = {2020-04-01},
  journaltitle = {Neural Comput \& Applic},
  volume = {32},
  number = {8},
  pages = {3685--3694},
  issn = {1433-3058},
  doi = {10.1007/s00521-018-3747-z},
  url = {https://doi.org/10.1007/s00521-018-3747-z},
  urldate = {2023-01-27},
  abstract = {Anomaly detection is a very effective method to extract useful information from abundant data. Most existing anomaly detection methods are based on normal region or some specific algorithms, which ignore the fact that many actual datasets are mainly imbalanced, resulting in not function properly or effectively in practical, especially in the medical field. On the other hand, imbalanced dataset is also a frequently encountered problem in the learning of neural network because the lack of data in a minority class may lead to uneven classification accuracy. In this paper, inspired by these observations, a novel anomaly detection approach by using classical echo state network (ESN), a brain-inspired neural computing model, is presented. The entire dataset of the proposed method obeys an extremely imbalanced distribution, that is, anomalies are much rarer than normal data. And the training dataset has only the normal data. When the ESN is well trained, the parameters in ESN are the memory of normal data. If the normal data are added into the well-trained network, the error between the input data and the corresponding output is smaller compared with the error between abnormal input data and its corresponding output. Then anomaly behavior is detected if the error between the input data and the corresponding predictive value exceeds a certain threshold. Different from setting an invariable threshold arbitrarily for all of the data, the threshold value used in the proposed method is determined from the analysis of information theory and can be adjust adaptively according to different datasets. Experiments on abnormal heart rate detection are conducted to demonstrate and verify the effectiveness of the proposed detection algorithm and theory.},
  langid = {english}
}

@unpublished{chenLearningMakeGeneralizable2019,
  ids = {chen2019learninga,chen2019learningb},
  title = {Learning to {{Make Generalizable}} and {{Diverse Predictions}} for {{Retrosynthesis}}},
  author = {Chen, Benson and Shen, Tianxiao and Jaakkola, Tommi S. and Barzilay, Regina},
  date = {2019-10-21},
  eprint = {1910.09688},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.09688},
  urldate = {2021-05-10},
  abstract = {We propose a new model for making generalizable and diverse retrosynthetic reaction predictions. Given a target compound, the task is to predict the likely chemical reactants to produce the target. This generative task can be framed as a sequence-to-sequence problem by using the SMILES representations of the molecules. Building on top of the popular Transformer architecture, we propose two novel pre-training methods that construct relevant auxiliary tasks (plausible reactions) for our problem. Furthermore, we incorporate a discrete latent variable model into the architecture to encourage the model to produce a diverse set of alternative predictions. On the 50k subset of reaction examples from the United States patent literature (USPTO-50k) benchmark dataset, our model greatly improves performance over the baseline, while also generating predictions that are more diverse.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7L59WB7Z\\Chen et al. - 2019 - Learning to Make Generalizable and Diverse Predict.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\9J2DA3NK\\Chen et al. - 2019 - Learning to Make Generalizable and Diverse Predict.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ILKEHC3U\\1910.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\WHWD4PH8\\1910.html}
}

@inproceedings{chenMoleculeOptimizationExplainable2020,
  title = {Molecule {{Optimization}} by {{Explainable Evolution}}},
  author = {Chen, Binghong and Wang, Tianzhe and Li, Chengtao and Dai, Hanjun and Song, Le},
  date = {2020-10-02},
  url = {https://openreview.net/forum?id=jHefDGsorp5},
  urldate = {2023-11-17},
  abstract = {Optimizing molecules for desired properties is a fundamental yet challenging task in chemistry, material science, and drug discovery. This paper develops a novel algorithm for optimizing molecular properties via an Expectation-Maximization (EM) like explainable evolutionary process. The algorithm is designed to mimic human experts in the process of searching for desirable molecules and alternate between two stages: the first stage on explainable local search which identifies rationales, i.e., critical subgraph patterns accounting for desired molecular properties, and the second stage on molecule completion which explores the larger space of molecules containing good rationales. We test our approach against various baselines on a real-world multi-property optimization task where each method is given the same number of queries to the property oracle. We show that our evolution-by-explanation algorithm is 79\% better than the best baseline in terms of a generic metric combining aspects such as success rate, novelty, and diversity. Human expert evaluation on optimized molecules shows that 60\% of top molecules obtained from our methods are deemed successful.},
  eventtitle = {{{ICLR}}},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\SY8AZF7A\Chen et al. - 2020 - Molecule Optimization by Explainable Evolution.pdf}
}

@unpublished{chenResidualFlowsInvertible2019,
  title = {Residual {{Flows}} for {{Invertible Generative Modeling}}},
  author = {Chen, Ricky T. Q. and Behrmann, Jens and Duvenaud, David and Jacobsen, Jörn-Henrik},
  date = {2019-06-06},
  eprint = {1906.02735},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.02735},
  urldate = {2019-06-12},
  abstract = {Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions rather than strict architectural constraints are needed for enforcing invertibility. However, prior work trained invertible residual networks for density estimation by relying on biased log-density estimates whose bias increased with the network's expressiveness. We give a tractable unbiased estimate of the log density, and reduce the memory required during training by a factor of ten. Furthermore, we improve invertible residual blocks by proposing the use of activation functions that avoid gradient saturation and generalizing the Lipschitz condition to induced mixed norms. The resulting approach, called Residual Flows, achieves state-of-the-art performance on density estimation amongst flow-based models, and outperforms networks that use coupling blocks at joint generative and discriminative modeling.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\C96UDU5J\\Chen et al_2019_Residual Flows for Invertible Generative Modeling.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZYDDH5Q7\\1906.html}
}

@unpublished{chenRetroLearningRetrosynthetic2020,
  title = {Retro*: {{Learning Retrosynthetic Planning}} with {{Neural Guided A}}* {{Search}}},
  shorttitle = {Retro*},
  author = {Chen, Binghong and Li, Chengtao and Dai, Hanjun and Song, Le},
  date = {2020-06-29},
  eprint = {2006.15820},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.15820},
  urldate = {2021-04-19},
  abstract = {Retrosynthetic planning is a critical task in organic chemistry which identifies a series of reactions that can lead to the synthesis of a target product. The vast number of possible chemical transformations makes the size of the search space very big, and retrosynthetic planning is challenging even for experienced chemists. However, existing methods either require expensive return estimation by rollout with high variance, or optimize for search speed rather than the quality. In this paper, we propose Retro*, a neural-based A*-like algorithm that finds high-quality synthetic routes efficiently. It maintains the search as an AND-OR tree, and learns a neural search bias with off-policy data. Then guided by this neural network, it performs best-first search efficiently during new planning episodes. Experiments on benchmark USPTO datasets show that, our proposed method outperforms existing state-of-the-art with respect to both the success rate and solution quality, while being more efficient at the same time.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DGC7N2C8\\Chen et al. - 2020 - Retro Learning Retrosynthetic Planning with Neur.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SZ8Y9M7K\\2006.html}
}

@article{chenRiseDeepLearning2018,
  title = {The Rise of Deep Learning in Drug Discovery},
  author = {Chen, Hongming and Engkvist, Ola and Wang, Yinhai and Olivecrona, Marcus and Blaschke, Thomas},
  date = {2018-06-01},
  journaltitle = {Drug Discovery Today},
  volume = {23},
  number = {6},
  pages = {1241--1250},
  issn = {1359-6446},
  doi = {10.1016/j.drudis.2018.01.039},
  url = {https://www.sciencedirect.com/science/article/pii/S1359644617303598},
  urldate = {2023-10-02},
  abstract = {Over the past decade, deep learning has achieved remarkable success in various artificial intelligence research areas. Evolved from the previous research on artificial neural networks, this technology has shown superior performance to other machine learning algorithms in areas such as image and voice recognition, natural language processing, among others. The first wave of applications of deep learning in pharmaceutical research has emerged in recent years, and its utility has gone beyond bioactivity predictions and has shown promise in addressing diverse problems in drug discovery. Examples will be discussed covering bioactivity prediction, de novo molecular design, synthesis prediction and biological image analysis.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\YK4YLBIM\\Chen et al. - 2018 - The rise of deep learning in drug discovery.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\M2KRFA7C\\S1359644617303598.html}
}

@inproceedings{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  date = {2020-11-21},
  pages = {1597--1607},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {http://proceedings.mlr.press/v119/chen20j.html},
  urldate = {2021-04-08},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring sp...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\WKG64LMH\\Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\66UDG548\\chen20j.html}
}

@article{chertiNovoDrugDesign2017,
  title = {De Novo Drug Design with Deep Generative Models : An Empirical Study},
  shorttitle = {De Novo Drug Design with Deep Generative Models},
  author = {Cherti, Mehdi and Kegl, Balazs and Kazakci, Akin},
  date = {2017-02-17},
  url = {https://openreview.net/forum?id=SkkC41HYl},
  urldate = {2018-09-24},
  abstract = {We present an empirical study about the usage of RNN generative models for stochastic optimization in the context of de novo drug design. We study different kinds of architectures and we find...},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\98FCDMXR\\Cherti et al_2017_De novo drug design with deep generative models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\A9BLM34Q\\Cherti et al_2017_De novo drug design with deep generative models2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\L2BLXA34\\forum.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\LHQZD2FX\\forum.html}
}

@article{chevillardSCUBIDOOLargeScreenable2015,
  title = {{{SCUBIDOO}}: {{A Large}} yet {{Screenable}} and {{Easily Searchable Database}} of {{Computationally Created Chemical Compounds Optimized}} toward {{High Likelihood}} of {{Synthetic Tractability}}},
  shorttitle = {{{SCUBIDOO}}},
  author = {Chevillard, F. and Kolb, P.},
  date = {2015-09-28},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {55},
  number = {9},
  pages = {1824--1835},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.5b00203},
  url = {https://doi.org/10.1021/acs.jcim.5b00203},
  urldate = {2020-06-03},
  abstract = {De novo drug design is widely assisted by computational approaches that enable the generation of a tremendous amount of new virtual molecules within a short time frame. While the novelty of the computationally generated compounds can easily be assessed, such approaches often neglect the synthetic feasibility of the molecules, thus creating a potential hurdle that can be a barrier to further investigation. Therefore, we have developed SCUBIDOO, a freely accessible database concept that currently holds 21 million virtual products originating from a small library of building blocks and a collection of robust organic reactions. This large data set was reduced to three representative and computationally tractable samples denoted as S, M, and L, containing 9994, 99\,977, and 999\,794 products, respectively. These small sets are useful as starting points for ligand identification and optimization projects. The generated products come with synthesis instructions and alerts of possible side reactions, and we show that they exhibit drug-like properties while still extending into unexplored quadrants of chemical space, thus suggesting novelty. We show multiple examples that demonstrate how SCUBIDOO can facilitate the search around initial hits. This database might be a useful idea generator for early ligand discovery projects since it allows a focus on those molecules that are likely to be synthetically feasible and can therefore be studied further. Together with its modular building block construction principle, this database is also suitable for structure–activity relationship studies or fragment-growing strategies.}
}

@unpublished{choiEmpiricalComparisonsOptimizers2020,
  ids = {choi2020empirical},
  title = {On {{Empirical Comparisons}} of {{Optimizers}} for {{Deep Learning}}},
  author = {Choi, Dami and Shallue, Christopher J. and Nado, Zachary and Lee, Jaehoon and Maddison, Chris J. and Dahl, George E.},
  date = {2020-01-04},
  eprint = {1910.05446},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.05446},
  urldate = {2020-02-25},
  abstract = {Selecting an optimizer is a central step in the contemporary deep learning pipeline. In this paper, we demonstrate the sensitivity of optimizer comparisons to the hyperparameter tuning protocol. Our findings suggest that the hyperparameter search space may be the single most important factor explaining the rankings obtained by recent empirical comparisons in the literature. In fact, we show that these results can be contradicted when hyperparameter search spaces are changed. As tuning effort grows without bound, more general optimizers should never underperform the ones they can approximate (i.e., Adam should never perform worse than momentum), but recent attempts to compare optimizers either assume these inclusion relationships are not practically relevant or restrict the hyperparameters in ways that break the inclusions. In our experiments, we find that inclusion relationships between optimizers matter in practice and always predict optimizer comparisons. In particular, we find that the popular adaptive gradient methods never underperform momentum or gradient descent. We also report practical tips around tuning often ignored hyperparameters of adaptive gradient methods and raise concerns about fairly benchmarking optimizers for neural network training.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\H5RPHJWS\\Choi et al. - 2020 - On Empirical Comparisons of Optimizers for Deep Le.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LFWQGTD2\\Choi et al_2020_On Empirical Comparisons of Optimizers for Deep Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XGQKXBN6\\Choi et al_2020_On Empirical Comparisons of Optimizers for Deep Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LK8CXDTW\\1910.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\WIVZ4JDK\\1910.html}
}

@unpublished{choiWAICWhyGenerative2018,
  title = {{{WAIC}}, but {{Why}}? {{Generative Ensembles}} for {{Robust Anomaly Detection}}},
  shorttitle = {{{WAIC}}, but {{Why}}?},
  author = {Choi, Hyunsun and Jang, Eric and Alemi, Alexander A.},
  date = {2018-10-02},
  eprint = {1810.01392},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.01392},
  urldate = {2019-04-30},
  abstract = {Machine learning models encounter Out-of-Distribution (OoD) errors when the data seen at test time are generated from a different stochastic generator than the one used to generate the training data. One proposal to scale OoD detection to high-dimensional data is to learn a tractable likelihood approximation of the training distribution, and use it to reject unlikely inputs. However, likelihood models on natural data are themselves susceptible to OoD errors, and even assign large likelihoods to samples from other datasets. To mitigate this problem, we propose Generative Ensembles, which robustify density-based OoD detection by way of estimating epistemic uncertainty of the likelihood model. We present a puzzling observation in need of an explanation -- although likelihood measures cannot account for the typical set of a distribution, and therefore should not be suitable on their own for OoD detection, WAIC performs surprisingly well in practice.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\MD3ZMBJU\\Choi et al_2018_WAIC, but Why.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BX9RARKF\\1810.html}
}

@inproceedings{choLearningPhraseRepresentations2014,
  ids = {cho2014learninga,cho2014learningb,cho2014learningc},
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}–{{Decoder}} for {{Statistical Machine Translation}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Cho, Kyunghyun and family=Merrienboer, given=Bart, prefix=van, useprefix=true and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  date = {2014},
  pages = {1724--1734},
  publisher = {Association for Computational Linguistics},
  location = {Doha, Qatar},
  doi = {10.3115/v1/D14-1179},
  url = {http://aclweb.org/anthology/D14-1179},
  urldate = {2020-07-20},
  eventtitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DJMZD4SX\\Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder–.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LU3FHUWI\\Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder–.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\MULC4CKK\\Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder–.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YXGBHQN9\\Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder–.pdf}
}

@unpublished{choLearningPhraseRepresentations2014a,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder-Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and family=Merrienboer, given=Bart, prefix=van, useprefix=true and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  date = {2014-06-03},
  eprint = {1406.1078},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1406.1078},
  urldate = {2018-05-19},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\3Z55AJ94\\Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WSLTYCDB\\1406.html}
}

@unpublished{cholletMeasureIntelligence2019,
  title = {The {{Measure}} of {{Intelligence}}},
  author = {Chollet, François},
  date = {2019-11-04},
  eprint = {1911.01547},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1911.01547},
  urldate = {2019-11-07},
  abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LB3MAWFI\\Chollet_2019_The Measure of Intelligence.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\APQD8LQ2\\1911.html}
}

@unpublished{choPropertiesNeuralMachine2014,
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder-Decoder Approaches}}},
  shorttitle = {On the {{Properties}} of {{Neural Machine Translation}}},
  author = {Cho, Kyunghyun and family=Merrienboer, given=Bart, prefix=van, useprefix=true and Bahdanau, Dzmitry and Bengio, Yoshua},
  date = {2014-10-07},
  eprint = {1409.1259},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1409.1259},
  urldate = {2020-03-17},
  abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\T2AGRE2K\\Cho et al_2014_On the Properties of Neural Machine Translation.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SKYSHEV6\\1409.html}
}

@unpublished{choromanskaLossSurfacesMultilayer2014,
  title = {The {{Loss Surfaces}} of {{Multilayer Networks}}},
  author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, Gérard Ben and LeCun, Yann},
  date = {2014-11-30},
  eprint = {1412.0233},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1412.0233},
  urldate = {2018-09-04},
  abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DEB4X4UM\\Choromanska et al_2014_The Loss Surfaces of Multilayer Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\T6JN8SIC\\1412.html}
}

@online{choudharyRuntimeEfficacyTradeoffAnomaly2017,
  title = {On the {{Runtime-Efficacy Trade-off}} of {{Anomaly Detection Techniques}} for {{Real-Time Streaming Data}}},
  author = {Choudhary, Dhruv and Kejariwal, Arun and Orsini, Francois},
  date = {2017-10-12},
  eprint = {1710.04735},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  doi = {10.48550/arXiv.1710.04735},
  url = {http://arxiv.org/abs/1710.04735},
  urldate = {2023-01-06},
  abstract = {Ever growing volume and velocity of data coupled with decreasing attention span of end users underscore the critical need for real-time analytics. In this regard, anomaly detection plays a key role as an application as well as a means to verify data fidelity. Although the subject of anomaly detection has been researched for over 100 years in a multitude of disciplines such as, but not limited to, astronomy, statistics, manufacturing, econometrics, marketing, most of the existing techniques cannot be used as is on real-time data streams. Further, the lack of characterization of performance -- both with respect to real-timeliness and accuracy -- on production data sets makes model selection very challenging. To this end, we present an in-depth analysis, geared towards real-time streaming data, of anomaly detection techniques. Given the requirements with respect to real-timeliness and accuracy, the analysis presented in this paper should serve as a guide for selection of the "best" anomaly detection technique. To the best of our knowledge, this is the first characterization of anomaly detection techniques proposed in very diverse set of fields, using production data sets corresponding to a wide set of application domains.},
  pubstate = {preprint},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\EKVRY526\\Choudhary et al. - 2017 - On the Runtime-Efficacy Trade-off of Anomaly Detec.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ARTWQ7HR\\1710.html}
}

@article{clarkOptiSimExtendedDissimilarity1997,
  title = {{{OptiSim}}:\, {{An Extended Dissimilarity Selection Method}} for {{Finding Diverse Representative Subsets}}},
  shorttitle = {{{OptiSim}}},
  author = {Clark, Robert D.},
  date = {1997-11-01},
  journaltitle = {J. Chem. Inf. Comput. Sci.},
  volume = {37},
  number = {6},
  pages = {1181--1188},
  publisher = {American Chemical Society},
  issn = {0095-2338},
  doi = {10.1021/ci970282v},
  url = {https://doi.org/10.1021/ci970282v},
  urldate = {2022-07-05},
  abstract = {Compound selection methods currently available to chemists are based on maximum or minimum dissimilarity selection or on hierarchical clustering. Optimizable K-Dissimilarity Selection (OptiSim) is a novel and efficient stochastic selection algorithm which includes maximum and minimum dissimilarity-based selection as special cases. By adjusting the subsample size parameter K, it is possible to adjust the balance between representativeness and diversity in the compounds selected. The OptiSim algorithm is described, along with some analytical tools for comparing it to other selection methods. Such comparisons indicate that OptiSim can mimic the representativeness of selections based on hierarchical clustering and, at least in some cases, improve upon them.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\S2Z7LW59\\Clark - 1997 - OptiSim  An Extended Dissimilarity Selection Meth.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LWHCWK7N\\ci970282v.html}
}

@article{cohenEarth051994,
  title = {The Earth Is Round (p {$<$} .05)},
  author = {Cohen, Jacob},
  date = {1994},
  journaltitle = {American Psychologist},
  volume = {49},
  pages = {997--1003},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1935-990X},
  doi = {10.1037/0003-066X.49.12.997},
  abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H₀ is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H₀ one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Null Hypothesis Testing},
  file = {C:\Users\USEBPERP\Zotero\storage\F2T5AHMK\1995-12080-001.html}
}

@article{cohnActiveLearningStatistical1996,
  title = {Active Learning with Statistical Models},
  author = {Cohn, David A. and Ghahramani, Zoubin and Jordan, Michael I.},
  date = {1996},
  journaltitle = {Journal of artificial intelligence research},
  volume = {4},
  pages = {129--145},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\Z9T64N5U\\Cohn et al_1996_Active learning with statistical models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\EL5LL6NB\\10158.html}
}

@online{colahUnderstandingLSTMNetworks2015,
  title = {Understanding {{LSTM Networks}} -- Colah's Blog},
  author = {Colah, Christopher},
  date = {2015-08-27},
  url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
  urldate = {2018-07-02},
  file = {C:\Users\USEBPERP\Zotero\storage\FREMJXN7\2015-08-Understanding-LSTMs.html}
}

@article{coleyAutonomousDiscoveryChemical,
  title = {Autonomous Discovery in the Chemical Sciences Part {{II}}: {{Outlook}}},
  shorttitle = {Autonomous Discovery in the Chemical Sciences Part {{II}}},
  author = {Coley, Connor W. and Eyke, Natalie S. and Jensen, Klavs F.},
  journaltitle = {Angewandte Chemie International Edition},
  volume = {n/a},
  number = {n/a},
  issn = {1521-3773},
  doi = {10.1002/anie.201909989},
  url = {https://www.onlinelibrary.wiley.com/doi/abs/10.1002/anie.201909989},
  urldate = {2020-05-29},
  abstract = {This two-part review examines how automation has contributed to different aspects of discovery in the chemical sciences. In this second part, we reflect on a selection of exemplary studies. It is increasingly important to articulate what the role of automation and computation has been in the scientific process and how that has or has not accelerated discovery. One can argue that even the best automated systems have yet to 'discover' despite being incredibly useful as laboratory assistants. We must carefully consider how they have been and can be applied to future problems of chemical discovery in order to effectively design and interact with future autonomous platforms. The majority of this article defines a large set of open research directions, including improving our ability to work with complex data, build empirical models, automate both physical and computational experiments for validation, select experiments, and evaluate whether we are making progress toward the ultimate goal of autonomous discovery. Addressing these practical and methodological challenges will greatly advance the extent to which autonomous systems can make meaningful discoveries.},
  langid = {english},
  keywords = {automation,autonomous,big data,discovery,machine learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7EV255LW\\Coley et al_Autonomous discovery in the chemical sciences part II.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2TPLEDC4\\anie.html}
}

@article{coleyComputerAssistedRetrosynthesisBased2017,
  ids = {coley2017computerassisteda,coleyComputerAssistedRetrosynthesisBased2017a},
  title = {Computer-{{Assisted Retrosynthesis Based}} on {{Molecular Similarity}}},
  author = {Coley, Connor W. and Rogers, Luke and Green, William H. and Jensen, Klavs F.},
  date = {2017-12-27},
  journaltitle = {ACS Cent. Sci.},
  volume = {3},
  number = {12},
  pages = {1237--1245},
  publisher = {American Chemical Society},
  issn = {2374-7943, 2374-7951},
  doi = {10.1021/acscentsci.7b00355},
  url = {https://pubs.acs.org/doi/10.1021/acscentsci.7b00355},
  urldate = {2020-11-19},
  issue = {12},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\GQU3YBLC\\Coley et al. - 2017 - Computer-Assisted Retrosynthesis Based on Molecula.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WH3F6BF9\\Coley et al. - 2017 - Computer-Assisted Retrosynthesis Based on Molecula.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SB8D4XS3\\acscentsci.html}
}

@article{coleyConvolutionalEmbeddingAttributed2017,
  title = {Convolutional {{Embedding}} of {{Attributed Molecular Graphs}} for {{Physical Property Prediction}}},
  author = {Coley, Connor W. and Barzilay, Regina and Green, William H. and Jaakkola, Tommi S. and Jensen, Klavs F.},
  date = {2017-08-28},
  journaltitle = {J Chem Inf Model},
  volume = {57},
  number = {8},
  eprint = {28696688},
  eprinttype = {pmid},
  pages = {1757--1772},
  issn = {1549-960X},
  doi = {10.1021/acs.jcim.6b00601},
  abstract = {The task of learning an expressive molecular representation is central to developing quantitative structure-activity and property relationships. Traditional approaches rely on group additivity rules, empirical measurements or parameters, or generation of thousands of descriptors. In this paper, we employ a convolutional neural network for this embedding task by treating molecules as undirected graphs with attributed nodes and edges. Simple atom and bond attributes are used to construct atom-specific feature vectors that take into account the local chemical environment using different neighborhood radii. By working directly with the full molecular graph, there is a greater opportunity for models to identify important features relevant to a prediction task. Unlike other graph-based approaches, our atom featurization preserves molecule-level spatial information that significantly enhances model performance. Our models learn to identify important features of atom clusters for the prediction of aqueous solubility, octanol solubility, melting point, and toxicity. Extensions and limitations of this strategy are discussed.},
  langid = {english},
  keywords = {Computer Graphics,Informatics,Neural Networks (Computer),Octanols,Physical Phenomena,Solubility,Toxicity Tests,Transition Temperature,Water}
}

@article{coleyDefiningExploringChemical2021,
  title = {Defining and {{Exploring Chemical Spaces}}},
  author = {Coley, Connor W.},
  date = {2021-02-01},
  journaltitle = {Trends in Chemistry},
  series = {Special {{Issue}}: {{Machine Learning}} for {{Molecules}} and {{Materials}}},
  volume = {3},
  number = {2},
  pages = {133--145},
  issn = {2589-5974},
  doi = {10.1016/j.trechm.2020.11.004},
  url = {https://www.sciencedirect.com/science/article/pii/S2589597420302884},
  urldate = {2022-09-13},
  abstract = {Designing functional molecules with desirable properties is often a challenging, multi-objective optimization. For decades, there have been computational approaches to facilitate this process through the simulation of physical processes, the prediction of molecular properties using structure–property relationships, and the selection or generation of molecular structures. This review provides an overview of some algorithmic approaches to defining and exploring chemical spaces that have the potential to operationalize the process of molecular discovery. We emphasize the potential roles of machine learning and the consideration of synthetic feasibility, which is a prerequisite to ‘closing the loop’. We conclude by summarizing important directions for the future development and evaluation of these methods.},
  langid = {english},
  keywords = {design,machine learning,molecular discovery,optimization},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\J7GH5G2M\\Coley - 2021 - Defining and Exploring Chemical Spaces.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\4UTDR984\\S2589597420302884.html}
}

@article{coleyDefiningExploringChemical2021a,
  title = {Defining and {{Exploring Chemical Spaces}}},
  author = {Coley, Connor W.},
  date = {2021-02-01},
  journaltitle = {TRECHEM},
  volume = {3},
  number = {2},
  pages = {133--145},
  publisher = {Elsevier},
  issn = {2589-7209, 2589-5974},
  doi = {10.1016/j.trechm.2020.11.004},
  url = {https://www.cell.com/trends/chemistry/abstract/S2589-5974(20)30288-4},
  urldate = {2024-01-05},
  langid = {english},
  keywords = {de novo design,machine learning,molecular discovery,optimization},
  file = {C:\Users\USEBPERP\Zotero\storage\9D78Z3BL\Coley - 2021 - Defining and Exploring Chemical Spaces.pdf}
}

@article{coleyMachineLearningComputerAided2018,
  ids = {coley2018machinea},
  title = {Machine {{Learning}} in {{Computer-Aided Synthesis Planning}}},
  author = {Coley, Connor W. and Green, William H. and Jensen, Klavs F.},
  date = {2018-05-15},
  journaltitle = {Acc. Chem. Res.},
  volume = {51},
  number = {5},
  pages = {1281--1289},
  publisher = {American Chemical Society},
  issn = {0001-4842, 1520-4898},
  doi = {10.1021/acs.accounts.8b00087},
  url = {https://pubs.acs.org/doi/10.1021/acs.accounts.8b00087},
  urldate = {2019-10-02},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\HPKIWE4M\\Coley et al_2018_Machine Learning in Computer-Aided Synthesis Planning2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JDXET5MW\\Coley et al. - 2018 - Machine Learning in Computer-Aided Synthesis Plann.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\R55BH642\\Coley et al. - 2018 - Machine Learning in Computer-Aided Synthesis Plann.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XPVT4IV3\\Coley et al_2018_Machine Learning in Computer-Aided Synthesis Planning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JE8B5JFM\\acs.accounts.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\XQFDXAAH\\acs.accounts.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\XTDI5PSY\\acs.accounts.html}
}

@article{coleyPredictionOrganicReaction2017,
  ids = {coley2017prediction},
  title = {Prediction of {{Organic Reaction Outcomes Using Machine Learning}}},
  author = {Coley, Connor W. and Barzilay, Regina and Jaakkola, Tommi S. and Green, William H. and Jensen, Klavs F.},
  date = {2017-05-24},
  journaltitle = {ACS Cent. Sci.},
  volume = {3},
  number = {5},
  pages = {434--443},
  publisher = {American Chemical Society},
  issn = {2374-7943},
  doi = {10.1021/acscentsci.7b00064},
  url = {https://doi.org/10.1021/acscentsci.7b00064},
  urldate = {2019-01-15},
  abstract = {Computer assistance in synthesis design has existed for over 40 years, yet retrosynthesis planning software has struggled to achieve widespread adoption. One critical challenge in developing high-quality pathway suggestions is that proposed reaction steps often fail when attempted in the laboratory, despite initially seeming viable. The true measure of success for any synthesis program is whether the predicted outcome matches what is observed experimentally. We report a model framework for anticipating reaction outcomes that combines the traditional use of reaction templates with the flexibility in pattern recognition afforded by neural networks. Using 15\,000 experimental reaction records from granted United States patents, a model is trained to select the major (recorded) product by ranking a self-generated list of candidates where one candidate is known to be the major product. Candidate reactions are represented using a unique edit-based representation that emphasizes the fundamental transformation from reactants to products, rather than the constituent molecules’ overall structures. In a 5-fold cross-validation, the trained model assigns the major product rank 1 in 71.8\% of cases, rank ≤3 in 86.7\% of cases, and rank ≤5 in 90.8\% of cases.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JQ4KWMDV\\Coley et al_2017_Prediction of Organic Reaction Outcomes Using Machine Learning3.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\PQISY3TZ\\Coley et al_2017_Prediction of Organic Reaction Outcomes Using Machine Learning2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\W8KGZY5I\\Coley et al_2017_Prediction of Organic Reaction Outcomes Using Machine Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XK76Q3PD\\Coley et al. - 2017 - Prediction of Organic Reaction Outcomes Using Mach.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2NEU989P\\acscentsci.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\4XWSQGIN\\acscentsci.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\EJSIUCEF\\acscentsci.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\GTT2CTI9\\acscentsci.html}
}

@article{coleyRDChiralRDKitWrapper2019,
  title = {{{RDChiral}}: {{An RDKit Wrapper}} for {{Handling Stereochemistry}} in {{Retrosynthetic Template Extraction}} and {{Application}}},
  shorttitle = {{{RDChiral}}},
  author = {Coley, Connor W. and Green, William H. and Jensen, Klavs F.},
  date = {2019-06-24},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {59},
  number = {6},
  pages = {2529--2537},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.9b00286},
  url = {https://doi.org/10.1021/acs.jcim.9b00286},
  urldate = {2021-04-08},
  abstract = {There is a renewed interest in computer-aided synthesis planning, where the vast majority of approaches require the application of retrosynthetic reaction templates. Here we introduce RDChiral, an open-source Python wrapper for RDKit designed to provide consistent handling of stereochemical information in applying retrosynthetic transformations encoded as SMARTS strings. RDChiral is designed to enforce the introduction, destruction, retention, and inversion of chiral tetrahedral centers as well as the cis/trans configuration of double bonds. We also introduce an open-source implementation of a retrosynthetic template extraction algorithm to generate SMARTS patterns from atom-mapped reaction SMILES strings. In this application note, we describe the implementation of these two pieces of code and illustrate their use through many examples.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ZYFY5VHB\\Coley et al. - 2019 - RDChiral An RDKit Wrapper for Handling Stereochem.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JQN5N9FZ\\acs.jcim.html}
}

@article{coleyRoboticPlatformFlow2019,
  title = {A Robotic Platform for Flow Synthesis of Organic Compounds Informed by {{AI}} Planning},
  author = {Coley, Connor W. and Thomas, Dale A. and Lummiss, Justin A. M. and Jaworski, Jonathan N. and Breen, Christopher P. and Schultz, Victor and Hart, Travis and Fishman, Joshua S. and Rogers, Luke and Gao, Hanyu and Hicklin, Robert W. and Plehiers, Pieter P. and Byington, Joshua and Piotti, John S. and Green, William H. and Hart, A. John and Jamison, Timothy F. and Jensen, Klavs F.},
  date = {2019-08-09},
  journaltitle = {Science},
  volume = {365},
  number = {6453},
  eprint = {31395756},
  eprinttype = {pmid},
  pages = {eaax1566},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aax1566},
  url = {https://science.sciencemag.org/content/365/6453/eaax1566},
  urldate = {2019-10-14},
  abstract = {Pairing prediction and robotic synthesis Progress in automated synthesis of organic compounds has been proceeding along parallel tracks. One goal is algorithmic prediction of viable routes to a desired compound; the other is implementation of a known reaction sequence on a platform that needs little to no human intervention. Coley et al. now report preliminary integration of these two protocols. They paired a retrosynthesis prediction algorithm with a robotically reconfigurable flow apparatus. Human intervention was still required to supplement the predictor with practical considerations such as solvent choice and precise stoichiometry, although predictions should improve as accessible data accumulate for training. Science, this issue p. eaax1566 Structured Abstract INTRODUCTIONThe ability to synthesize complex organic molecules is essential to the discovery and manufacture of functional compounds, including small-molecule medicines. Despite advances in laboratory automation, the identification and development of synthetic routes remain a manual process and experimental synthesis platforms must be manually configured to suit the type of chemistry to be performed, requiring time and effort investment from expert chemists. The ideal automated synthesis platform would be capable of planning its own synthetic routes and executing them under conditions that facilitate scale-up to production goals. Individual elements of the chemical development process (design, route development, experimental configuration, and execution) have been streamlined in previous studies, but none has presented a path toward integration of computer-aided synthesis planning (CASP), expert refined chemical recipe generation, and robotically executed chemical synthesis. RATIONALEWe describe an approach toward automated, scalable synthesis that combines techniques in artificial intelligence (AI) for planning and robotics for execution. Millions of previously published reactions inform the computational design of synthetic routes; expert-refined chemical recipe files (CRFs) are run on a robotic flow chemistry platform for scalable, reproducible synthesis. This development strategy augments a chemist’s ability to approach target-oriented flow synthesis while substantially reducing the necessary information gathering and manual effort. RESULTSWe developed an open source software suite for CASP trained on millions of reactions from the Reaxys database and the U.S. Patent and Trademark Office. The software was designed to generalize known chemical reactions to new substrates by learning to apply retrosynthetic transformations, to identify suitable reaction conditions, and to evaluate whether reactions are likely to be successful when attempted experimentally. Suggested routes partially populate CRFs, which require additional details from chemist users to define residence times, stoichiometries, and concentrations that are compatible with continuous flow. To execute these syntheses, a robotic arm assembles modular process units (reactors and separators) into a continuous flow path according to the desired process configuration defined in the CRF. The robot also connects reagent lines and computer-controlled pumps to reactor inlets through a fluidic switchboard. When that is completed, the system primes the lines and starts the synthesis. After a specified synthesis time, the system flushes the lines with a cleaning solvent, and the robotic arm disconnects reagent lines and removes process modules to their appropriate storage locations.This paradigm of flow chemistry development was demonstrated for a suite of 15 medicinally relevant small molecules. In order of increasing complexity, we investigated the synthesis of aspirin and secnidazole run back to back; lidocaine and diazepam run back to back to use a common feedstock; (S)-warfarin and safinamide to demonstrate the planning program’s stereochemical awareness; and two compound libraries: a family of five ACE inhibitors including quinapril and a family of four nonsteroidal anti-inflammatory drugs including celecoxib. These targets required a total of eight particular retrosynthetic routes and nine specific process configurations. CONCLUSIONThe software and platform herein represent a milestone on the path toward fully autonomous chemical synthesis, where routes still require human input and process development. Over time, the results generated by this and similar automated experimental platforms may reduce our reliance on historical reaction data, particularly in combination with smaller-scale flow-screening platforms. Increased availability of reaction data will further enable robotically realized syntheses based on AI recommendations, relieving expert chemists of manual tasks so that they may focus on new ideas. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/365/6453/eaax1566/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Planning and execution.A robotically reconfigurable flow chemistry platform performs multistep chemical syntheses planned in part by AI. The synthesis of complex organic molecules requires several stages, from ideation to execution, that require time and effort investment from expert chemists. Here, we report a step toward a paradigm of chemical synthesis that relieves chemists from routine tasks, combining artificial intelligence–driven synthesis planning and a robotically controlled experimental platform. Synthetic routes are proposed through generalization of millions of published chemical reactions and validated in silico to maximize their likelihood of success. Additional implementation details are determined by expert chemists and recorded in reusable recipe files, which are executed by a modular continuous-flow platform that is automatically reconfigured by a robotic arm to set up the required unit operations and carry out the reaction. This strategy for computer-augmented chemical synthesis is demonstrated for 15 drug or drug-like substances. An automated synthesis platform conducts reactions on the basis of a human-devised workflow informed by a retrosynthesis algorithm. An automated synthesis platform conducts reactions on the basis of a human-devised workflow informed by a retrosynthesis algorithm.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\9QPKE853\\Coley et al_2019_A robotic platform for flow synthesis of organic compounds informed by AI.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\5NIRRLUE\\tab-pdf.html}
}

@article{coleySCScoreSyntheticComplexity2018,
  title = {{{SCScore}}: {{Synthetic Complexity Learned}} from a {{Reaction Corpus}}},
  shorttitle = {{{SCScore}}},
  author = {Coley, Connor W. and Rogers, Luke and Green, William H. and Jensen, Klavs F.},
  date = {2018-02-26},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {58},
  number = {2},
  pages = {252--261},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.7b00622},
  url = {https://doi.org/10.1021/acs.jcim.7b00622},
  urldate = {2019-10-23},
  abstract = {Several definitions of molecular complexity exist to facilitate prioritization of lead compounds, to identify diversity-inducing and complexifying reactions, and to guide retrosynthetic searches. In this work, we focus on synthetic complexity and reformalize its definition to correlate with the expected number of reaction steps required to produce a target molecule, with implicit knowledge about what compounds are reasonable starting materials. We train a neural network model on 12 million reactions from the Reaxys database to impose a pairwise inequality constraint enforcing the premise of this definition: that on average, the products of published chemical reactions should be more synthetically complex than their corresponding reactants. The learned metric (SCScore) exhibits highly desirable nonlinear behavior, particularly in recognizing increases in synthetic complexity throughout a number of linear synthetic routes.},
  issue = {2},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\N29KLN4C\\Coley et al_2018_SCScore.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZJSVA3XS\\acs.jcim.html}
}

@online{ComparingMolecularPatterns2021,
  title = {Comparing {{Molecular Patterns Using}} the {{Example}} of {{SMARTS}}: {{Theory}} and {{Algorithms}} | {{Journal}} of {{Chemical Information}} and {{Modeling}}},
  date = {2021-01-22},
  url = {https://pubs.acs.org/doi/10.1021/acs.jcim.9b00250},
  urldate = {2021-01-22}
}

@online{ConceptsApplicationsMolecular,
  title = {Concepts and {{Applications}} of {{Molecular Similarity}} | {{Wiley}}},
  url = {https://www.wiley.com/en-us/Concepts+and+Applications+of+Molecular+Similarity-p-9780471621751},
  urldate = {2022-09-05},
  abstract = {Offers authoritative overviews of topics related to the definition, computation and application of molecular similarity and emphasizes current research trends with molecular similarity as the unifying concept. Introduces and defines the concept of molecular similarity and explains how it can be used to explore the data containing 2-D and 3-D chemical information. Addresses the basic problem of relating chemical structures to their associated chemical and biological properties. Final chapters illustrate the use of similarity arguments in the study of chemical reaction pathways and present theoretical approaches to the concept of molecular similarity.},
  langid = {american},
  organization = {Wiley.com},
  file = {C:\Users\USEBPERP\Zotero\storage\G67ZSCHR\Concepts+and+Applications+of+Molecular+Similarity-p-9780471621751.html}
}

@article{cookAnomalyDetectionIoT2020,
  title = {Anomaly {{Detection}} for {{IoT Time-Series Data}}: {{A Survey}}},
  shorttitle = {Anomaly {{Detection}} for {{IoT Time-Series Data}}},
  author = {Cook, Andrew A. and Mısırlı, Göksel and Fan, Zhong},
  date = {2020-07},
  journaltitle = {IEEE Internet of Things Journal},
  volume = {7},
  number = {7},
  pages = {6481--6494},
  issn = {2327-4662},
  doi = {10.1109/JIOT.2019.2958185},
  abstract = {Anomaly detection is a problem with applications for a wide variety of domains; it involves the identification of novel or unexpected observations or sequences within the data being captured. The majority of current anomaly detection methods are highly specific to the individual use case, requiring expert knowledge of the method as well as the situation to which it is being applied. The Internet of Things (IoT) as a rapidly expanding field offers many opportunities for this type of data analysis to be implemented, however, due to the nature of the IoT, this may be difficult. This review provides a background on the challenges which may be encountered when applying anomaly detection techniques to IoT data, with examples of applications for the IoT anomaly detection taken from the literature. We discuss a range of approaches that have been developed across a variety of domains, not limited to IoT due to the relative novelty of this application. Finally, we summarize the current challenges being faced in the anomaly detection domain with a view to identifying potential research opportunities for the future.},
  eventtitle = {{{IEEE Internet}} of {{Things Journal}}},
  keywords = {Anomaly detection,data analysis,Data analysis,Internet of Things,Internet of Things (IoT),Monitoring,Performance evaluation,Sensors,survey,Urban areas},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2JP4I547\\Cook et al. - 2020 - Anomaly Detection for IoT Time-Series Data A Surv.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\KQMZBUA2\\8926446.html}
}

@article{cookLessonsLearnedFate2014,
  ids = {cook2014lessonsa},
  title = {Lessons Learned from the Fate of {{AstraZeneca}}'s Drug Pipeline: A Five-Dimensional Framework},
  shorttitle = {Lessons Learned from the Fate of {{AstraZeneca}}'s Drug Pipeline},
  author = {Cook, David and Brown, Dearg and Alexander, Robert and March, Ruth and Morgan, Paul and Satterthwaite, Gemma and Pangalos, Menelas N.},
  date = {2014-06},
  journaltitle = {Nature Reviews Drug Discovery},
  volume = {13},
  number = {6},
  pages = {419--431},
  publisher = {Nature Publishing Group},
  issn = {1474-1784},
  doi = {10.1038/nrd4309},
  url = {https://www.nature.com/articles/nrd4309},
  urldate = {2020-07-09},
  abstract = {Pangalos and colleagues discuss the results of a comprehensive longitudinal review of AstraZeneca's small-molecule drug projects from 2005 to 2010. They present a framework to guide research and development teams based on the five most important technical determinants of project success and pipeline quality: the right target, the right patient, the right tissue, the right safety and the right commercial potential. A sixth factor — the right culture — is also crucial in encouraging effective decision-making.},
  issue = {6},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\AAA8SKXB\nrd4309.html}
}

@article{cookReactionScreeningMultiwell2021,
  title = {Reaction Screening in Multiwell Plates: High-Throughput Optimization of a {{Buchwald}}–{{Hartwig}} Amination},
  shorttitle = {Reaction Screening in Multiwell Plates},
  author = {Cook, Adam and Clément, Roxanne and Newman, Stephen G.},
  date = {2021-01-11},
  journaltitle = {Nature Protocols},
  pages = {1--18},
  publisher = {Nature Publishing Group},
  issn = {1750-2799},
  doi = {10.1038/s41596-020-00452-7},
  url = {https://www.nature.com/articles/s41596-020-00452-7},
  urldate = {2021-01-25},
  abstract = {Chemical space is vast, and chemical reactions involve the complex interplay of multiple variables. As a consequence, reactions can fail for subtle reasons, necessitating screening of conditions. High-throughput experimentation (HTE) techniques enable a more comprehensive array of data to be obtained in a relatively short amount of time. Although HTE can be most efficiently achieved with automated robotic dispensing equipment, the benefits of running reaction microarrays can be accessed in any regularly equipped laboratory using inexpensive consumables. Herein, we present a cost-efficient approach to HTE, examining a Buchwald–Hartwig amination as our model reaction. Experiments are carried out in a machined aluminum 96-well plate, taking advantage of solid transfer scoops and pipettes to facilitate rapid reagent transfer. Reaction vials are simultaneously heated and mixed, using a magnetic stirrer, and worked up in parallel, using a plastic filter plate. Analysis by gas chromatography provides the chemist with 96 data points with minimal commitment of time and resources. The best-performing experiment can be selected for scale-up and isolation, or the data can be used for designing future optimization experiments.},
  langid = {english}
}

@article{coreyComputerassistedAnalysisOrganic1985,
  title = {Computer-Assisted Analysis in Organic Synthesis},
  author = {Corey, E. J. and Long, A. K. and Rubenstein, S. D.},
  date = {1985-04-26},
  journaltitle = {Science},
  volume = {228},
  number = {4698},
  eprint = {3838594},
  eprinttype = {pmid},
  pages = {408--418},
  issn = {0036-8075},
  doi = {10.1126/science.3838594},
  abstract = {The planning of alternative routes for the synthesis of complex organic molecules has been facilitated by the formulation of guiding strategies that can be applied to a broad range of problems. Analysis of organic synthesis can be carried out in the retrosynthetic direction, opposite to the actual process of chemical synthesis, or bidirectionally, that is, as a combined retrosynthetic and synthetic search. An interactive computer program is described which utilizes the general strategies of retrosynthetic analysis and an appropriate database to generate pathways of chemical intermediates for chemical synthesis of a particular target structure. Computer graphics and standard chemical structures are used for man-machine communication.},
  langid = {english},
  keywords = {Chemical Phenomena,Chemistry,Chemistry Organic,Computers,Forecasting,Software}
}

@article{coreyComputerAssistedDesignComplex1969,
  title = {Computer-{{Assisted Design}} of {{Complex Organic Syntheses}}},
  author = {Corey, E. J. and Wipke, W. Todd},
  date = {1969},
  journaltitle = {Science},
  volume = {166},
  number = {3902},
  eprint = {1727162},
  eprinttype = {jstor},
  pages = {178--192},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  url = {https://www.jstor.org/stable/1727162},
  urldate = {2021-04-08}
}

@article{coreyLogicChemicalSynthesis1991,
  title = {The {{Logic}} of {{Chemical Synthesis}}: {{Multistep Synthesis}} of {{Complex Carbogenic Molecules}} ({{Nobel Lecture}})},
  shorttitle = {The {{Logic}} of {{Chemical Synthesis}}},
  author = {Corey, Elias James},
  date = {1991},
  journaltitle = {Angewandte Chemie International Edition in English},
  volume = {30},
  number = {5},
  pages = {455--465},
  issn = {1521-3773},
  doi = {10.1002/anie.199104553},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.199104553},
  urldate = {2021-04-08},
  abstract = {“Chemical synthesis is uniquely positioned at the heart of chemistry, and its impact on our lives and society is all-pervasive.” Elias James Corey expresses this opinion in his Nobel Lecture, in which he discusses the origin and development of his contributions leading to the award of the Nobel Prize in chemistry in 1990. His concept of retrosynthetic analysis revolutionized both the practice and teaching of organic chemistry. His application of computers to synthetic planning was pioneering. The numerous synthetic methods developed by him and his co-workers have proven invaluable in organic synthesis. Finally, the total syntheses carried out in his laboratories—for example, the synthesis of gibberellic acid and of ginkgolide B—are milestones in the breathtaking development of organic synthesis over the last thirty years.},
  keywords = {Computer chemistry,Nobel lecture,Retro reactions,Synthetic methods},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ZQNRRQS9\\Corey - 1991 - The Logic of Chemical Synthesis Multistep Synthes.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WDEJXINS\\anie.html}
}

@book{coreyLogicChemicalSynthesis1991a,
  title = {The Logic of Chemical Synthesis},
  author = {Corey, E. J.},
  date = {1991},
  eprint = {0YIVAwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {Рипол Классик},
  isbn = {978-5-88501-081-8},
  langid = {english},
  pagetotal = {447},
  keywords = {Science / Chemistry / Organic}
}

@online{CorporationsReceivingBailout,
  title = {Corporations {{Receiving Bailout Billions Have Laid Off Staff}} and {{Paid Investors}}},
  url = {https://www.vice.com/en_us/article/m7jxvn/corporations-receiving-bailout-billions-have-laid-off-staff-and-paid-investors},
  urldate = {2020-08-05},
  abstract = {Campaigners have criticised the UK government for spending billions on "corporate welfare" after a VICE News investigation found companies making redundancies and paying dividends while taking tax-payer money.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\D5QM5B36\corporations-receiving-bailout-billions-have-laid-off-staff-and-paid-investors.html}
}

@article{cortes-cirianoDeepConfidenceComputationally2019,
  title = {Deep {{Confidence}}: {{A Computationally Efficient Framework}} for {{Calculating Reliable Prediction Errors}} for {{Deep Neural Networks}}},
  shorttitle = {Deep {{Confidence}}},
  author = {Cortés-Ciriano, Isidro and Bender, Andreas},
  date = {2019-03-25},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {59},
  number = {3},
  pages = {1269--1281},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.8b00542},
  url = {https://doi.org/10.1021/acs.jcim.8b00542},
  urldate = {2019-04-15},
  abstract = {Deep learning architectures have proved versatile in a number of drug discovery applications, including the modeling of in vitro compound activity. While controlling for prediction confidence is essential to increase the trust, interpretability, and usefulness of virtual screening models in drug discovery, techniques to estimate the reliability of the predictions generated with deep learning networks remain largely underexplored. Here, we present Deep Confidence, a framework to compute valid and efficient confidence intervals for individual predictions using the deep learning technique Snapshot Ensembling and conformal prediction. Specifically, Deep Confidence generates an ensemble of deep neural networks by recording the network parameters throughout the local minima visited during the optimization phase of a single neural network. This approach serves to derive a set of base learners (i.e., snapshots) with comparable predictive power on average that will however generate slightly different predictions for a given instance. The variability across base learners and the validation residuals are in turn harnessed to compute confidence intervals using the conformal prediction framework. Using a set of 24 diverse IC50 data sets from ChEMBL 23, we show that Snapshot Ensembles perform on par with Random Forest (RF) and ensembles of independently trained deep neural networks. In addition, we find that the confidence regions predicted using the Deep Confidence framework span a narrower set of values. Overall, Deep Confidence represents a highly versatile error prediction framework that can be applied to any deep learning-based application at no extra computational cost.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6S888JT9\\Cortés-Ciriano_Bender_2019_Deep Confidence.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SVHTDATM\\acs.jcim.html}
}

@article{cortes-cirianoReliablePredictionErrors2019,
  title = {Reliable {{Prediction Errors}} for {{Deep Neural Networks Using Test-Time Dropout}}},
  author = {Cortés-Ciriano, Isidro and Bender, Andreas},
  date = {2019-06-17},
  journaltitle = {J. Chem. Inf. Model.},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.9b00297},
  url = {https://doi.org/10.1021/acs.jcim.9b00297},
  urldate = {2019-07-08},
  abstract = {While the use of deep learning in drug discovery is gaining increasing attention, the lack of methods to compute reliable errors in prediction for Neural Networks prevents their application to guide decision making in domains where identifying unreliable predictions is essential, e.g., precision medicine. Here, we present a framework to compute reliable errors in prediction for Neural Networks using Test-Time Dropout and Conformal Prediction. Specifically, the algorithm consists of training a single Neural Network using dropout, and then applying it N times to both the validation and test sets, also employing dropout in this step. Therefore, for each instance in the validation and test sets an ensemble of predictions are generated. The residuals and absolute errors in prediction for the validation set are then used to compute prediction errors for the test set instances using Conformal Prediction. We show using 24 bioactivity data sets from ChEMBL 23 that Dropout Conformal Predictors are valid (i.e., the fraction of instances whose true value lies within the predicted interval strongly correlates with the confidence level) and efficient, as the predicted confidence intervals span a narrower set of values than those computed with Conformal Predictors generated using Random Forest (RF) models. Lastly, we show in retrospective virtual screening experiments that dropout and RF-based Conformal Predictors lead to comparable retrieval rates of active compounds. Overall, we propose a computationally efficient framework (as only N extra forward passes are required in addition to training a single network) to harness Test-Time Dropout and the Conformal Prediction framework, which is generally applicable to generate reliable prediction errors for Deep Neural Networks in drug discovery and beyond.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7393LW2B\\Cortés-Ciriano_Bender_2019_Reliable Prediction Errors for Deep Neural Networks Using Test-Time Dropout.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7SSWMTKJ\\Cortes-Ciriano_Bender_2019_Reliable Prediction Errors for Deep Neural Networks Using Test-Time Dropout.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8P4RVUU7\\Cortés-Ciriano_Bender_2019_Reliable Prediction Errors for Deep Neural Networks Using Test-Time Dropout2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LUILER9V\\Cortes-Ciriano_Bender_2019_Reliable Prediction Errors for Deep Neural Networks Using Test-Time Dropout2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\4CRDVT6A\\1904.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\ABTSHPQJ\\acs.jcim.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\CCXQZZ4L\\acs.jcim.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\G8J5X5NG\\1904.html}
}

@online{CriticalPerspectivesComputer,
  title = {Critical {{Perspectives}} on {{Computer Vision}}},
  url = {https://slideslive.com/38923500/critical-perspectives-on-computer-vision?ref=og-meta-tags},
  urldate = {2020-06-22},
  organization = {SlidesLive},
  file = {C:\Users\USEBPERP\Zotero\storage\3GU7ACSN\critical-perspectives-on-computer-vision.html}
}

@online{CS231nConvolutionalNeural,
  title = {{{CS231n Convolutional Neural Networks}} for {{Visual Recognition}}},
  url = {https://cs231n.github.io/neural-networks-3/#sgd},
  urldate = {2020-10-16},
  file = {C:\Users\USEBPERP\Zotero\storage\FHPSAVN2\neural-networks-3.html}
}

@article{cummingChemicalPredictiveModelling2013,
  title = {Chemical Predictive Modelling to Improve Compound Quality},
  author = {Cumming, John G. and Davis, Andrew M. and Muresan, Sorel and Haeberlein, Markus and Chen, Hongming},
  date = {2013-12},
  journaltitle = {Nat Rev Drug Discov},
  volume = {12},
  number = {12},
  eprint = {24287782},
  eprinttype = {pmid},
  pages = {948--962},
  issn = {1474-1784},
  doi = {10.1038/nrd4128},
  abstract = {The 'quality' of small-molecule drug candidates, encompassing aspects including their potency, selectivity and ADMET (absorption, distribution, metabolism, excretion and toxicity) characteristics, is a key factor influencing the chances of success in clinical trials. Importantly, such characteristics are under the control of chemists during the identification and optimization of lead compounds. Here, we discuss the application of computational methods, particularly quantitative structure-activity relationships (QSARs), in guiding the selection of higher-quality drug candidates, as well as cultural factors that may have affected their use and impact.},
  langid = {english},
  keywords = {Animals,Drug Compounding,Forecasting,Humans,Models Chemical,Pharmaceutical Preparations,Quantitative Structure-Activity Relationship}
}

@article{dacremaAreWeReally2019,
  title = {Are {{We Really Making Much Progress}}? {{A Worrying Analysis}} of {{Recent Neural Recommendation Approaches}}},
  shorttitle = {Are {{We Really Making Much Progress}}?},
  author = {Dacrema, Maurizio Ferrari and Cremonesi, Paolo and Jannach, Dietmar},
  date = {2019},
  journaltitle = {Proceedings of the 13th ACM Conference on Recommender Systems - RecSys '19},
  eprint = {1907.06902},
  eprinttype = {arxiv},
  pages = {101--109},
  doi = {10.1145/3298689.3347058},
  url = {http://arxiv.org/abs/1907.06902},
  urldate = {2020-03-25},
  abstract = {Deep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difficult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today's research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models. In this work, we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifically, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced with reasonable effort. For these methods, it however turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today's machine learning scholarship and calls for improved scientific practices in this area. Source code of our experiments and full results are available at: https://github.com/MaurizioFD/RecSys2019\_DeepLearning\_Evaluation.},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\8S8WCU4F\\Dacrema et al_2019_Are We Really Making Much Progress.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7MZLQYDE\\1907.html}
}

@online{dagGeorgeOrwellFront,
  title = {George {{Orwell}}: {{In Front}} of {{Your Nose}}},
  shorttitle = {George {{Orwell}}},
  author = {Dag, O.},
  url = {https://www.orwell.ru/library/articles/nose/english/e_nose},
  urldate = {2020-12-09},
  abstract = {In Front of Your Nose, the essay of George Orwell. First published: March 22, 1946 by/in Tribune, GB, London},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\VPQ7CJUG\e_nose.html}
}

@online{dahlumAcceleratingLocalSearch2016,
  title = {Accelerating {{Local Search}} for the {{Maximum Independent Set Problem}}},
  author = {Dahlum, Jakob and Lamm, Sebastian and Sanders, Peter and Schulz, Christian and Strash, Darren and Werneck, Renato F.},
  date = {2016-02-04},
  eprint = {1602.01659},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1602.01659},
  url = {http://arxiv.org/abs/1602.01659},
  urldate = {2023-09-02},
  abstract = {Computing high-quality independent sets quickly is an important problem in combinatorial optimization. Several recent algorithms have shown that kernelization techniques can be used to find exact maximum independent sets in medium-sized sparse graphs, as well as high-quality independent sets in huge sparse graphs that are intractable for exact (exponential-time) algorithms. However, a major drawback of these algorithms is that they require significant preprocessing overhead, and therefore cannot be used to find a high-quality independent set quickly. In this paper, we show that performing simple kernelization techniques in an online fashion significantly boosts the performance of local search, and is much faster than pre-computing a kernel using advanced techniques. In addition, we show that cutting high-degree vertices can boost local search performance even further, especially on huge (sparse) complex networks. Our experiments show that we can drastically speed up the computation of large independent sets compared to other state-of-the-art algorithms, while also producing results that are very close to the best known solutions.},
  pubstate = {preprint},
  keywords = {Computer Science - Data Structures and Algorithms,F.2.2,G.2.2},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\URB7SNY8\\Dahlum et al. - 2016 - Accelerating Local Search for the Maximum Independ.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6LIIMKG7\\1602.html}
}

@inproceedings{dahlumAcceleratingLocalSearch2016a,
  title = {Accelerating {{Local Search}} for the {{Maximum Independent Set Problem}}},
  booktitle = {Experimental {{Algorithms}}},
  author = {Dahlum, Jakob and Lamm, Sebastian and Sanders, Peter and Schulz, Christian and Strash, Darren and Werneck, Renato F.},
  editor = {Goldberg, Andrew V. and Kulikov, Alexander S.},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {118--133},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-38851-9_9},
  abstract = {Computing high-quality independent sets quickly is an important problem in combinatorial optimization. Several recent algorithms have shown that kernelization techniques can be used to find exact maximum independent sets in medium-sized sparse graphs, as well as high-quality independent sets in huge sparse graphs that are intractable for exact (exponential-time) algorithms. However, a major drawback of these algorithms is that they require significant preprocessing overhead, and therefore cannot be used to find a high-quality independent set quickly.},
  isbn = {978-3-319-38851-9},
  langid = {english},
  keywords = {Kernelization,Local search,Maximum independent set,Minimum vertex cover,Reduction},
  file = {C:\Users\USEBPERP\Zotero\storage\8PWX6GIK\Dahlum et al. - 2016 - Accelerating Local Search for the Maximum Independ.pdf}
}

@unpublished{daiHiddenTalentsVariational2017,
  title = {Hidden {{Talents}} of the {{Variational Autoencoder}}},
  author = {Dai, Bin and Wang, Yu and Aston, John and Hua, Gang and Wipf, David},
  date = {2017-06-16},
  eprint = {1706.05148},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.05148},
  urldate = {2018-10-11},
  abstract = {Variational autoencoders (VAE) represent a popular, flexible form of deep generative model that can be stochastically fit to samples from a given random process using an information-theoretic variational bound on the true underlying distribution. Once so-obtained, the model can be putatively used to generate new samples from this distribution, or to provide a low-dimensional latent representation of existing samples. While quite effective in numerous application domains, certain important mechanisms which govern the behavior of the VAE are obfuscated by the intractable integrals and resulting stochastic approximations involved. Moreover, as a highly non-convex model, it remains unclear exactly how minima of the underlying energy relate to original design purposes. We attempt to better quantify these issues by analyzing a series of tractable special cases of increasing complexity. In doing so, we unveil interesting connections with more traditional dimensionality reduction models, as well as an intrinsic yet underappreciated propensity for robustly dismissing sparse outliers when estimating latent manifolds. With respect to the latter, we demonstrate that the VAE can be viewed as the natural evolution of recent robust PCA models, capable of learning nonlinear manifolds of unknown dimension obscured by gross corruptions.},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\EZD3IARU\\Dai et al_2017_Hidden Talents of the Variational Autoencoder.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\NJX7HFIM\\1706.html}
}

@unpublished{daiRetrosynthesisPredictionConditional2020,
  ids = {dai2020retrosynthesisa},
  title = {Retrosynthesis {{Prediction}} with {{Conditional Graph Logic Network}}},
  author = {Dai, Hanjun and Li, Chengtao and Coley, Connor W. and Dai, Bo and Song, Le},
  date = {2020-01-06},
  eprint = {2001.01408},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2001.01408},
  urldate = {2021-04-08},
  abstract = {Retrosynthesis is one of the fundamental problems in organic chemistry. The task is to identify reactants that can be used to synthesize a specified product molecule. Recently, computer-aided retrosynthesis is finding renewed interest from both chemistry and computer science communities. Most existing approaches rely on template-based models that define subgraph matching rules, but whether or not a chemical reaction can proceed is not defined by hard decision rules. In this work, we propose a new approach to this task using the Conditional Graph Logic Network, a conditional graphical model built upon graph neural networks that learns when rules from reaction templates should be applied, implicitly considering whether the resulting reaction would be both chemically feasible and strategic. We also propose an efficient hierarchical sampling to alleviate the computation cost. While achieving a significant improvement of \$8.1\textbackslash\%\$ over current state-of-the-art methods on the benchmark dataset, our model also offers interpretations for the prediction.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\9BQYMBR4\\Dai et al. - 2020 - Retrosynthesis Prediction with Conditional Graph L.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\A5LL5FKG\\2001.html}
}

@unpublished{daiSyntaxDirectedVariationalAutoencoder2018,
  title = {Syntax-{{Directed Variational Autoencoder}} for {{Structured Data}}},
  author = {Dai, Hanjun and Tian, Yingtao and Dai, Bo and Skiena, Steven and Song, Le},
  date = {2018-02-23},
  eprint = {1802.08786},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.08786},
  urldate = {2018-09-17},
  abstract = {Deep generative models have been enjoying success in modeling continuous data. However it remains challenging to capture the representations for discrete structures with formal grammars and semantics, e.g., computer programs and molecular structures. How to generate both syntactically and semantically correct data still remains largely an open problem. Inspired by the theory of compiler where the syntax and semantics check is done via syntax-directed translation (SDT), we propose a novel syntax-directed variational autoencoder (SD-VAE) by introducing stochastic lazy attributes. This approach converts the offline SDT check into on-the-fly generated guidance for constraining the decoder. Comparing to the state-of-the-art methods, our approach enforces constraints on the output space so that the output will be not only syntactically valid, but also semantically reasonable. We evaluate the proposed model with applications in programming language and molecules, including reconstruction and program/molecule optimization. The results demonstrate the effectiveness in incorporating syntactic and semantic constraints in discrete generative models, which is significantly better than current state-of-the-art approaches.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\37JYI5UV\\Dai et al_2018_Syntax-Directed Variational Autoencoder for Structured Data.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LQYBDAX3\\1802.html}
}

@unpublished{damourUnderspecificationPresentsChallenges2020,
  title = {Underspecification {{Presents Challenges}} for {{Credibility}} in {{Modern Machine Learning}}},
  author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
  date = {2020-11-06},
  eprint = {2011.03395},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2011.03395},
  urldate = {2020-11-09},
  abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\YCUZ6UHR\\D'Amour et al_2020_Underspecification Presents Challenges for Credibility in Modern Machine.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YDLWQFNR\\2011.html}
}

@unpublished{dasguptaCausalReasoningMetareinforcement2019,
  title = {Causal {{Reasoning}} from {{Meta-reinforcement Learning}}},
  author = {Dasgupta, Ishita and Wang, Jane and Chiappa, Silvia and Mitrovic, Jovana and Ortega, Pedro and Raposo, David and Hughes, Edward and Battaglia, Peter and Botvinick, Matthew and Kurth-Nelson, Zeb},
  date = {2019-01-23},
  eprint = {1901.08162},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1901.08162},
  urldate = {2019-06-06},
  abstract = {Discovering and exploiting the causal structure in the environment is a crucial challenge for intelligent agents. Here we explore whether causal reasoning can emerge via meta-reinforcement learning. We train a recurrent network with model-free reinforcement learning to solve a range of problems that each contain causal structure. We find that the trained agent can perform causal reasoning in novel situations in order to obtain rewards. The agent can select informative interventions, draw causal inferences from observational data, and make counterfactual predictions. Although established formal causal reasoning algorithms also exist, in this paper we show that such reasoning can arise from model-free reinforcement learning, and suggest that causal reasoning in complex settings may benefit from the more end-to-end learning-based approaches presented here. This work also offers new strategies for structured exploration in reinforcement learning, by providing agents with the ability to perform -- and interpret -- experiments.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\72ITK2UY\\Dasgupta et al_2019_Causal Reasoning from Meta-reinforcement Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LM8QBXCT\\1901.html}
}

@online{DataCompressionExplained,
  title = {Data {{Compression Explained}}},
  url = {http://mattmahoney.net/dc/dce.html#Section_4},
  urldate = {2020-09-01},
  file = {C:\Users\USEBPERP\Zotero\storage\K83YBL49\dce.html}
}

@online{DateienNextcloud,
  title = {Dateien - {{Nextcloud}}},
  url = {https://cloud.ml.jku.at/apps/files/?dir=/&fileid=11067},
  urldate = {2020-01-14},
  file = {C:\Users\USEBPERP\Zotero\storage\PXYY52X5\files.html}
}

@article{daviesDigitizationOrganicSynthesis2019,
  title = {The Digitization of Organic Synthesis},
  author = {Davies, Ian W.},
  date = {2019-06},
  journaltitle = {Nature},
  volume = {570},
  number = {7760},
  pages = {175--181},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1288-y},
  url = {https://www.nature.com/articles/s41586-019-1288-y},
  urldate = {2019-10-14},
  abstract = {This Perspective discusses the challenges associated with the prediction of chemical synthesis, in particular the reaction conditions required for organic transformations, and the role of machine-learning approaches in the prediction process.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\FIY6RDLU\\Davies_2019_The digitization of organic synthesis.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AQ4MBL8K\\s41586-019-1288-y.html}
}

@online{dayhoffPAMPaperPdf,
  title = {{{PAM}} Paper.Pdf},
  author = {Dayhoff},
  url = {http://chagall.med.cornell.edu/BioinfoCourse/PDFs/Lecture2/Dayhoff1978.pdf},
  urldate = {2018-10-24},
  file = {C:\Users\USEBPERP\Zotero\storage\MKTD9K4K\Dayhoff_PAM paper.pdf}
}

@online{DaylightTheoryFingerprints,
  title = {Daylight {{Theory}}: {{Fingerprints}}},
  url = {https://www.daylight.com/dayhtml/doc/theory/theory.finger.html},
  urldate = {2021-05-25},
  file = {C:\Users\USEBPERP\Zotero\storage\VIM3YQ3F\theory.finger.html}
}

@unpublished{decaoMolGANImplicitGenerative2018,
  title = {{{MolGAN}}: {{An}} Implicit Generative Model for Small Molecular Graphs},
  shorttitle = {{{MolGAN}}},
  author = {De Cao, Nicola and Kipf, Thomas},
  date = {2018-05-30},
  eprint = {1805.11973},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.11973},
  urldate = {2018-09-17},
  abstract = {Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100\% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, albeit being susceptible to mode collapse.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7J84X4TP\\De Cao_Kipf_2018_MolGAN2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AZ82MBPY\\De Cao_Kipf_2018_MolGAN4.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DQRGGLMJ\\De Cao_Kipf_2018_MolGAN.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\MJAP6ZP8\\De Cao_Kipf_2018_MolGAN3.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\3UE46PFB\\1805.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\BM6FAYK2\\1805.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\CLMI3Z27\\1805.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\U5MA7YPR\\1805.html}
}

@online{DeepConfidenceComputationally,
  title = {Deep {{Confidence}}: {{A Computationally Efficient Framework}} for {{Calculating Reliable Prediction Errors}} for {{Deep Neural Networks}} - {{Journal}} of {{Chemical Information}} and {{Modeling}} ({{ACS Publications}})},
  url = {https://pubs.acs.org/doi/10.1021/acs.jcim.8b00542},
  urldate = {2019-03-19},
  file = {C:\Users\USEBPERP\Zotero\storage\WSVL42JJ\Deep Confidence.pdf}
}

@online{DeepReinforcementLearning,
  title = {Deep Reinforcement Learning for de Novo Drug Design | {{Science Advances}}},
  url = {https://advances.sciencemag.org/content/4/7/eaap7885},
  urldate = {2019-05-06},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7AUNQR4Z\\eaap7885.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\X4YHRQUL\\eaap7885.html}
}

@online{DefiningExploringChemical,
  title = {Defining and {{Exploring Chemical Spaces}}: {{Trends}} in {{Chemistry}}},
  url = {https://www.cell.com/trends/chemistry/fulltext/S2589-5974(20)30288-4},
  urldate = {2024-01-05},
  file = {C:\Users\USEBPERP\Zotero\storage\M4IWESWQ\S2589-5974(20)30288-4.html}
}

@article{dehghaniUniversalTransformers2018,
  title = {Universal {{Transformers}}},
  author = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=HyzdRiR9Y7},
  urldate = {2019-04-24},
  abstract = {Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their...},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\RLZL97DT\\Dehghani et al_2018_Universal Transformers.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BDRAFS58\\forum.html}
}

@unpublished{depewegDecompositionUncertaintyBayesian2017,
  title = {Decomposition of {{Uncertainty}} in {{Bayesian Deep Learning}} for {{Efficient}} and {{Risk-sensitive Learning}}},
  author = {Depeweg, Stefan and Hernández-Lobato, José Miguel and Doshi-Velez, Finale and Udluft, Steffen},
  date = {2017-10-19},
  eprint = {1710.07283},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1710.07283},
  urldate = {2019-05-08},
  abstract = {Bayesian neural networks with latent variables are scalable and flexible probabilistic models: They account for uncertainty in the estimation of the network weights and, by making use of latent variables, can capture complex noise patterns in the data. We show how to extract and decompose uncertainty into epistemic and aleatoric components for decision-making purposes. This allows us to successfully identify informative points for active learning of functions with heteroscedastic and bimodal noise. Using the decomposition we further define a novel risk-sensitive criterion for reinforcement learning to identify policies that balance expected cost, model-bias and noise aversion.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LV37WZII\\Depeweg et al_2017_Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\X5HMBJXI\\1710.html}
}

@unpublished{devriesDoesObjectRecognition2019,
  ids = {devries2019doesa},
  title = {Does {{Object Recognition Work}} for {{Everyone}}?},
  author = {DeVries, Terrance and Misra, Ishan and Wang, Changhan and family=Maaten, given=Laurens, prefix=van der, useprefix=true},
  date = {2019-06-18},
  eprint = {1906.02659},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1906.02659},
  urldate = {2020-06-23},
  abstract = {The paper analyzes the accuracy of publicly available object-recognition systems on a geographically diverse dataset. This dataset contains household items and was designed to have a more representative geographical coverage than commonly used image datasets in object recognition. We find that the systems perform relatively poorly on household items that commonly occur in countries with a low household income. Qualitative analyses suggest the drop in performance is primarily due to appearance differences within an object class (e.g., dish soap) and due to items appearing in a different context (e.g., toothbrushes appearing outside of bathrooms). The results of our study suggest that further work is needed to make object-recognition systems work equally well for people across different countries and income levels.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6CEH7NWN\\DeVries et al_2019_Does Object Recognition Work for Everyone.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\794J6LW8\\DeVries et al_2019_Does Object Recognition Work for Everyone.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\IH6IGIGK\\1906.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\KI3LXF5H\\1906.html}
}

@online{DglGeometryDGL,
  title = {Dgl.Geometry — {{DGL}} 0.7.2 Documentation},
  url = {https://docs.dgl.ai/en/0.7.x/api/python/dgl.geometry.html},
  urldate = {2022-08-23}
}

@article{dietterichSolvingMultipleInstance1997,
  title = {Solving the Multiple Instance Problem with Axis-Parallel Rectangles},
  author = {Dietterich, Thomas G. and Lathrop, Richard H. and Lozano-Pérez, Tomás},
  date = {1997-01-01},
  journaltitle = {Artificial Intelligence},
  volume = {89},
  number = {1},
  pages = {31--71},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(96)00034-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370296000343},
  urldate = {2021-04-08},
  abstract = {The multiple instance problem arises in tasks where the training examples are ambiguous: a single example object may have many alternative feature vectors (instances) that describe it, and yet only one of those feature vectors may be responsible for the observed classification of the object. This paper describes and compares three kinds of algorithms that learn axis-parallel rectangles to solve the multiple instance problem. Algorithms that ignore the multiple instance problem perform very poorly. An algorithm that directly confronts the multiple instance problem (by attempting to identify which feature vectors are responsible for the observed classifications) performs best, giving 89\% correct predictions on a musk odor prediction task. The paper also illustrates the use of artificial data to debug and compare these algorithms.},
  langid = {english},
  keywords = {Drug design,Machine learning,Structure-activity relationships},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\MV4BF4ZC\\Dietterich et al. - 1997 - Solving the multiple instance problem with axis-pa.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\RKL87M94\\S0004370296000343.html}
}

@article{dimitrovAutonomousMolecularDesign2019,
  title = {Autonomous {{Molecular Design}}: {{Then}} and {{Now}}},
  shorttitle = {Autonomous {{Molecular Design}}},
  author = {Dimitrov, Tanja and Kreisbeck, Christoph and Becker, Jill S. and Aspuru-Guzik, Alán and Saikin, Semion K.},
  date = {2019-03-25},
  journaltitle = {ACS Appl. Mater. Interfaces},
  issn = {1944-8244},
  doi = {10.1021/acsami.9b01226},
  url = {https://pubs.acs.org/doi/abs/10.1021/acsami.9b01226},
  urldate = {2019-07-16},
  abstract = {The success of deep machine learning in processing of large amounts of data, for example, in image or voice recognition and generation, raises the possibilities that these tools can also be applied for solving complex problems in materials science. In this forum article, we focus on molecular design that aims to answer the question on how we can predict and synthesize molecules with tailored physical, chemical, or biological properties. A potential answer to this question could be found by using intelligent systems that integrate physical models and computational machine learning techniques with automated synthesis and characterization tools. Such systems learn through every single experiment in an analogy to a human scientific expert. While the general idea of an autonomous system for molecular synthesis and characterization has been around for a while, its implementations for the materials sciences are sparse. Here we provide an overview of the developments in chemistry automation and the applications of machine learning techniques in the chemical and pharmaceutical industries with a focus on the novel capabilities that deep learning brings in.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\5JGYCXWT\\Dimitrov et al_2019_Autonomous Molecular Design.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8IFLP7KU\\acsami.html}
}

@online{DiversityNetCollaborativeBenchmark,
  title = {{{DiversityNet}}: A Collaborative Benchmark for Generative {{AI}} Models in Chemistry},
  shorttitle = {{{DiversityNet}}},
  url = {https://www.authorea.com/users/226673/articles/285209-diversitynet-a-collaborative-benchmark-for-generative-ai-models-in-chemistry?access_token=moZglmzN-88ww58zdyGFVw},
  urldate = {2018-09-24},
  abstract = {Commenting on the document is possible without registration, but for editing, you need to:   Register on Authorea:~https://www.authorea.com/  Join the DiversityNet gr},
  organization = {Authorea},
  file = {C:\Users\USEBPERP\Zotero\storage\WRLBQI5Z\285209-diversitynet-a-collaborative-benchmark-for-generative-ai-models-in-chemistry.html}
}

@artwork{dmacksDerivingSMILESRepresentation2007,
  title = {Deriving the {{SMILES}} Representation of a Chemical Molecule, {{Shown}} Example:  Ciprofloxacin, a Fluoroquinolone Antibiotic.},
  shorttitle = {Deriving the {{SMILES}} Representation of a Chemical Molecule, {{Shown}} Example},
  author = {DMacks, slight edit by, Original by Fdardel},
  year = {2007-08-13, updated 22:57, 27 June 2010 (UTC)},
  url = {https://commons.wikimedia.org/wiki/File:SMILES.png},
  urldate = {2018-08-15},
  file = {C:\Users\USEBPERP\Zotero\storage\N6U9PXT8\FileSMILES.html}
}

@article{domingosFewUsefulThings2012,
  title = {A Few Useful Things to Know about Machine Learning},
  author = {Domingos, Pedro},
  date = {2012-10-01},
  journaltitle = {Communications of the ACM},
  volume = {55},
  number = {10},
  pages = {78},
  issn = {00010782},
  doi = {10.1145/2347736.2347755},
  url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
  urldate = {2018-12-19},
  abstract = {Machine learning algorithms can figure out how to perform important tasks by generalizing from examples. This is often feasible and cost-effective where manual programming is not. As more data becomes available, more ambitious problems can be tackled. As a result, machine learning is widely used in computer science and other fields. However, developing successful machine learning applications requires a substantial amount of “black art” that is hard to find in textbooks. This article summarizes twelve key lessons that machine learning researchers and practitioners have learned. These include pitfalls to avoid, important issues to focus on, and answers to common questions.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\KK6X8ALH\Domingos_2012_A few useful things to know about machine learning.pdf}
}

@online{DonMakeMine2020,
  title = {Don't {{Make Mine Mink}}},
  date = {2020-11-05T14:07:20-05:00},
  url = {https://blogs.sciencemag.org/pipeline/archives/2020/11/05/dont-make-mine-mink},
  urldate = {2020-11-06},
  abstract = {There's a situation in Denmark that deserves some epidemiological attention. Now, put me in the (rather large) category who did not realize how large the Danish mink industry is - or that a Danish mink industry existed at all - but there are plenty of mink farmers there and millions of mink. 40\% of},
  langid = {american},
  organization = {In the Pipeline},
  file = {C:\Users\USEBPERP\Zotero\storage\52K7VCNN\dont-make-mine-mink.html}
}

@article{douguetGeneticAlgorithmAutomated2000,
  title = {A Genetic Algorithm for the Automated Generation of Small Organic Molecules: {{Drug}} Design Using an Evolutionary Algorithm},
  shorttitle = {A Genetic Algorithm for the Automated Generation of Small Organic Molecules},
  author = {Douguet, Dominique and Thoreau, Etienne and Grassy, Gérard},
  date = {2000-07-01},
  journaltitle = {J Comput Aided Mol Des},
  volume = {14},
  number = {5},
  pages = {449--466},
  issn = {1573-4951},
  doi = {10.1023/A:1008108423895},
  url = {https://doi.org/10.1023/A:1008108423895},
  urldate = {2020-04-09},
  abstract = {Rational drug design involves finding solutions to large combinatorial problems for which an exhaustive search is impractical. Genetic algorithms provide a novel tool for the investigation of such problems. These are a class of algorithms that mimic some of the major characteristics of Darwinian evolution. LEA has been designed in order to conceive novel small organic molecules which satisfy quantitative structure-activity relationship based rules (fitness). The fitness consists of a sum of constraints that are range properties. The algorithm takes an initial set of fragments and iteratively improves them by means of crossover and mutation operators that are related to those involved in Darwinian evolution. The basis of the algorithm, its implementation and parameterization, are described together with an application in de novo molecular design of new retinoids. The results may be promising for chemical synthesis and show that this tool may find extensive applications in de novo drug design projects.},
  langid = {english}
}

@online{DRACONDisconnectedGraph2021,
  title = {{{DRACON}}: {{Disconnected Graph Neural Network}} for {{Atom Mapping}} in {{Chemical Reactions}}},
  date = {2021-04-19},
  url = {https://chemrxiv.org/articles/preprint/DRACON_Disconnected_Graph_Neural_Network_for_Atom_Mapping_in_Chemical_Reactions/12594785/2},
  urldate = {2021-04-19}
}

@article{dragosPredictingPredictabilityUnified2009,
  title = {Predicting the {{Predictability}}: {{A Unified Approach}} to the {{Applicability Domain Problem}} of {{QSAR Models}}},
  shorttitle = {Predicting the {{Predictability}}},
  author = {Dragos, Horvath and Gilles, Marcou and Alexandre, Varnek},
  date = {2009-07-27},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {49},
  number = {7},
  pages = {1762--1776},
  issn = {1549-9596},
  doi = {10.1021/ci9000579},
  url = {https://doi.org/10.1021/ci9000579},
  urldate = {2019-10-08},
  abstract = {The present work proposes a unified conceptual framework to describe and quantify the important issue of the Applicability Domains (AD) of Quantitative Structure−Activity Relationships (QSARs). AD models are conceived as meta-models μμ designed to associate an untrustworthiness score to any molecule M subject to property prediction by a QSAR model μ. Untrustworthiness scores or “AD metrics” Ψμ(M) are an expression of the relationship between M (represented by its descriptors in chemical space) and the space zones populated by the training molecules at the basis of model μ. Scores integrating some of the classical AD criteria (similarity-based, box-based) were considered in addition to newly invented terms such as the consensus prediction variance, the dissimilarity to outlier-free training sets, and the correlation breakdown count (the former two being most successful). A loose correlation is expected to exist between this untrustworthiness and the error |Pμ(M)-Pexpt(M)| affecting the property Pμ(M) predicted by μ. While high untrustworthiness does not preclude correct predictions, inaccurate predictions at low untrustworthiness must be imperatively avoided. This kind of relationship is characteristic for the Neighborhood Behavior (NB) problem: dissimilar molecule pairs may or may not display similar properties, but similar molecule pairs with different properties are explicitly “forbidden”. Therefore, statistical tools developed to tackle this latter aspect were applied and lead to a unified AD metric benchmarking scheme. A first use of untrustworthiness scores resides in prioritization of predictions, without the need to specify a hard AD border. Moreover, if a significant set of external compounds is available, the formalism allows optimal AD borderlines to be fitted. Eventually, consensus AD definitions were built by means of a nonparametric mixing scheme of two AD metrics of comparable quality and shown to outperform their respective parents.},
  file = {C:\Users\USEBPERP\Zotero\storage\T9A229GT\ci9000579.html}
}

@article{drakakisElucidatingCompoundMechanism2019,
  title = {Elucidating {{Compound Mechanism}} of {{Action}} and {{Predicting Cytotoxicity Using Machine Learning Approaches}}, {{Taking Prediction Confidence}} into {{Account}}},
  author = {Drakakis, Georgios and Cortés‐Ciriano, Isidro and Alexander‐Dann, Ben and Bender, Andreas},
  date = {2019},
  journaltitle = {Current Protocols in Chemical Biology},
  volume = {11},
  number = {3},
  pages = {e73},
  issn = {2160-4762},
  doi = {10.1002/cpch.73},
  url = {https://currentprotocols.onlinelibrary.wiley.com/doi/abs/10.1002/cpch.73},
  urldate = {2019-08-19},
  abstract = {The modes of action (MoAs) of drugs frequently are unknown, because many are small molecules initially identified from phenotypic screens, giving rise to the need to elucidate their MoAs. In addition, the high attrition rate for candidate drugs in preclinical studies due to intolerable toxicity has motivated the development of computational approaches to predict drug candidate (cyto)toxicity as early as possible in the drug-discovery process. Here, we provide detailed instructions for capitalizing on bioactivity predictions to elucidate the MoAs of small molecules and infer their underlying phenotypic effects. We illustrate how these predictions can be used to infer the underlying antidepressive effects of marketed drugs. We also provide the necessary functionalities to model cytotoxicity data using single and ensemble machine-learning algorithms. Finally, we give detailed instructions on how to calculate confidence intervals for individual predictions using the conformal prediction framework. © 2019 by John Wiley \& Sons, Inc.},
  langid = {english},
  keywords = {ChEMBL,cytotoxicity,in silico bioactivity prediction,mechanism of action,polypharmacology,toxicology modeling},
  file = {C:\Users\USEBPERP\Zotero\storage\265G6ZS8\cpch.html}
}

@online{DruGANAdvancedGenerative,
  title = {{{druGAN}}: {{An Advanced Generative Adversarial Autoencoder Model}} for de {{Novo Generation}} of {{New Molecules}} with {{Desired Molecular Properties}} in {{Silico}} | {{Molecular Pharmaceutics}}},
  url = {https://pubs.acs.org/doi/10.1021/acs.molpharmaceut.7b00346},
  urldate = {2020-03-17},
  file = {C:\Users\USEBPERP\Zotero\storage\TRMDMV4Y\acs.molpharmaceut.html}
}

@unpublished{DrugDiscoveryDevelopment2011,
  type = {Health \& Medicine},
  title = {Drug Discovery and Development},
  date = {2011-12-27},
  url = {https://www.slideshare.net/rahul_pharma/drug-discovery-and-development-10698574},
  urldate = {2018-07-03},
  abstract = {Brief account of drug discovery by Molecular Designing and Drug development.}
}

@book{DrugDiscoveryToday,
  title = {Drug {{Discovery Today}}: {{Technologies}}},
  shorttitle = {Drug {{Discovery Today}}},
  url = {https://www.journals.elsevier.com/drug-discovery-today-technologies/most-downloaded-articles},
  urldate = {2021-06-07},
  abstract = {The most downloaded articles from Drug Discovery Today: Technologies in the last 90 days.},
  file = {C:\Users\USEBPERP\Zotero\storage\ZDLS69KI\most-downloaded-articles.html}
}

@unpublished{duanOneShotImitationLearning2017,
  title = {One-{{Shot Imitation Learning}}},
  author = {Duan, Yan and Andrychowicz, Marcin and Stadie, Bradly C. and Ho, Jonathan and Schneider, Jonas and Sutskever, Ilya and Abbeel, Pieter and Zaremba, Wojciech},
  date = {2017-03-21},
  eprint = {1703.07326},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.07326},
  urldate = {2019-06-06},
  abstract = {Imitation learning has been commonly applied to solve different tasks in isolation. This usually requires either careful feature engineering, or a significant number of samples. This is far from what we desire: ideally, robots should be able to learn from very few demonstrations of any given task, and instantly generalize to new situations of the same task, without requiring task-specific engineering. In this paper, we propose a meta-learning framework for achieving such capability, which we call one-shot imitation learning. Specifically, we consider the setting where there is a very large set of tasks, and each task has many instantiations. For example, a task could be to stack all blocks on a table into a single tower, another task could be to place all blocks on a table into two-block towers, etc. In each case, different instances of the task would consist of different sets of blocks with different initial states. At training time, our algorithm is presented with pairs of demonstrations for a subset of all tasks. A neural net is trained that takes as input one demonstration and the current state (which initially is the initial state of the other demonstration of the pair), and outputs an action with the goal that the resulting sequence of states and actions matches as closely as possible with the second demonstration. At test time, a demonstration of a single instance of a new task is presented, and the neural net is expected to perform well on new instances of this new task. The use of soft attention allows the model to generalize to conditions and tasks unseen in the training data. We anticipate that by training this model on a much greater variety of tasks and settings, we will obtain a general system that can turn any demonstrations into robust policies that can accomplish an overwhelming variety of tasks. Videos available at https://bit.ly/nips2017-oneshot .},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\FAK3VVG6\\Duan et al_2017_One-Shot Imitation Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\P7BNK7S6\\1703.html}
}

@article{duchiAdaptiveSubgradientMethods2011,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  pages = {2121--2159},
  issn = {ISSN 1533-7928},
  url = {http://jmlr.org/papers/v12/duchi11a.html},
  urldate = {2018-05-20},
  issue = {Jul},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\WWXL7WUI\\Duchi et al_2011_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\T652DGLB\\duchi11a.html}
}

@incollection{duelenMedicinalBiotechnologyDisease2019,
  title = {Medicinal {{Biotechnology}} for {{Disease Modeling}}, {{Clinical Therapy}}, and {{Drug Discovery}} and {{Development}}},
  booktitle = {Introduction to {{Biotech Entrepreneurship}}: {{From Idea}} to {{Business}}: {{A European Perspective}}},
  author = {Duelen, Robin and Corvelyn, Marlies and Tortorella, Ilaria and Leonardi, Leonardo and Chai, Yoke Chin and Sampaolesi, Maurilio},
  editor = {Matei, Florentina and Zirra, Daniela},
  date = {2019},
  pages = {89--128},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-22141-6_5},
  url = {https://doi.org/10.1007/978-3-030-22141-6_5},
  urldate = {2022-09-28},
  abstract = {Over the past decades, stem cell technology has revolutionalized medical biotechnology due to the unlimited self-renewal ability and differentiation capacity of stem cells to generate cells and tissues of the entire human body. Many efforts have focused on providing cutting-edge stem cell therapies in order to repair or replace damaged cells or tissues, hoping to ultimately cure devastating diseases. Undoubtedly, this novel technology guarantees a serial entrepreneur’s confidence in the future prospects of stem cell-based products and services. Here, we describe the state of the art of several applications of adult stem cells, as well as of embryonic and induced pluripotent stem cells in biotechnology that represent entrepreneurial opportunities. Although the contribution of stem cells to medical research is enormous, several hurdles still have to be overcome, including ethical and regulatory issues, functional maturation of stem cell progenitors, stringent manufacturing guidelines, immune rejection, and tumorigenicity.},
  isbn = {978-3-030-22141-6},
  langid = {english},
  keywords = {Computer-aided drug discovery and toxicity,Disease modeling,Entrepreneurship,Organ-in-a-dish,Regenerative medicine,Stem cell biotechnology,Veterinary medicine}
}

@unpublished{duvenaudConvolutionalNetworksGraphs2015,
  title = {Convolutional {{Networks}} on {{Graphs}} for {{Learning Molecular Fingerprints}}},
  author = {Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and Gómez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alán and Adams, Ryan P.},
  date = {2015-09-30},
  eprint = {1509.09292},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1509.09292},
  urldate = {2018-09-17},
  abstract = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\SPLYWS4V\\Duvenaud et al_2015_Convolutional Networks on Graphs for Learning Molecular Fingerprints.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\UP8TNC9F\\1509.html}
}

@online{EfficientAlgorithmsMining,
  title = {Efficient Algorithms for Mining Outliers from Large Data Sets | {{ACM SIGMOD Record}}},
  url = {https://dl.acm.org/doi/10.1145/335191.335437},
  urldate = {2023-04-11},
  file = {C:\Users\USEBPERP\Zotero\storage\IV92ED5Q\335191.html}
}

@inproceedings{eldarFarthestPointStrategy1994,
  title = {The Farthest Point Strategy for Progressive Image Sampling},
  booktitle = {Proceedings of the 12th {{IAPR International Conference}} on {{Pattern Recognition}}, {{Vol}}. 2 - {{Conference B}}: {{Computer Vision}} \& {{Image Processing}}. ({{Cat}}. {{No}}.{{94CH3440-5}})},
  author = {Eldar, Y. and Lindenbaum, M. and Porat, M. and Zeevi, Y.Y.},
  date = {1994-10},
  pages = {93-97 vol.3},
  doi = {10.1109/ICPR.1994.577129},
  abstract = {A new method of "farthest point strategy" (FPS) for progressive image acquisition is presented. Its main advantage is in retaining its uniformity with the increased density, providing an efficient means for sparse image sampling and display. In contrast to previously presented stochastic approaches, the FPS guarantees the uniformity in a deterministic min-max sense. Within this uniformity criterion, the sampling points are irregularly spaced, exhibiting superior antialiasing properties. A straightforward modification of the FPS yields an image-dependent adaptive sampling scheme. An efficient, O(N log(N)), algorithm for both versions is introduced, and several applications of the FPS are discussed.},
  eventtitle = {Proceedings of the 12th {{IAPR International Conference}} on {{Pattern Recognition}}, {{Vol}}. 2 - {{Conference B}}: {{Computer Vision}} \& {{Image Processing}}. ({{Cat}}. {{No}}.{{94CH3440-5}})},
  keywords = {Computational efficiency,Image reconstruction,Image resolution,Image sampling,Image segmentation,Interpolation,Mean square error methods,Sampling methods,Stochastic processes,Surface reconstruction},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\N77F75X3\\Eldar et al. - 1994 - The farthest point strategy for progressive image .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GLZDJ3LR\\577129.html}
}

@online{elsayedWeReallyNeed2021,
  title = {Do {{We Really Need Deep Learning Models}} for {{Time Series Forecasting}}?},
  author = {Elsayed, Shereen and Thyssens, Daniela and Rashed, Ahmed and Jomaa, Hadi Samer and Schmidt-Thieme, Lars},
  date = {2021-10-20},
  eprint = {2101.02118},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2101.02118},
  urldate = {2023-02-09},
  abstract = {Time series forecasting is a crucial task in machine learning, as it has a wide range of applications including but not limited to forecasting electricity consumption, traffic, and air quality. Traditional forecasting models rely on rolling averages, vector auto-regression and auto-regressive integrated moving averages. On the other hand, deep learning and matrix factorization models have been recently proposed to tackle the same problem with more competitive performance. However, one major drawback of such models is that they tend to be overly complex in comparison to traditional techniques. In this paper, we report the results of prominent deep learning models with respect to a well-known machine learning baseline, a Gradient Boosting Regression Tree (GBRT) model. Similar to the deep neural network (DNN) models, we transform the time series forecasting task into a window-based regression problem. Furthermore, we feature-engineered the input and output structure of the GBRT model, such that, for each training window, the target values are concatenated with external features, and then flattened to form one input instance for a multi-output GBRT model. We conducted a comparative study on nine datasets for eight state-of-the-art deep-learning models that were presented at top-level conferences in the last years. The results demonstrate that the window-based input transformation boosts the performance of a simple GBRT model to levels that outperform all state-of-the-art DNN models evaluated in this paper.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\I9TX872L\\Elsayed et al. - 2021 - Do We Really Need Deep Learning Models for Time Se.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\4BI59MNC\\2101.html}
}

@online{ElsevierEnhancedReader,
  title = {Elsevier {{Enhanced Reader}}},
  doi = {10.1016/j.ijforecast.2021.11.013},
  url = {https://reader.elsevier.com/reader/sd/pii/S0169207021001874?token=EDDCF560E07A4B516CD40ABDB0578BA9F8BD09CED2C3AB157C8ED7AD44443890D54F603341C562E2DB7D532B66E74949&originRegion=eu-west-1&originCreation=20230201100038},
  urldate = {2023-02-01},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\VDUNQ94D\Elsevier Enhanced Reader.pdf}
}

@article{eltonDeepLearningMolecular2019,
  title = {Deep Learning for Molecular Design—a Review of the State of the Art},
  author = {Elton, Daniel C. and Boukouvalas, Zois and Fuge, Mark D. and Chung, Peter W.},
  date = {2019-08-05},
  journaltitle = {Mol. Syst. Des. Eng.},
  volume = {4},
  number = {4},
  pages = {828--849},
  publisher = {The Royal Society of Chemistry},
  issn = {2058-9689},
  doi = {10.1039/C9ME00039A},
  url = {https://pubs.rsc.org/en/content/articlelanding/2019/me/c9me00039a},
  urldate = {2020-04-20},
  abstract = {In the space of only a few years, deep generative modeling has revolutionized how we think of artificial creativity, yielding autonomous systems which produce original images, music, and text. Inspired by these successes, researchers are now applying deep generative modeling techniques to the generation and optimization of molecules—in our review we found 45 papers on the subject published in the past two years. These works point to a future where such systems will be used to generate lead molecules, greatly reducing resources spent downstream synthesizing and characterizing bad leads in the lab. In this review we survey the increasingly complex landscape of models and representation schemes that have been proposed. The four classes of techniques we describe are recursive neural networks, autoencoders, generative adversarial networks, and reinforcement learning. After first discussing some of the mathematical fundamentals of each technique, we draw high level connections and comparisons with other techniques and expose the pros and cons of each. Several important high level themes emerge as a result of this work, including the shift away from the SMILES string representation of molecules towards more sophisticated representations such as graph grammars and 3D representations, the importance of reward function design, the need for better standards for benchmarking and testing, and the benefits of adversarial training and reinforcement learning over maximum likelihood based training.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BGTPZHV8\\Elton et al. - 2019 - Deep learning for molecular design—a review of the.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\E6TMTVDW\\Elton et al. - 2019 - Deep learning for molecular design—a review of the.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\M944KRR7\\Elton et al_2019_Deep learning for molecular design—a review of the state of the art.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VM66BY7H\\Elton et al_2019_Deep learning for molecular design—a review of the state of the art.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BFUUXEU8\\unauth.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\GV2TVYF4\\C9ME00039A.html}
}

@online{EndlichVerstehtMich,
  title = {„{{Endlich}} Versteht Mich Jemand“},
  url = {https://www.falter.at/zeitung/20180410/endlich-versteht-mich-jemand},
  urldate = {2020-11-04},
  abstract = {Wieso treibt ein 17-jähriger aus gutem Hause einen 12-Jährigen zu einem Anschlag an? Ein Wiener Strafprozess gibt viele Antworten},
  file = {C:\Users\USEBPERP\Zotero\storage\3IMT7U7J\endlich-versteht-mich-jemand.html}
}

@artwork{EnglishCaffeine3D2010,
  title = {English:  {{Caffeine 3D}} Structure},
  date = {2010-01-11},
  url = {https://commons.wikimedia.org/wiki/File:Caffeine_3d_structure.png},
  urldate = {2024-06-09},
  file = {C:\Users\USEBPERP\Zotero\storage\FKZTNDTA\FileCaffeine_3d_structure.html}
}

@online{EnhancingRetrosyntheticReaction2019,
  title = {Enhancing {{Retrosynthetic Reaction Prediction}} with {{Deep Learning Using Multiscale Reaction Classification}} | {{Journal}} of {{Chemical Information}} and {{Modeling}}},
  date = {2019},
  url = {https://pubs.acs.org/doi/abs/10.1021/acs.jcim.8b00801},
  urldate = {2021-04-08},
  file = {C:\Users\USEBPERP\Zotero\storage\MFZSME5L\acs.jcim.html}
}

@online{EnhancingRetrosyntheticReaction2021,
  title = {Enhancing {{Retrosynthetic Reaction Prediction}} with {{Deep Learning Using Multiscale Reaction Classification}} | {{Journal}} of {{Chemical Information}} and {{Modeling}}},
  date = {2021-01-20},
  url = {https://pubs.acs.org/doi/10.1021/acs.jcim.8b00801},
  urldate = {2021-01-20}
}

@online{EntangledConditionalAdversarial,
  title = {Entangled {{Conditional Adversarial Autoencoder}} for de {{Novo Drug Discovery}} - {{Molecular Pharmaceutics}} ({{ACS Publications}})},
  url = {https://pubs.acs.org/doi/10.1021/acs.molpharmaceut.8b00839},
  urldate = {2019-05-06},
  file = {C:\Users\USEBPERP\Zotero\storage\2ZF34HBB\acs.molpharmaceut.html}
}

@online{Enumeration166Billion,
  title = {Enumeration of 166 {{Billion Organic Small Molecules}} in the {{Chemical Universe Database GDB-17}} | {{Journal}} of {{Chemical Information}} and {{Modeling}}},
  url = {https://pubs.acs.org/doi/abs/10.1021/ci300415d},
  urldate = {2020-03-17},
  file = {C:\Users\USEBPERP\Zotero\storage\UQL8UUIB\ci300415d.html}
}

@article{erikssonSelectionTrainingSet2000,
  title = {On the Selection of the Training Set in Environmental {{QSAR}} Analysis When Compounds Are Clustered},
  author = {Eriksson, Lennart and Johansson, Erik and Müller, Martin and Wold, Svante},
  date = {2000},
  journaltitle = {Journal of Chemometrics},
  volume = {14},
  number = {5-6},
  pages = {599--616},
  issn = {1099-128X},
  doi = {10.1002/1099-128X(200009/12)14:5/6<599::AID-CEM619>3.0.CO;2-8},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1099-128X%28200009/12%2914%3A5/6%3C599%3A%3AAID-CEM619%3E3.0.CO%3B2-8},
  urldate = {2022-07-26},
  abstract = {In QSAR analysis in environmental sciences, adverse effects of chemicals released to the environment are modelled and predicted as a function of the chemical properties of the pollutants. Usually the set of compounds under study contains several classes of substances, i.e. a more or less strongly clustered set. It is then needed to ensure that the selected training set comprises compounds representing all those chemical classes. Multivariate design in the principal properties of the compound classes is usually appropriate for selecting a meaningful training set. However, with clustered data, often seen in environmental chemistry and toxicology, a single multivariate design may be suboptimal because of the risk of ignoring small classes with few members and only selecting training set compounds from the largest classes. Recently a procedure for training set selection recognizing clustering was proposed by us. In this approach, when non-selective biological or environmental responses are modelled, local multivariate designs are constructed within each cluster (class). The chosen compounds arising from the local designs are finally united in the overall training set, which thus will contain members from all clusters. The proposed strategy is here further tested and elaborated by applying it to a series of 351 chemical substances for which the soil sorption coefficient is available. These compounds are divided into 14 classes containing between 10 and 52 members. The training set selection is discussed, followed by multivariate QSAR modelling, model interpretation and predictions for the test set. Various types of statistical experimental designs are tested during the training set selection phase. Copyright © 2000 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {multivariate design,multivariate QSAR,PCA,PLS,soil sorption},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\Z8ITDVXP\\Eriksson et al. - 2000 - On the selection of the training set in environmen.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BLZGVYT2\\6599AID-CEM6193.0.html}
}

@article{ertlEstimationSyntheticAccessibility2009,
  title = {Estimation of Synthetic Accessibility Score of Drug-like Molecules Based on Molecular Complexity and Fragment Contributions},
  author = {Ertl, Peter and Schuffenhauer, Ansgar},
  date = {2009-06-10},
  journaltitle = {Journal of Cheminformatics},
  volume = {1},
  number = {1},
  pages = {8},
  issn = {1758-2946},
  doi = {10.1186/1758-2946-1-8},
  url = {https://doi.org/10.1186/1758-2946-1-8},
  urldate = {2019-03-19},
  abstract = {A method to estimate ease of synthesis (synthetic accessibility) of drug-like molecules is needed in many areas of the drug discovery process. The development and validation of such a method that is able to characterize molecule synthetic accessibility as a score between 1 (easy to make) and 10 (very difficult to make) is described in this article.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\WIHPG3XE\\Ertl_Schuffenhauer_2009_Estimation of synthetic accessibility score of drug-like molecules based on.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YK4P3XHR\\1758-2946-1-8.html}
}

@unpublished{ertlSilicoGenerationNovel2017,
  title = {In Silico Generation of Novel, Drug-like Chemical Matter Using the {{LSTM}} Neural Network},
  author = {Ertl, Peter and Lewis, Richard and Martin, Eric and Polyakov, Valery},
  date = {2017-12-20},
  eprint = {1712.07449},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/1712.07449},
  urldate = {2018-09-24},
  abstract = {The exploration of novel chemical spaces is one of the most important tasks of cheminformatics when supporting the drug discovery process. Properly designed and trained deep neural networks can provide a viable alternative to brute-force de novo approaches or various other machine-learning techniques for generating novel drug-like molecules. In this article we present a method to generate molecules using a long short-term memory (LSTM) neural network and provide an analysis of the results, including a virtual screening test. Using the network one million drug-like molecules were generated in 2 hours. The molecules are novel, diverse (contain numerous novel chemotypes), have good physicochemical properties and have good synthetic accessibility, even though these qualities were not specific constraints. Although novel, their structural features and functional groups remain closely within the drug-like space defined by the bioactive molecules from ChEMBL. Virtual screening using the profile QSAR approach confirms that the potential of these novel molecules to show bioactivity is comparable to the ChEMBL set from which they were derived. The molecule generator written in Python used in this study is available on request.},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\MB4UIBH5\\Ertl et al_2017_In silico generation of novel, drug-like chemical matter using the LSTM neural.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\T4ISH5SP\\1712.html}
}

@online{ETHEperiodica,
  title = {{{ETH}} - e-Periodica},
  url = {https://www.e-periodica.ch/digbib/view?pid=bsv-002:1902:38::503#110},
  urldate = {2019-09-18},
  file = {C:\Users\USEBPERP\Zotero\storage\97AH3H8U\view.html}
}

@unpublished{everittUnderstandingAgentIncentives2019,
  title = {Understanding {{Agent Incentives}} Using {{Causal Influence Diagrams}}. {{Part I}}: {{Single Action Settings}}},
  shorttitle = {Understanding {{Agent Incentives}} Using {{Causal Influence Diagrams}}. {{Part I}}},
  author = {Everitt, Tom and Ortega, Pedro A. and Barnes, Elizabeth and Legg, Shane},
  date = {2019-02-26},
  eprint = {1902.09980},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1902.09980},
  urldate = {2019-06-06},
  abstract = {Agents are systems that optimize an objective function in an environment. Together, the goal and the environment induce secondary objectives, incentives. Modeling the agent-environment interaction in graphical models called influence diagrams, we can answer two fundamental questions about an agent's incentives directly from the graph: (1) which nodes is the agent incentivized to observe, and (2) which nodes is the agent incentivized to influence? The answers tell us which information and influence points need extra protection. For example, we may want a classifier for job applications to not use the ethnicity of the candidate, and a reinforcement learning agent not to take direct control of its reward mechanism. Different algorithms and training paradigms can lead to different influence diagrams, so our method can be used to identify algorithms with problematic incentives and help in designing algorithms with better incentives.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.6,I.2.8},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\L3JLPYL4\\Everitt et al_2019_Understanding Agent Incentives using Causal Influence Diagrams.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\KRWLEP35\\1902.html}
}

@online{ExascaleSearchMolecules,
  title = {Exascale Search of Molecules},
  url = {https://mila.quebec/en/ai-society/exascale-search-of-molecules/},
  urldate = {2020-12-04},
  langid = {american},
  organization = {Mila},
  file = {C:\Users\USEBPERP\Zotero\storage\PCYAXKFK\exascale-search-of-molecules.html}
}

@unpublished{eysenbachSearchReplayBuffer2019,
  title = {Search on the {{Replay Buffer}}: {{Bridging Planning}} and {{Reinforcement Learning}}},
  shorttitle = {Search on the {{Replay Buffer}}},
  author = {Eysenbach, Benjamin and Salakhutdinov, Ruslan and Levine, Sergey},
  date = {2019-06-12},
  eprint = {1906.05253},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1906.05253},
  urldate = {2019-06-14},
  abstract = {The history of learning for control has been an exciting back and forth between two broad classes of algorithms: planning and reinforcement learning. Planning algorithms effectively reason over long horizons, but assume access to a local policy and distance metric over collision-free paths. Reinforcement learning excels at learning policies and the relative values of states, but fails to plan over long horizons. Despite the successes of each method in various domains, tasks that require reasoning over long horizons with limited feedback and high-dimensional observations remain exceedingly challenging for both planning and reinforcement learning algorithms. Frustratingly, these sorts of tasks are potentially the most useful, as they are simple to design (a human only need to provide an example goal state) and avoid reward shaping, which can bias the agent towards finding a sub-optimal solution. We introduce a general control algorithm that combines the strengths of planning and reinforcement learning to effectively solve these tasks. Our aim is to decompose the task of reaching a distant goal state into a sequence of easier tasks, each of which corresponds to reaching a subgoal. Planning algorithms can automatically find these waypoints, but only if provided with suitable abstractions of the environment -- namely, a graph consisting of nodes and edges. Our main insight is that this graph can be constructed via reinforcement learning, where a goal-conditioned value function provides edge weights, and nodes are taken to be previously seen observations in a replay buffer. Using graph search over our replay buffer, we can automatically generate this sequence of subgoals, even in image-based environments. Our algorithm, search on the replay buffer (SoRB), enables agents to solve sparse reward tasks over one hundred steps, and generalizes substantially better than standard RL algorithms.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\L4J7ZPYQ\\Eysenbach et al_2019_Search on the Replay Buffer.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\NGBN5AHU\\1906.html}
}

@online{FailureModesMolecule,
  title = {On Failure Modes in Molecule Generation and Optimization | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.ddtec.2020.09.003},
  url = {https://reader.elsevier.com/reader/sd/pii/S1740674920300159?token=AF26E58AB8DEFFF210645ADF06DD707D631B373CEBBC429E6B5485DBB9AC16B825453FE0632179C0782527B86180A5E7&originRegion=eu-west-1&originCreation=20220428140926},
  urldate = {2022-04-29},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\84DCGBVH\\On failure modes in molecule generation and optimi.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8I44F9EG\\S1740674920300159.html}
}

@unpublished{fangKnowledgeawareContrastiveMolecular2021,
  title = {Knowledge-Aware {{Contrastive Molecular Graph Learning}}},
  author = {Fang, Yin and Yang, Haihong and Zhuang, Xiang and Shao, Xin and Fan, Xiaohui and Chen, Huajun},
  date = {2021-03-24},
  eprint = {2103.13047},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2103.13047},
  urldate = {2021-04-08},
  abstract = {Leveraging domain knowledge including fingerprints and functional groups in molecular representation learning is crucial for chemical property prediction and drug discovery. When modeling the relation between graph structure and molecular properties implicitly, existing works can hardly capture structural or property changes and complex structure, with much smaller atom vocabulary and highly frequent atoms. In this paper, we propose the Contrastive Knowledge-aware GNN (CKGNN) for self-supervised molecular representation learning to fuse domain knowledge into molecular graph representation. We explicitly encode domain knowledge via knowledge-aware molecular encoder under the contrastive learning framework, ensuring that the generated molecular embeddings equipped with chemical domain knowledge to distinguish molecules with similar chemical formula but dissimilar functions. Extensive experiments on 8 public datasets demonstrate the effectiveness of our model with a 6\textbackslash\% absolute improvement on average against strong competitors. Ablation study and further investigation also verify the best of both worlds: incorporation of chemical domain knowledge into self-supervised learning.},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\SQGFRIQ9\\Fang et al. - 2021 - Knowledge-aware Contrastive Molecular Graph Learni.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\5IRKMX5D\\2103.html}
}

@article{fanUnsupervisedAnomalyDetection2023,
  title = {Unsupervised {{Anomaly Detection}} for {{Intermittent Sequences Based}} on {{Multi-Granularity Abnormal Pattern Mining}}},
  author = {Fan, Lilin and Zhang, Jiahu and Mao, Wentao and Cao, Fukang},
  date = {2023-01},
  journaltitle = {Entropy},
  volume = {25},
  number = {1},
  pages = {123},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e25010123},
  url = {https://www.mdpi.com/1099-4300/25/1/123},
  urldate = {2023-02-06},
  abstract = {In the actual maintenance of manufacturing enterprises, abnormal changes in after-sale parts demand data often make the inventory strategies unreasonable. Due to the intermittent and small-scale characteristics of demand sequences, it is difficult to accurately identify the anomalies in such sequences using current anomaly detection algorithms. To solve this problem, this paper proposes an unsupervised anomaly detection method for intermittent time series. First, a new abnormal fluctuation similarity matrix is built by calculating the squared coefficient of variation and the maximum information coefficient from the macroscopic granularity. The abnormal fluctuation sequence can then be adaptively screened by using agglomerative hierarchical clustering. Second, the demand change feature and interval feature of the abnormal sequence are constructed and fed into the support vector data description model to perform hypersphere training. Then, the unsupervised abnormal point location detection is realized at the micro-granularity level from the abnormal sequence. Comparative experiments are carried out on the actual demand data of after-sale parts of two large manufacturing enterprises. The results show that, compared with the current representative anomaly detection methods, the proposed approach can effectively identify the abnormal fluctuation position in the intermittent sequence of small samples, and also obtain better detection results.},
  issue = {1},
  langid = {english},
  keywords = {after-sale parts management,anomaly detection,intermittent sequence,safety stock,unsupervised learning},
  file = {C:\Users\USEBPERP\Zotero\storage\JCAW4SA8\Fan et al. - 2023 - Unsupervised Anomaly Detection for Intermittent Se.pdf}
}

@online{FATECVSchedule,
  title = {FATE/CV - Schedule},
  url = {https://sites.google.com/view/fatecv-tutorial/schedule},
  urldate = {2020-06-22},
  langid = {ngerman},
  file = {C:\Users\USEBPERP\Zotero\storage\DLDALYA9\schedule.html}
}

@online{februaryTerrificPaperProblems2016,
  ids = {february2016terrifica},
  title = {A {{Terrific Paper}} on the {{Problems}} in {{Drug Discovery}}},
  author = {February, Derek Lowe 18 and {2016}},
  date = {2016-02-18T09:28:00+00:00},
  url = {https://blogs.sciencemag.org/pipeline/archives/2016/02/18/a-terrific-paper-on-the-problems-in-drug-discovery},
  urldate = {2020-07-09},
  abstract = {Here's a really interesting paper from consultants Jack Scannell and Jim Bosley in PLoS ONE, on the productivity crisis in drug discovery. Several things distinguish it: for one, it's not just another "whither the drug industry" think piece, of which we have plenty already. This one get quantitative},
  langid = {american},
  organization = {In the Pipeline},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\8YVNTN24\\a-terrific-paper-on-the-problems-in-drug-discovery.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\KNLHZUNM\\a-terrific-paper-on-the-problems-in-drug-discovery.html}
}

@unpublished{feldmanDoesLearningRequire2019,
  title = {Does {{Learning Require Memorization}}? {{A Short Tale}} about a {{Long Tail}}},
  shorttitle = {Does {{Learning Require Memorization}}?},
  author = {Feldman, Vitaly},
  date = {2019-06-12},
  eprint = {1906.05271},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.05271},
  urldate = {2019-06-14},
  abstract = {State-of-the-art results on image recognition tasks are achieved using over-parameterized learning algorithms that (nearly) perfectly fit the training set. This phenomenon is referred to as data interpolation or, informally, as memorization of the training data. The question of why such algorithms generalize well to unseen data is not adequately addressed by the standard theoretical frameworks and, as a result, significant theoretical and experimental effort has been devoted to understanding the properties of such algorithms. We provide a simple and generic model for prediction problems in which interpolating the dataset is necessary for achieving close-to-optimal generalization error. The model is motivated and supported by the results of several recent empirical works. In our model, data is sampled from a mixture of subpopulations and the frequencies of these subpopulations are chosen from some prior. The model allows to quantify the effect of not fitting the training data on the generalization performance of the learned classifier and demonstrates that memorization is necessary whenever frequencies are long-tailed. Image and text data are known to follow such distributions and therefore our results establish a formal link between these empirical phenomena. To the best of our knowledge, this is the first general framework that demonstrates statistical benefits of plain memorization for learning. Our results also have concrete implications for the cost of ensuring differential privacy in learning.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\YJDEBR6G\\Feldman_2019_Does Learning Require Memorization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QCCC8863\\1906.html}
}

@unpublished{fengPADMEDeepLearningbased2018,
  title = {{{PADME}}: {{A Deep Learning-based Framework}} for {{Drug-Target Interaction Prediction}}},
  shorttitle = {{{PADME}}},
  author = {Feng, Qingyuan and Dueva, Evgenia and Cherkasov, Artem and Ester, Martin},
  date = {2018-07-25},
  eprint = {1807.09741},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1807.09741},
  urldate = {2018-09-17},
  abstract = {In silico Drug-target Interaction (DTI) prediction is an important and challenging problem in medicinal chemistry with a huge potential benefit to the pharmaceutical industry and patients. Most existing methods for DTI prediction generally have binary endpoints, which could be an oversimplification of the problem. With the advent of deep learning, some deep learning models were devised to solve the DTI prediction problem, but most of them still use binary endpoints, and they are generally unable to handle cold-target problems, i.e., problems involving target protein that never appeared in the training set. We contrived PADME (Protein And Drug Molecule interaction prEdiction), a framework based on Deep Neural Networks, to predict real-valued interaction strength between compounds and proteins. PADME inputs both compound and protein information into the model, so it is applicable to cold-target problems. To our knowledge, we are also the first to incorporate Molecular Graph Convolution (MGC) into the model for compound featurization. We used different Cross-Validation split schemes and different metrics to measure the performance of PADME on multiple datasets (in which we are the first to use ToxCast for such problems), and PADME consistently dominates baseline methods. We also conducted a case study, predicting the interaction between compounds and androgen receptor (AR) and compared the prediction results with growth inhibition activity of the compounds in NCI60, which also gave us satisfactory results, suggesting PADME's potential in drug development. We expect different variants of PADME to be proposed and experimented on in the future, and we believe Deep Learning will transform the field of cheminformatics.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\I8ZDLBQX\\Feng et al_2018_PADME.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\85XS5JXI\\1807.html}
}

@inproceedings{fontaineDifferentiableQualityDiversity2021,
  title = {Differentiable {{Quality Diversity}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Fontaine, Matthew and Nikolaidis, Stefanos},
  date = {2021},
  volume = {34},
  pages = {10040--10052},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2021/hash/532923f11ac97d3e7cb0130315b067dc-Abstract.html},
  urldate = {2022-08-29},
  abstract = {Quality diversity (QD) is a growing branch of stochastic optimization research that studies the problem of generating an archive of solutions that maximize a given objective function but are also diverse with respect to a set of specified measure functions. However, even when these functions are differentiable, QD algorithms treat them as "black boxes", ignoring gradient information. We present the differentiable quality diversity (DQD) problem, a special case of QD, where both the objective and measure functions are first order differentiable. We then present MAP-Elites via a Gradient Arborescence (MEGA), a DQD algorithm that leverages gradient information to efficiently explore the joint range of the objective and measure functions. Results in two QD benchmark domains and in searching the latent space of a StyleGAN show that MEGA significantly outperforms state-of-the-art QD algorithms, highlighting DQD's promise for efficient quality diversity optimization when gradient information is available. Source code is available at https://github.com/icaros-usc/dqd.},
  file = {C:\Users\USEBPERP\Zotero\storage\FZCUVIZY\Fontaine and Nikolaidis - 2021 - Differentiable Quality Diversity.pdf}
}

@article{foorthuisNatureTypesAnomalies2021,
  title = {On the Nature and Types of Anomalies: A Review of Deviations in Data},
  shorttitle = {On the Nature and Types of Anomalies},
  author = {Foorthuis, Ralph},
  date = {2021},
  journaltitle = {Int J Data Sci Anal},
  volume = {12},
  number = {4},
  eprint = {34368422},
  eprinttype = {pmid},
  pages = {297--331},
  issn = {2364-415X},
  doi = {10.1007/s41060-021-00265-1},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8331998/},
  urldate = {2023-01-24},
  abstract = {Anomalies are occurrences in a dataset that are in some way unusual and do not fit the general patterns. The concept of the anomaly is typically ill defined and perceived as vague and domain-dependent. Moreover, despite some 250~years of publications on the topic, no comprehensive and concrete overviews of the different types of anomalies have hitherto been published. By means of an extensive literature review this study therefore offers the first theoretically principled and domain-independent typology of data anomalies and presents a full overview of anomaly types and subtypes. To concretely define the concept of the anomaly and its different manifestations, the typology employs five dimensions: data type, cardinality of relationship, anomaly level, data structure, and data distribution. These fundamental and data-centric dimensions naturally yield 3 broad groups, 9 basic types, and 63 subtypes of anomalies. The typology facilitates the evaluation of the functional capabilities of anomaly detection algorithms, contributes to explainable data science, and provides insights into relevant topics such as local versus global anomalies.},
  pmcid = {PMC8331998},
  file = {C:\Users\USEBPERP\Zotero\storage\FCH4XFN3\Foorthuis - 2021 - On the nature and types of anomalies a review of .pdf}
}

@unpublished{fordAdversarialExamplesAre2019,
  title = {Adversarial {{Examples Are}} a {{Natural Consequence}} of {{Test Error}} in {{Noise}}},
  author = {Ford, Nic and Gilmer, Justin and Carlini, Nicolas and Cubuk, Dogus},
  date = {2019-01-29},
  eprint = {1901.10513},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1901.10513},
  urldate = {2020-02-14},
  abstract = {Over the last few years, the phenomenon of adversarial examples --- maliciously constructed inputs that fool trained machine learning models --- has captured the attention of the research community, especially when the adversary is restricted to small modifications of a correctly handled input. Less surprisingly, image classifiers also lack human-level performance on randomly corrupted images, such as images with additive Gaussian noise. In this paper we provide both empirical and theoretical evidence that these are two manifestations of the same underlying phenomenon, establishing close connections between the adversarial robustness and corruption robustness research programs. This suggests that improving adversarial robustness should go hand in hand with improving performance in the presence of more general and realistic image corruptions. Based on our results we recommend that future adversarial defenses consider evaluating the robustness of their methods to distributional shift with benchmarks such as Imagenet-C.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BV58HAXZ\\Ford et al_2019_Adversarial Examples Are a Natural Consequence of Test Error in Noise.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2IJY2XPK\\1901.html}
}

@article{fortunatoDataAugmentationPretraining2020,
  ids = {fortunato2020datab,fortunatoDataAugmentationPretraining2020a},
  title = {Data {{Augmentation}} and {{Pretraining}} for {{Template-Based Retrosynthetic Prediction}} in {{Computer-Aided Synthesis Planning}}},
  author = {Fortunato, Michael E. and Coley, Connor W. and Barnes, Brian C. and Jensen, Klavs F.},
  date = {2020-07-27},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {60},
  number = {7},
  pages = {3398--3407},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.0c00403},
  url = {https://doi.org/10.1021/acs.jcim.0c00403},
  urldate = {2020-11-18},
  abstract = {This work presents efforts to augment the performance of data-driven machine learning algorithms for reaction template recommendation used in computer-aided synthesis planning software. Often, machine learning models designed to perform the task of prioritizing reaction templates or molecular transformations are focused on reporting high-accuracy metrics for the one-to-one mapping of product molecules in reaction databases to the template extracted from the recorded reaction. The available templates that get selected for inclusion in these machine learning models have been previously limited to those that appear frequently in the reaction databases and exclude potentially useful transformations. By augmenting open-access data sets of organic reactions with explicitly calculated template applicability and pretraining a template-relevance neural network on this augmented applicability data set, we report an increase in the template applicability recall and an increase in the diversity of predicted precursors. The augmentation and pretraining effectively teaches the neural network an increased set of templates that could theoretically lead to successful reactions for a given target. Even on a small data set of well-curated reactions, the data augmentation and pretraining methods resulted in an increase in top-1 accuracy, especially for rare templates, indicating that these strategies can be very useful for small data sets.},
  issue = {7},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BCHFSGAB\\Fortunato et al. - 2020 - Data Augmentation and Pretraining for Template-Bas.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZSH2K3CZ\\Fortunato et al. - 2020 - Data Augmentation and Pretraining for Template-Bas.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\27IY2Z4M\\acs.jcim.html}
}

@article{fortunatoDataAugmentationPretraining2020a,
  title = {Data {{Augmentation}} and {{Pretraining}} for {{Template-Based Retrosynthetic Prediction}} in {{Computer-Aided Synthesis Planning}}},
  author = {Fortunato, Michael and Coley, Connor W. and Barnes, Brian and Jensen, Klavs F.},
  date = {2020-02-07},
  publisher = {ChemRxiv},
  doi = {10.26434/chemrxiv.11811564.v1},
  url = {/articles/preprint/Data_Augmentation_and_Pretraining_for_Template-Based_Retrosynthetic_Prediction_in_Computer-Aided_Synthesis_Planning/11811564/1},
  urldate = {2021-01-20},
  abstract = {This work presents efforts to augment the performance of data-driven machine learning algorithms for reaction template recommendation used in computer-aided synthesis planning software. Often, machine learning models designed to perform the task of prioritizing reaction templates or molecular transformations are focused on reporting high accuracy metrics for the one-to-one mapping of product molecules in reaction databases to the template extracted from the recorded reaction. The available templates that get selected for inclusion in these machine learning models have been previously limited to those that appear frequently in the reaction databases and exclude potentially useful transformations. By augmenting open-access datasets of organic reactions with artificially calculated template applicability and pretraining a template relevance neural network on this augmented applicability dataset, we report an increase in the template applicability recall and an increase in the diversity of predicted precursors. The augmentation and pretraining effectively teaches the neural network an increased set of templates that could theoretically lead to successful reactions for a given target. Even on a small dataset of well curated reactions, the data augmentation and pretraining methods resulted in an increase in top-1 accuracy, especially for rare templates, indicating these strategies can be very useful for small datasets.},
  langid = {english}
}

@article{fortunatoMachineLearnedPrediction2020,
  title = {Machine {{Learned Prediction}} of {{Reaction Template Applicability}} for {{Data-Driven Retrosynthetic Predictions}} of {{Energetic Materials}}},
  author = {Fortunato, Michael and Coley, Connor W. and Barnes, Brian and Jensen, Klavs F.},
  date = {2020-03-31},
  publisher = {ChemRxiv},
  doi = {10.26434/chemrxiv.12046623.v1},
  url = {/articles/preprint/Machine_Learned_Prediction_of_Reaction_Template_Applicability_for_Data-Driven_Retrosynthetic_Predictions_of_Energetic_Materials/12046623/1},
  urldate = {2021-01-22},
  abstract = {State of the art computer-aided synthesis planning models are naturally biased toward commonly reported chemical reactions, thus reducing the usefulness of those models for the unusual chemistry relevant to shock physics. To address this problem, a neural network was trained to recognize reaction template applicability for small organic molecules to supplement the rare reaction examples of relevance to energetic materials. The training data for the neural network was generated by brute force determination of template subgraph matching for product molecules from a database of reactions in U.S. patent literature. This data generation strategy successfully augmented the information about template applicability for rare reaction mechanisms in the reaction database. The increased ability to recognize rare reaction templates was demonstrated for reaction templates of interest for energetic material synthesis such as heterocycle ring formation.The following article has been submitted to by the 21st Biennial APS Conference on Shock Compression of Condensed Matter. After it is published, it will be found at https://publishing.aip.org/resources/librarians/products/journals/.},
  langid = {english}
}

@article{foxHighThroughputScreeningUpdate2006,
  title = {High-{{Throughput Screening}}: {{Update}} on {{Practices}} and {{Success}}},
  shorttitle = {High-{{Throughput Screening}}},
  author = {Fox, Sandra and Farr-Jones, Shauna and Sopchak, Lynne and Boggs, Amy and Nicely, Helen Wang and Khoury, Richard and Biros, Michael},
  date = {2006-10-01},
  journaltitle = {J Biomol Screen},
  volume = {11},
  number = {7},
  pages = {864--869},
  publisher = {SAGE Publications Inc STM},
  issn = {1087-0571},
  doi = {10.1177/1087057106292473},
  url = {https://doi.org/10.1177/1087057106292473},
  urldate = {2020-05-27},
  abstract = {High-throughput screening (HTS) has become an important part of drug discovery at most pharmaceutical and many biotechnology companies worldwide, and use of HTS technologies is expanding into new areas. Target validation, assay development, secondary screening, ADME/Tox, and lead optimization are among the areas in which there is an increasing use of HTS technologies. It is becoming fully integrated within drug discovery, both upstream and downstream, which includes increasing use of cell-based assays and high-content screening (HCS) technologies to achieve more physiologically relevant results and to find higher quality leads. In addition, HTS laboratories are continually evaluating new technologies as they struggle to increase their success rate for finding drug candidates. The material in this article is based on a 900-page HTS industry report involving 54 HTS directors representing 58 HTS laboratories and 34 suppliers.},
  file = {C:\Users\USEBPERP\Zotero\storage\D75GVTRK\Fox et al. - 2006 - High-Throughput Screening Update on Practices and.pdf}
}

@unpublished{frankleEarlyPhaseNeural2020,
  title = {The {{Early Phase}} of {{Neural Network Training}}},
  author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
  date = {2020-02-24},
  eprint = {2002.10365},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.10365},
  urldate = {2020-03-04},
  abstract = {Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge (Frankle et al., 2019), gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period (Achille et al., 2019). Here, we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of Frankle et al. (2019) to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\CYCN32NV\\Frankle et al_2020_The Early Phase of Neural Network Training.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GKMNJZR3\\2002.html}
}

@unpublished{frankleLotteryTicketHypothesis2018,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Small}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  date = {2018-03-09},
  eprint = {1803.03635},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1803.03635},
  urldate = {2018-09-04},
  abstract = {Neural network compression techniques are able to reduce the parameter counts of trained networks by over 90\%--decreasing storage requirements and improving inference performance--without compromising accuracy. However, contemporary experience is that it is difficult to train small architectures from scratch, which would similarly improve training performance. We articulate a new conjecture to explain why it is easier to train large networks: the "lottery ticket hypothesis." It states that large networks that train successfully contain subnetworks that--when trained in isolation--converge in a comparable number of iterations to comparable accuracy. These subnetworks, which we term "winning tickets," have won the initialization lottery: their connections have initial weights that make training particularly effective. We find that a standard technique for pruning unnecessary network weights naturally uncovers a subnetwork which, at the start of training, comprised a winning ticket. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis. We consistently find winning tickets that are less than 20\% of the size of several fully-connected, convolutional, and residual architectures for MNIST and CIFAR10. Furthermore, winning tickets at moderate levels of pruning (20-50\% of the original network size) converge up to 6.7x faster than the original network and exhibit higher test accuracy.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\UMBUTCXP\\Frankle_Carbin_2018_The Lottery Ticket Hypothesis.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\EMXUT239\\1803.html}
}

@article{frechetDistanceDeuxLois1957,
  title = {Sur La Distance de Deux Lois de Probabilité},
  author = {Fréchet, M.},
  date = {1957},
  journaltitle = {C. R. Acad. Sci. Paris},
  volume = {244},
  pages = {689--692}
}

@article{frederickTimeDiscountingTime,
  title = {Time {{Discounting}} and {{Time Preference}}: {{A Critical Review}}},
  author = {Frederick, Shane and Loewenstein, George},
  pages = {52},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\VEFXBDNH\Frederick_Loewenstein_Time Discounting and Time Preference.pdf}
}

@article{freemanExperimentalComparisonSurvey2021,
  title = {Experimental {{Comparison}} and {{Survey}} of {{Twelve Time Series Anomaly Detection Algorithms}}},
  author = {Freeman, Cynthia and Merriman, Jonathan and Beaver, Ian and Mueen, Abdullah},
  date = {2021-11-18},
  journaltitle = {Journal of Artificial Intelligence Research},
  volume = {72},
  pages = {849--899},
  issn = {1076-9757},
  doi = {10.1613/jair.1.12698},
  url = {https://www.jair.org/index.php/jair/article/view/12698},
  urldate = {2022-08-16},
  abstract = {The existence of an anomaly detection method that is optimal for all domains is a myth. Thus, there exists a plethora of anomaly detection methods which increases every year for a wide variety of domains. But a strength can also be a weakness; given this massive library of methods, how can one select the best method for their application? Current literature is focused on creating new anomaly detection methods or large frameworks for experimenting with multiple methods at the same time. However, and especially as the literature continues to expand, an extensive evaluation of every anomaly detection method is simply not feasible. To reduce this evaluation burden, we present guidelines to intelligently choose the optimal anomaly detection methods based on the characteristics the time series displays such as seasonality, trend, level change concept drift, and missing time steps. We provide a comprehensive experimental validation and survey of twelve anomaly detection methods over different time series characteristics to form guidelines based on several metrics: the AUC (Area Under the Curve), windowed F-score, and Numenta Anomaly Benchmark (NAB) scoring model. Applying our methodologies can save time and effort by surfacing the most promising anomaly detection methods instead of experimenting extensively with a rapidly expanding library of anomaly detection methods, especially in an online setting.},
  langid = {english},
  keywords = {data mining,knowledge acquisition,knowledge discovery,knowledge engineering},
  file = {C:\Users\USEBPERP\Zotero\storage\9G7SV4VA\Freeman et al. - 2021 - Experimental Comparison and Survey of Twelve Time .pdf}
}

@article{fromerComputeraidedMultiobjectiveOptimization2023,
  title = {Computer-Aided Multi-Objective Optimization in Small Molecule Discovery},
  author = {Fromer, Jenna C. and Coley, Connor W.},
  date = {2023-02-10},
  journaltitle = {Patterns},
  volume = {4},
  number = {2},
  pages = {100678},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2023.100678},
  url = {https://www.sciencedirect.com/science/article/pii/S2666389923000016},
  urldate = {2024-04-03},
  abstract = {Molecular discovery is a multi-objective optimization problem that requires identifying a molecule or set of molecules that balance multiple, often competing, properties. Multi-objective molecular design is commonly addressed by combining properties of interest into a single objective function using scalarization, which imposes assumptions about relative importance and uncovers little about the trade-offs between objectives. In contrast to scalarization, Pareto optimization does not require knowledge of relative importance and reveals the trade-offs between objectives. However, it introduces additional considerations in algorithm design. In this review, we describe pool-based and de novo generative approaches to multi-objective molecular discovery with a focus on Pareto optimization algorithms. We show how pool-based molecular discovery is a relatively direct extension of multi-objective Bayesian optimization and how the plethora of different generative models extend from single-objective to multi-objective optimization in similar ways using non-dominated sorting in the reward function (reinforcement learning) or to select molecules for retraining (distribution learning) or propagation (genetic algorithms). Finally, we discuss some remaining challenges and opportunities in the field, emphasizing the opportunity to adopt Bayesian optimization techniques into multi-objective de novo design.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2A5MIJG5\\Fromer and Coley - 2023 - Computer-aided multi-objective optimization in sma.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\MP6US9MS\\S2666389923000016.html}
}

@online{fuMIMOSAMulticonstraintMolecule2022,
  title = {{{MIMOSA}}: {{Multi-constraint Molecule Sampling}} for {{Molecule Optimization}}},
  shorttitle = {{{MIMOSA}}},
  author = {Fu, Tianfan and Xiao, Cao and Li, Xinhao and Glass, Lucas M. and Sun, Jimeng},
  date = {2022-02-27},
  eprint = {2010.02318},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.02318},
  url = {http://arxiv.org/abs/2010.02318},
  urldate = {2023-10-29},
  abstract = {Molecule optimization is a fundamental task for accelerating drug discovery, with the goal of generating new valid molecules that maximize multiple drug properties while maintaining similarity to the input molecule. Existing generative models and reinforcement learning approaches made initial success, but still face difficulties in simultaneously optimizing multiple drug properties. To address such challenges, we propose the MultI-constraint MOlecule SAmpling (MIMOSA) approach, a sampling framework to use input molecule as an initial guess and sample molecules from the target distribution. MIMOSA first pretrains two property agnostic graph neural networks (GNNs) for molecule topology and substructure-type prediction, where a substructure can be either atom or single ring. For each iteration, MIMOSA uses the GNNs' prediction and employs three basic substructure operations (add, replace, delete) to generate new molecules and associated weights. The weights can encode multiple constraints including similarity and drug property constraints, upon which we select promising molecules for next iteration. MIMOSA enables flexible encoding of multiple property- and similarity-constraints and can efficiently generate new molecules that satisfy various property constraints and achieved up to 49.6\% relative improvement over the best baseline in terms of success rate. The code repository (including readme file, data preprocessing and model construction, evaluation) is available https://github.com/futianfan/MIMOSA.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\GJ5P5B2K\\Fu et al. - 2022 - MIMOSA Multi-constraint Molecule Sampling for Mol.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\U6DUZTGY\\2010.html}
}

@unpublished{galDropoutBayesianApproximation2015,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  date = {2015-06-06},
  eprint = {1506.02142},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1506.02142},
  urldate = {2019-04-12},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BHP68ETV\\Gal_Ghahramani_2015_Dropout as a Bayesian Approximation2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\IK6H2YQ9\\Gal_Ghahramani_2015_Dropout as a Bayesian Approximation.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\K2X47B5B\\1506.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\UVI2XZMP\\1506.html}
}

@online{gaoAmortizedTreeGeneration2021,
  title = {Amortized {{Tree Generation}} for {{Bottom-up Synthesis Planning}} and {{Synthesizable Molecular Design}}},
  author = {Gao, Wenhao and Mercado, Rocío and Coley, Connor W.},
  date = {2021-10-12},
  url = {https://arxiv.org/abs/2110.06389v2},
  urldate = {2023-10-09},
  abstract = {Molecular design and synthesis planning are two critical steps in the process of molecular discovery that we propose to formulate as a single shared task of conditional synthetic pathway generation. We report an amortized approach to generate synthetic pathways as a Markov decision process conditioned on a target molecular embedding. This approach allows us to conduct synthesis planning in a bottom-up manner and design synthesizable molecules by decoding from optimized conditional codes, demonstrating the potential to solve both problems of design and synthesis simultaneously. The approach leverages neural networks to probabilistically model the synthetic trees, one reaction step at a time, according to reactivity rules encoded in a discrete action space of reaction templates. We train these networks on hundreds of thousands of artificial pathways generated from a pool of purchasable compounds and a list of expert-curated templates. We validate our method with (a) the recovery of molecules using conditional generation, (b) the identification of synthesizable structural analogs, and (c) the optimization of molecular structures given oracle functions relevant to drug discovery.},
  langid = {english},
  organization = {arXiv.org},
  file = {C:\Users\USEBPERP\Zotero\storage\2NZACWP8\Gao et al. - 2021 - Amortized Tree Generation for Bottom-up Synthesis .pdf}
}

@article{gaoSampleEfficiencyMatters,
  title = {Sample {{Efﬁciency Matters}}: {{A Benchmark}} for {{Practical Molecular Optimization}}},
  author = {Gao, Wenhao and Fu, Tianfan and Sun, Jimeng and Coley, Connor W},
  abstract = {Molecular optimization is a fundamental goal in the chemical sciences and is of central interest to drug and material design. In recent years, significant progress has been made in solving challenging problems across various aspects of computational molecular optimizations, emphasizing high validity, diversity, and, most recently, synthesizability. Despite this progress, many papers report results on trivial or selfdesigned tasks, bringing additional challenges to directly assessing the performance of new methods. Moreover, the sample efficiency of the optimization—the number of molecules evaluated by the oracle—is rarely discussed, despite being an essential consideration for realistic discovery applications.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\UREERD8I\\Gao et al. - Sample Efﬁciency Matters A Benchmark for Practica.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WMKLFE37\\Gao et al. - Sample Efﬁciency Matters A Benchmark for Practica.pdf}
}

@online{gaoSampleEfficiencyMatters2022,
  title = {Sample {{Efficiency Matters}}: {{A Benchmark}} for {{Practical Molecular Optimization}}},
  shorttitle = {Sample {{Efficiency Matters}}},
  author = {Gao, Wenhao and Fu, Tianfan and Sun, Jimeng and Coley, Connor W.},
  date = {2022-06-22},
  eprint = {2206.12411},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  doi = {10.48550/arXiv.2206.12411},
  url = {http://arxiv.org/abs/2206.12411},
  urldate = {2022-07-04},
  abstract = {Molecular optimization is a fundamental goal in the chemical sciences and is of central interest to drug and material design. In recent years, significant progress has been made in solving challenging problems across various aspects of computational molecular optimizations, emphasizing high validity, diversity, and, most recently, synthesizability. Despite this progress, many papers report results on trivial or self-designed tasks, bringing additional challenges to directly assessing the performance of new methods. Moreover, the sample efficiency of the optimization--the number of molecules evaluated by the oracle--is rarely discussed, despite being an essential consideration for realistic discovery applications. To fill this gap, we have created an open-source benchmark for practical molecular optimization, PMO, to facilitate the transparent and reproducible evaluation of algorithmic advances in molecular optimization. This paper thoroughly investigates the performance of 25 molecular design algorithms on 23 tasks with a particular focus on sample efficiency. Our results show that most "state-of-the-art" methods fail to outperform their predecessors under a limited oracle budget allowing 10K queries and that no existing algorithm can efficiently solve certain molecular optimization problems in this setting. We analyze the influence of the optimization algorithm choices, molecular assembly strategies, and oracle landscapes on the optimization performance to inform future algorithm development and benchmarking. PMO provides a standardized experimental setup to comprehensively evaluate and compare new molecule optimization methods with existing ones. All code can be found at https://github.com/wenhao-gao/mol\_opt.},
  pubstate = {preprint},
  keywords = {Computer Science - Computational Engineering Finance and Science,Quantitative Biology - Biomolecules},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JTTA6G59\\Gao et al. - 2022 - Sample Efficiency Matters A Benchmark for Practic.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AIFPLEGA\\2206.html}
}

@article{gaoSynthesizabilityMoleculesProposed2020,
  ids = {gao2020synthesizabilitya,gao2020synthesizabilityb},
  title = {The {{Synthesizability}} of {{Molecules Proposed}} by {{Generative Models}}},
  author = {Gao, Wenhao and Coley, Connor W.},
  date = {2020-04-06},
  journaltitle = {J. Chem. Inf. Model.},
  eprint = {2002.07007},
  eprinttype = {arxiv},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.0c00174},
  url = {https://doi.org/10.1021/acs.jcim.0c00174},
  urldate = {2020-04-22},
  abstract = {The discovery of functional molecules is an expensive and time-consuming process, exemplified by the rising costs of small molecule therapeutic discovery. One class of techniques of growing interest for early stage drug discovery is de novo molecular generation and optimization, catalyzed by the development of new deep learning approaches. These techniques can suggest novel molecular structures intended to maximize a multiobjective function, e.g., suitability as a therapeutic against a particular target, without relying on brute-force exploration of a chemical space. However, the utility of these approaches is stymied by ignorance of synthesizability. To highlight the severity of this issue, we use a data-driven computer-aided synthesis planning program to quantify how often molecules proposed by state-of-the-art generative models cannot be readily synthesized. Our analysis demonstrates that there are several tasks for which these models generate unrealistic molecular structures despite performing well on popular quantitative benchmarks. Synthetic complexity heuristics can successfully bias generation toward synthetically tractable chemical space, although doing so necessarily detracts from the primary objective. This analysis suggests that to improve the utility of these models in real discovery workflows, new algorithm development is warranted.},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\J4N54K87\\Gao_Coley_2020_The Synthesizability of Molecules Proposed by Generative Models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\PDGBD2DD\\Gao_Coley_2020_The Synthesizability of Molecules Proposed by Generative Models2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DA2WJ6MM\\2002.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\NNIKFX93\\2002.html}
}

@online{GarbageGarbageOut,
  title = {Garbage {{In}}, {{Garbage Out}}},
  url = {https://www.law.georgetown.edu/privacy-technology-center/publications/garbage-in-garbage-out-face-recognition-on-flawed-data/},
  urldate = {2020-06-23},
  langid = {american},
  file = {C:\Users\USEBPERP\Zotero\storage\UUVGATB9\garbage-in-garbage-out-face-recognition-on-flawed-data.html}
}

@article{garcia-ortegonDOCKSTRINGEasyMolecular2022,
  title = {{{DOCKSTRING}}: {{Easy Molecular Docking Yields Better Benchmarks}} for {{Ligand Design}}},
  shorttitle = {{{DOCKSTRING}}},
  author = {García-Ortegón, Miguel and Simm, Gregor N. C. and Tripp, Austin J. and Hernández-Lobato, José Miguel and Bender, Andreas and Bacallado, Sergio},
  date = {2022-08-08},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {62},
  number = {15},
  pages = {3486--3502},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.1c01334},
  url = {https://doi.org/10.1021/acs.jcim.1c01334},
  urldate = {2023-10-09},
  abstract = {The field of machine learning for drug discovery is witnessing an explosion of novel methods. These methods are often benchmarked on simple physicochemical properties such as solubility or general druglikeness, which can be readily computed. However, these properties are poor representatives of objective functions in drug design, mainly because they do not depend on the candidate compound’s interaction with the target. By contrast, molecular docking is a widely applied method in drug discovery to estimate binding affinities. However, docking studies require a significant amount of domain knowledge to set up correctly, which hampers adoption. Here, we present dockstring, a bundle for meaningful and robust comparison of ML models using docking scores. dockstring consists of three components: (1) an open-source Python package for straightforward computation of docking scores, (2) an extensive dataset of docking scores and poses of more than 260,000 molecules for 58 medically relevant targets, and (3) a set of pharmaceutically relevant benchmark tasks such as virtual screening or de novo design of selective kinase inhibitors. The Python package implements a robust ligand and target preparation protocol that allows nonexperts to obtain meaningful docking scores. Our dataset is the first to include docking poses, as well as the first of its size that is a full matrix, thus facilitating experiments in multiobjective optimization and transfer learning. Overall, our results indicate that docking scores are a more realistic evaluation objective than simple physicochemical properties, yielding benchmark tasks that are more challenging and more closely related to real problems in drug discovery.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\PZMUKW4N\\García-Ortegón et al. - 2022 - DOCKSTRING Easy Molecular Docking Yields Better B.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\KJDCAH5M\\acs.jcim.html}
}

@inproceedings{gatysImageStyleTransfer2016,
  title = {Image {{Style Transfer Using Convolutional Neural Networks}}},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  date = {2016-06},
  pages = {2414--2423},
  publisher = {IEEE},
  doi = {10.1109/CVPR.2016.265},
  url = {http://ieeexplore.ieee.org/document/7780634/},
  urldate = {2018-06-27},
  abstract = {Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic information and, thus, allow to separate image content from style. Here we use image representations derived from Convolutional Neural Networks optimised for object recognition, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can separate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an arbitrary photograph with the appearance of numerous wellknown artworks. Our results provide new insights into the deep image representations learned by Convolutional Neural Networks and demonstrate their potential for high level image synthesis and manipulation.},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\5NLBCPAI\Gatys et al_2016_Image Style Transfer Using Convolutional Neural Networks.pdf}
}

@unpublished{gatysNeuralAlgorithmArtistic2015,
  title = {A {{Neural Algorithm}} of {{Artistic Style}}},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  date = {2015-08-26},
  eprint = {1508.06576},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/1508.06576},
  urldate = {2018-06-05},
  abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6ZVB4MVG\\Gatys et al_2015_A Neural Algorithm of Artistic Style.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\W7IZ2TCB\\1508.html}
}

@inproceedings{gehlerFeatureCombinationMulticlass2009,
  title = {On Feature Combination for Multiclass Object Classification},
  booktitle = {2009 {{IEEE}} 12th {{International Conference}} on {{Computer Vision}}},
  author = {Gehler, P. and Nowozin, S.},
  date = {2009-09},
  pages = {221--228},
  doi = {10.1109/ICCV.2009.5459169},
  abstract = {A key ingredient in the design of visual object classification systems is the identification of relevant class specific aspects while being robust to intra-class variations. While this is a necessity in order to generalize beyond a given set of training images, it is also a very difficult problem due to the high variability of visual appearance within each class. In the last years substantial performance gains on challenging benchmark datasets have been reported in the literature. This progress can be attributed to two developments: the design of highly discriminative and robust image features and the combination of multiple complementary features based on different aspects such as shape, color or texture. In this paper we study several models that aim at learning the correct weighting of different features from training data. These include multiple kernel learning as well as simple baseline methods. Furthermore we derive ensemble methods inspired by Boosting which are easily extendable to several multiclass setting. All methods are thoroughly evaluated on object classification datasets using a multitude of feature descriptors. The key results are that even very simple baseline methods, that are orders of magnitude faster than learning techniques are highly competitive with multiple kernel learning. Furthermore the Boosting type methods are found to produce consistently better results in all experiments. We provide insight of when combination methods can be expected to work and how the benefit of complementary features can be exploited most efficiently.},
  eventtitle = {2009 {{IEEE}} 12th {{International Conference}} on {{Computer Vision}}},
  keywords = {Boosting,Computer vision,Cybernetics,Image classification,Kernel,Object recognition,Performance gain,Robustness,Shape,Training data},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\W6UV642A\\Gehler_Nowozin_2009_On feature combination for multiclass object classification.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HI3RWZ5Q\\5459169.html}
}

@unpublished{geifmanBiasReducedUncertaintyEstimation2018,
  title = {Bias-{{Reduced Uncertainty Estimation}} for {{Deep Neural Classifiers}}},
  author = {Geifman, Yonatan and Uziel, Guy and El-Yaniv, Ran},
  date = {2018-05-21},
  eprint = {1805.08206},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.08206},
  urldate = {2019-09-19},
  abstract = {We consider the problem of uncertainty estimation in the context of (non-Bayesian) deep neural classification. In this context, all known methods are based on extracting uncertainty signals from a trained network optimized to solve the classification problem at hand. We demonstrate that such techniques tend to introduce biased estimates for instances whose predictions are supposed to be highly confident. We argue that this deficiency is an artifact of the dynamics of training with SGD-like optimizers, and it has some properties similar to overfitting. Based on this observation, we develop an uncertainty estimation algorithm that selectively estimates the uncertainty of highly confident points, using earlier snapshots of the trained model, before their estimates are jittered (and way before they are ready for actual classification). We present extensive experiments indicating that the proposed algorithm provides uncertainty estimates that are consistently better than all known methods.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\HFRGP42J\\Geifman et al_2018_Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers5.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VLGCTK6J\\1805.html}
}

@article{gellyMonteCarloTreeSearch2011,
  title = {Monte-{{Carlo}} Tree Search and Rapid Action Value Estimation in Computer {{Go}}},
  author = {Gelly, Sylvain and Silver, David},
  date = {2011-07},
  journaltitle = {Artificial Intelligence},
  volume = {175},
  number = {11},
  pages = {1856--1875},
  issn = {00043702},
  doi = {10.1016/j.artint.2011.03.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S000437021100052X},
  urldate = {2020-06-05},
  abstract = {A new paradigm for search, based on Monte-Carlo simulation, has revolutionised the performance of computer Go programs. In this article we describe two extensions to the Monte-Carlo tree search algorithm, which significantly improve the effectiveness of the basic algorithm. When we applied these two extensions to the Go program MoGo, it became the first program to achieve dan (master) level in 9 × 9 Go. In this article we survey the Monte-Carlo revolution in computer Go, outline the key ideas that led to the success of MoGo and subsequent Go programs, and provide for the first time a comprehensive description, in theory and in practice, of this extended framework for Monte-Carlo tree search.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\UQBY5EN5\Gelly and Silver - 2011 - Monte-Carlo tree search and rapid action value est.pdf}
}

@online{GenerativeRecurrentNetworks,
  title = {Generative {{Recurrent Networks}} for {{De Novo Drug Design}} - {{Gupta}} - 2018 - {{Molecular Informatics}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1002/minf.201700111},
  urldate = {2019-05-06},
  file = {C:\Users\USEBPERP\Zotero\storage\JU69X5I4\minf.html}
}

@book{ghoseCombinatorialLibraryDesign2001,
  title = {Combinatorial Library Design and Evaluation: Principles, Software Tools, and Applications in Drug Discovery},
  shorttitle = {Combinatorial Library Design and Evaluation},
  editor = {Ghose, Arup K. and Viswanadhan, Vellarkad N.},
  date = {2001},
  publisher = {M. Dekker},
  location = {New York},
  isbn = {978-0-8247-0487-2},
  langid = {english},
  pagetotal = {631},
  keywords = {Combinatorial chemistry,Computer simulation,Design,Design Computer simulation,Drugs},
  file = {C:\Users\USEBPERP\Zotero\storage\HPD8RY2W\Ghose and Viswanadhan - 2001 - Combinatorial library design and evaluation princ.pdf}
}

@article{gigerenzerHelpingDoctorsPatients2007,
  title = {Helping {{Doctors}} and {{Patients Make Sense}} of {{Health Statistics}}},
  author = {Gigerenzer, Gerd and Gaissmaier, Wolfgang and Kurz-Milcke, Elke and Schwartz, Lisa M. and Woloshin, Steven},
  date = {2007-11},
  journaltitle = {Psychol Sci Public Interest},
  volume = {8},
  number = {2},
  pages = {53--96},
  issn = {1529-1006, 1539-6053},
  doi = {10.1111/j.1539-6053.2008.00033.x},
  url = {http://journals.sagepub.com/doi/10.1111/j.1539-6053.2008.00033.x},
  urldate = {2022-09-12},
  abstract = {Many doctors, patients, journalists, and politicians alike do not understand what health statistics mean or draw wrong conclusions without noticing. Collective statistical illiteracy refers to the widespread inability to understand the meaning of numbers. For instance, many citizens are unaware that higher survival rates with cancer screening do not imply longer life, or that the statement that mammography screening reduces the risk of dying from breast cancer by 25\% in fact means that 1 less woman out of 1,000 will die of the disease. We provide evidence that statistical illiteracy (a) is common to patients, journalists, and physicians; (b) is created by nontransparent framing of information that is sometimes an unintentional result of lack of understanding but can also be a result of intentional efforts to manipulate or persuade people; and (c) can have serious consequences for health.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\7HC93BNE\Gigerenzer et al. - 2007 - Helping Doctors and Patients Make Sense of Health .pdf}
}

@article{gigerenzerMindlessStatistics2004,
  title = {Mindless Statistics},
  author = {Gigerenzer, Gerd},
  date = {2004-11-01},
  journaltitle = {The Journal of Socio-Economics},
  series = {Statistical {{Significance}}},
  volume = {33},
  number = {5},
  pages = {587--606},
  issn = {1053-5357},
  doi = {10.1016/j.socec.2004.09.033},
  url = {https://www.sciencedirect.com/science/article/pii/S1053535704000927},
  urldate = {2023-01-05},
  abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the “null ritual” consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5\% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.},
  langid = {english},
  keywords = {Collective illusions,Editors,Rituals,Statistical significance,Textbooks},
  file = {C:\Users\USEBPERP\Zotero\storage\VRPZ5GXJ\S1053535704000927.html}
}

@article{gilensTestingTheoriesAmerican2014,
  title = {Testing {{Theories}} of {{American Politics}}: {{Elites}}, {{Interest Groups}}, and {{Average Citizens}}},
  shorttitle = {Testing {{Theories}} of {{American Politics}}},
  author = {Gilens, Martin and Page, Benjamin I.},
  date = {2014-09},
  journaltitle = {Perspectives on Politics},
  volume = {12},
  number = {3},
  pages = {564--581},
  publisher = {Cambridge University Press},
  issn = {1537-5927, 1541-0986},
  doi = {10.1017/S1537592714001595},
  url = {https://www.cambridge.org/core/journals/perspectives-on-politics/article/testing-theories-of-american-politics-elites-interest-groups-and-average-citizens/62327F513959D0A304D4893B382B992B},
  urldate = {2020-07-08},
  abstract = {Each of four theoretical traditions in the study of American politics—which can be characterized as theories of Majoritarian Electoral Democracy, Economic-Elite Domination, and two types of interest-group pluralism, Majoritarian Pluralism and Biased Pluralism—offers different predictions about which sets of actors have how much influence over public policy: average citizens; economic elites; and organized interest groups, mass-based or business-oriented. A great deal of empirical research speaks to the policy influence of one or another set of actors, but until recently it has not been possible to test these contrasting theoretical predictions against each other within a single statistical model. We report on an effort to do so, using a unique data set that includes measures of the key variables for 1,779 policy issues. Multivariate analysis indicates that economic elites and organized groups representing business interests have substantial independent impacts on U.S. government policy, while average citizens and mass-based interest groups have little or no independent influence. The results provide substantial support for theories of Economic-Elite Domination and for theories of Biased Pluralism, but not for theories of Majoritarian Electoral Democracy or Majoritarian Pluralism.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\M4FUANX3\\Gilens and Page - 2014 - Testing Theories of American Politics Elites, Int.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8EJXPQT3\\62327F513959D0A304D4893B382B992B.html}
}

@incollection{gilletComputationalMethodsAnalysis2002,
  title = {Computational Methods for the Analysis of Molecular Diversity},
  booktitle = {Pharmacochemistry {{Library}}},
  author = {Gillet, Valerie J. and Willett, Peter},
  editor = {family=Goot, given=Henk, prefix=van der, useprefix=true},
  date = {2002-01-01},
  series = {Trends in {{Drug Research III}}},
  volume = {32},
  pages = {125--133},
  publisher = {Elsevier},
  doi = {10.1016/S0165-7208(02)80014-9},
  url = {https://www.sciencedirect.com/science/article/pii/S0165720802800149},
  urldate = {2022-06-07},
  abstract = {This chapter discusses the computational methods for the analysis of molecular diversity. Four principal types of selection method are described: cluster-based selection, partition-based selection, dissimilarity-based selection, and optimization-based selection. Cluster-based selection involves subdividing a set of molecules into groups, or clusters, that exhibit a high degree of both intra-cluster similarity and inter-cluster dissimilarity, and a diverse subset is then obtained by choosing one compound from each of the clusters in turn. Partition-based selection requires the identification of a small number of characteristics, these typically being molecular properties that would be expected to affect binding at a receptor site. The range of values for each such characteristic is sub-divided into a set of sub-ranges, and the combinatorial product of all possible sub-ranges then defines the set of cells that make up the partition. Optimization-based approaches involve defining some quantitative measure of diversity, or diversity index, and then formulating the identification of the most diverse subset as a combinatorial optimization problem.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\KZ5GSJC7\\Gillet and Willett - 2002 - Computational methods for the analysis of molecula.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\RNAWP3MV\\S0165720802800149.html}
}

@inproceedings{gilmerNeuralMessagePassing2017,
  title = {Neural {{Message Passing}} for {{Quantum Chemistry}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  date = {2017-07-17},
  pages = {1263--1272},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {http://proceedings.mlr.press/v70/gilmer17a.html},
  urldate = {2021-04-08},
  abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models inva...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\LVAVC9RW\Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf}
}

@article{gneitingStrictlyProperScoring2007,
  title = {Strictly {{Proper Scoring Rules}}, {{Prediction}}, and {{Estimation}}},
  author = {Gneiting, Tilmann and Raftery, Adrian E},
  date = {2007-03},
  journaltitle = {Journal of the American Statistical Association},
  volume = {102},
  number = {477},
  pages = {359--378},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214506000001437},
  url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
  urldate = {2019-06-13},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\IHMXS4BZ\Gneiting and Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Est.pdf}
}

@article{gobbiAtomAtomPathSimilaritySphere2015,
  title = {Atom-{{Atom-Path}} Similarity and {{Sphere Exclusion}} Clustering: Tools for Prioritizing Fragment Hits},
  shorttitle = {Atom-{{Atom-Path}} Similarity and {{Sphere Exclusion}} Clustering},
  author = {Gobbi, Alberto and Giannetti, Anthony M. and Chen, Huifen and Lee, Man-Ling},
  date = {2015-03-25},
  journaltitle = {Journal of Cheminformatics},
  volume = {7},
  number = {1},
  pages = {11},
  issn = {1758-2946},
  doi = {10.1186/s13321-015-0056-8},
  url = {https://doi.org/10.1186/s13321-015-0056-8},
  urldate = {2022-05-11},
  abstract = {After performing a fragment based screen the resulting hits need to be prioritized for follow-up structure elucidation and chemistry. This paper describes a new similarity metric, Atom-Atom-Path (AAP) similarity that is used in conjunction with the Directed Sphere Exclusion (DISE) clustering method to effectively organize and prioritize the fragment hits. The AAP similarity rewards common substructures and recognizes minimal structure differences. The DISE method is order-dependent and can be used to enrich fragments with properties of interest in the first clusters.},
  keywords = {Clustering,Command line program,Fragment screening,Hit prioritization,Similarity,Sphere exclusion},
  file = {C:\Users\USEBPERP\Zotero\storage\UCMZUVK2\Gobbi et al. - 2015 - Atom-Atom-Path similarity and Sphere Exclusion clu.pdf}
}

@article{gobbiDISEDirectedSphere2003,
  title = {{{DISE}}:\, {{Directed Sphere Exclusion}}},
  shorttitle = {{{DISE}}},
  author = {Gobbi, Alberto and Lee, Man-Ling},
  date = {2003-01-01},
  journaltitle = {J. Chem. Inf. Comput. Sci.},
  volume = {43},
  number = {1},
  pages = {317--323},
  publisher = {American Chemical Society},
  issn = {0095-2338},
  doi = {10.1021/ci025554v},
  url = {https://doi.org/10.1021/ci025554v},
  urldate = {2022-05-11},
  abstract = {The Sphere Exclusion algorithm is a well-known algorithm used to select diverse subsets from chemical-compound libraries or collections. It can be applied with any given distance measure between two structures. It is popular because of the intuitive geometrical interpretation of the method and its good performance on large data sets. This paper describes Directed Sphere Exclusion (DISE), a modification of the Sphere Exclusion algorithm, which retains all positive properties of the Sphere Exclusion algorithm but generates a more even distribution of the selected compounds in the chemical space. In addition, the computational requirement is significantly reduced, thus it can be applied to very large data sets.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\UEHNQAJC\\Gobbi and Lee - 2003 - DISE  Directed Sphere Exclusion.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SJY67SML\\ci025554v.html}
}

@article{goddardIndependentDominationGraphs2013,
  title = {Independent Domination in Graphs: {{A}} Survey and Recent Results},
  shorttitle = {Independent Domination in Graphs},
  author = {Goddard, Wayne and Henning, Michael A.},
  date = {2013-04},
  journaltitle = {Discrete Mathematics},
  volume = {313},
  number = {7},
  pages = {839--854},
  issn = {0012365X},
  doi = {10.1016/j.disc.2012.11.031},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0012365X13000083},
  urldate = {2022-05-25},
  abstract = {A set S of vertices in a graph G is an independent dominating set of G if S is an independent set and every vertex not in S is adjacent to a vertex in S. In this paper, we offer a survey of selected recent results on independent domination in graphs.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\M5S2PY7N\Goddard and Henning - 2013 - Independent domination in graphs A survey and rec.pdf}
}

@inreference{GoesaertCleary2020,
  title = {\emph{Goesaert v. }{{\emph{Cleary}}}},
  booktitle = {Wikipedia},
  date = {2020-12-06T09:09:34Z},
  url = {https://en.wikipedia.org/w/index.php?title=Goesaert_v._Cleary&oldid=992634396},
  urldate = {2020-12-23},
  abstract = {Goesaert v. Cleary, 335 U.S. 464 (1948), was a United States Supreme Court case in which the Court upheld a Michigan law, which prohibited women from being licensed as a bartender in all cities having a population of 50,000 or more unless their father or husband owned the establishment. Valentine Goesaert, the plaintiff in the case, challenged the law on the ground that it infringed on the Fourteenth Amendment's Equal Protection Clause. Speaking for the majority, Justice Felix Frankfurter affirmed the judgment of the Detroit district court and upheld the constitutionality of the state law. The state argued that since the profession of bartending could potentially lead to moral and social problems for women, it was within the state's power to bar them from working as bartenders. Only when the owner of the bar was a sufficiently close relative to the women bartender could it be guaranteed that such immorality would not be present. The decision was subsequently overruled by Craig v. Boren.},
  langid = {english},
  annotation = {Page Version ID: 992634396},
  file = {C:\Users\USEBPERP\Zotero\storage\UV8IAD9N\index.html}
}

@inproceedings{goldsteinHistogrambasedOutlierScore2012,
  title = {Histogram-Based {{Outlier Score}} ({{HBOS}}): {{A}} Fast {{Unsupervised Anomaly Detection Algorithm}}},
  shorttitle = {Histogram-Based {{Outlier Score}} ({{HBOS}})},
  author = {Goldstein, Markus and Dengel, Andreas},
  date = {2012-09-26},
  abstract = {Unsupervised anomaly detection is the process of finding outliers in data sets without prior training. In this paper, a histogram-based outlier detection (HBOS) algorithm is presented, which scores records in linear time. It assumes independence of the features making it much faster than multivariate approaches at the cost of less precision. A comparative evaluation on three UCI data sets and 10 standard algorithms show, that it can detect global outliers as reliable as state-of-the-art algorithms, but it performs poor on local outlier problems. HBOS is in our experiments up to 5 times faster than clustering based algorithms and up to 7 times faster than nearest-neighbor based methods.},
  file = {C:\Users\USEBPERP\Zotero\storage\D3S8Q4N2\Goldstein and Dengel - 2012 - Histogram-based Outlier Score (HBOS) A fast Unsup.pdf}
}

@article{gomez-bombarelliAutomaticChemicalDesign2018,
  title = {Automatic {{Chemical Design Using}} a {{Data-Driven Continuous Representation}} of {{Molecules}}},
  author = {Gómez-Bombarelli, Rafael and Wei, Jennifer N. and Duvenaud, David and Hernández-Lobato, José Miguel and Sánchez-Lengeling, Benjamín and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D. and Adams, Ryan P. and Aspuru-Guzik, Alán},
  date = {2018-02-28},
  journaltitle = {ACS Cent. Sci.},
  volume = {4},
  number = {2},
  pages = {268--276},
  issn = {2374-7943},
  doi = {10.1021/acscentsci.7b00572},
  url = {https://doi.org/10.1021/acscentsci.7b00572},
  urldate = {2018-10-09},
  abstract = {We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds. A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder, and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule. Continuous representations of molecules allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the domain of drug-like molecules and also in a set of molecules with fewer that nine heavy atoms.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DQ4AZTVB\\Gómez-Bombarelli et al_2018_Automatic Chemical Design Using a Data-Driven Continuous Representation of.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YCXKLR3K\\Gómez-Bombarelli et al_2018_Automatic Chemical Design Using a Data-Driven Continuous Representation of2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YHABK7J6\\Gómez-Bombarelli et al_2018_Automatic Chemical Design Using a Data-Driven Continuous Representation of3.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\5DM4SSCU\\acscentsci.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\5UIW79H7\\acscentsci.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\SEE2Y2Q8\\acscentsci.html}
}

@unpublished{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  date = {2014-06-10},
  eprint = {1406.2661},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1406.2661},
  urldate = {2018-07-03},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\AMVGHC2R\\Goodfellow et al_2014_Generative Adversarial Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TCA6CVUW\\1406.html}
}

@unpublished{goodfellowQualitativelyCharacterizingNeural2014,
  title = {Qualitatively Characterizing Neural Network Optimization Problems},
  author = {Goodfellow, Ian J. and Vinyals, Oriol and Saxe, Andrew M.},
  date = {2014-12-19},
  eprint = {1412.6544},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1412.6544},
  urldate = {2019-10-02},
  abstract = {Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\YJ5K2L59\\Goodfellow et al_2014_Qualitatively characterizing neural network optimization problems.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6NCV3E2Y\\1412.html}
}

@article{gornitzHiddenMarkovAnomaly,
  title = {Hidden {{Markov Anomaly Detection}}},
  author = {Gornitz, Nico and Braun, Mikio and Kloft, Marius},
  abstract = {We introduce a new anomaly detection methodology for data with latent dependency structure. As a particular instantiation, we derive a hidden Markov anomaly detector that extends the regular one-class support vector machine. We optimize the approach, which is non-convex, via a DC (difference of convex functions) algorithm, and show that the parameter ν can be conveniently used to control the number of outliers in the model. The empirical evaluation on artificial and real data from the domains of computational biology and computational sustainability shows that the approach can achieve significantly higher anomaly detection performance than the regular one-class SVM.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\CHIVZ4N5\\Gornitz et al. - Hidden Markov Anomaly Detection.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\G7UNIQ7J\\Gornitz et al. - Hidden Markov Anomaly Detection.pdf}
}

@article{gorseDiversityMedicinalChemistry2006,
  title = {Diversity in {{Medicinal Chemistry Space}}},
  author = {Gorse, Alain-Dominique},
  date = {2006-01-01},
  journaltitle = {Current Topics in Medicinal Chemistry},
  volume = {6},
  number = {1},
  pages = {3--18},
  abstract = {The chemical universe containing organic molecules within a reasonable molecular weight is vast and largely unexplored. Estimations of possible numbers of unique molecules range from 10 13 to 101 80. These numbers have to be compared with the few tens of millions of compounds currently known. Design of libraries that populate the medicinally relevant chemical subspace and tools that help to maximise the chance of identifying leads are necessary. This review describes various molecular representations that lead to the definition of chemical space, drug space or activity space. Strategies for compound selection in such spaces are discussed, as well as potential sources of diversity that could be used to explore the medicinal space in quest of new drugs.},
  keywords = {chemical space,database,descriptors,Diversity,selection},
  file = {C:\Users\USEBPERP\Zotero\storage\S2YVKWUQ\Gorse - 2006 - Diversity in Medicinal Chemistry Space.pdf}
}

@article{gorseMolecularDiversityIts1999,
  title = {Molecular Diversity and Its Analysis},
  author = {Gorse, Dominique and Rees, Anthony and Kaczorek, Michel and Lahana, Roger},
  date = {1999-06-01},
  journaltitle = {Drug Discovery Today},
  volume = {4},
  number = {6},
  pages = {257--264},
  issn = {1359-6446},
  doi = {10.1016/S1359-6446(99)01334-3},
  url = {https://www.sciencedirect.com/science/article/pii/S1359644699013343},
  urldate = {2022-06-07},
  abstract = {We have recently developed a novel strategy for the rational design of compounds. This ‘in silico screening’ approach is based on the design and screening of virtual combinatorial libraries. Screening is performed using defined rules derived from a comprehensive description of active and inactive molecules in a relevant learning set. This strategy allows the development of potential ligands without the necessity of any knowledge of the 3D-structure of the target receptor. Key to the success of such methods is the quality of the information being processed, in particular, the diversity of the data in the context of the molecular population in the libraries concerned. Here, we review the problem of data diversity, its definition and its analysis using a new software tool, named Diverser.},
  langid = {english},
  keywords = {Molecular diversity,Rational drug design,Virtual combinatorial chemistry,Virtual screening},
  file = {C:\Users\USEBPERP\Zotero\storage\LQUTZSP8\S1359644699013343.html}
}

@online{goswamiUnsupervisedModelSelection2022,
  title = {Unsupervised {{Model Selection}} for {{Time-series Anomaly Detection}}},
  author = {Goswami, Mononito and Challu, Cristian and Callot, Laurent and Minorics, Lenon and Kan, Andrey},
  date = {2022-10-14},
  eprint = {2210.01078},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.01078},
  urldate = {2023-01-05},
  abstract = {Anomaly detection in time-series has a wide range of practical applications. While numerous anomaly detection methods have been proposed in the literature, a recent survey concluded that no single method is the most accurate across various datasets. To make matters worse, anomaly labels are scarce and rarely available in practice. The practical problem of selecting the most accurate model for a given dataset without labels has received little attention in the literature. This paper answers this question i.e. Given an unlabeled dataset and a set of candidate anomaly detectors, how can we select the most accurate model? To this end, we identify three classes of surrogate (unsupervised) metrics, namely, prediction error, model centrality, and performance on injected synthetic anomalies, and show that some metrics are highly correlated with standard supervised anomaly detection performance metrics such as the \$F\_1\$ score, but to varying degrees. We formulate metric combination with multiple imperfect surrogate metrics as a robust rank aggregation problem. We then provide theoretical justification behind the proposed approach. Large-scale experiments on multiple real-world datasets demonstrate that our proposed unsupervised approach is as effective as selecting the most accurate model based on partially labeled data.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\5XJZ4AW2\\Goswami et al. - 2022 - Unsupervised Model Selection for Time-series Anoma.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\R75T9SDJ\\Goswami et al. - 2022 - Unsupervised Model Selection for Time-series Anoma.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YQJPHL6E\\2210.html}
}

@unpublished{gottipatiLearningNavigateSynthetically2020,
  ids = {gottipati2020learninga,gottipatilearning},
  title = {Learning {{To Navigate The Synthetically Accessible Chemical Space Using Reinforcement Learning}}},
  author = {Gottipati, Sai Krishna and Sattarov, Boris and Niu, Sufeng and Pathak, Yashaswi and Wei, Haoran and Liu, Shengchao and Thomas, Karam M. J. and Blackburn, Simon and Coley, Connor W. and Tang, Jian and Chandar, Sarath and Bengio, Yoshua},
  date = {2020-04-26},
  eprint = {2004.12485},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2004.12485},
  urldate = {2020-04-29},
  abstract = {Over the last decade, there has been significant progress in the field of machine learning for de novo drug design, particularly in deep generative models. However, current generative approaches exhibit a significant challenge as they do not ensure that the proposed molecular structures can be feasibly synthesized nor do they provide the synthesis routes of the proposed small molecules, thereby seriously limiting their practical applicability. In this work, we propose a novel forward synthesis framework powered by reinforcement learning (RL) for de novo drug design, Policy Gradient for Forward Synthesis (PGFS), that addresses this challenge by embedding the concept of synthetic accessibility directly into the de novo drug design system. In this setup, the agent learns to navigate through the immense synthetically accessible chemical space by subjecting commercially available small molecule building blocks to valid chemical reactions at every time step of the iterative virtual multi-step synthesis process. The proposed environment for drug discovery provides a highly challenging test-bed for RL algorithms owing to the large state space and high-dimensional continuous action space with hierarchical actions. PGFS achieves state-of-the-art performance in generating structures with high QED and logP. Moreover, we validate PGFS in an in-silico proof-of-concept associated with three HIV targets, and the candidates generated with PGFS outperformed the existing benchmarks in optimizing the activity of the biological targets. Finally, we describe how the end-to-end training conceptualized in this study represents an important paradigm in radically expanding the synthesizable chemical space and automating the drug discovery process.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\IFXTMRSW\\Gottipati et al_2020_Learning To Navigate The Synthetically Accessible Chemical Space Using.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JDEYG5XC\\Gottipati et al. - Learning to Navigate The Synthetically Accessible .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JHUB6SZ3\\Gottipati et al_2020_Learning To Navigate The Synthetically Accessible Chemical Space Using.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VYQMABUP\\Gottipati et al. - 2020 - Learning To Navigate The Synthetically Accessible .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\CDVQL9QQ\\2004.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\IPL4S3E3\\2004.html}
}

@unpublished{goyalAccurateLargeMinibatch2017,
  title = {Accurate, {{Large Minibatch SGD}}: {{Training ImageNet}} in 1 {{Hour}}},
  shorttitle = {Accurate, {{Large Minibatch SGD}}},
  author = {Goyal, Priya and Dollár, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  date = {2017-06-08},
  eprint = {1706.02677},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.02677},
  urldate = {2019-08-26},
  abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textasciitilde 90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\EJ4JU3IC\\Goyal et al_2017_Accurate, Large Minibatch SGD.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BQIYFWZI\\1706.html}
}

@video{GradientDescentMomentum,
  entrysubtype = {video},
  title = {Gradient {{Descent With Momentum}} ({{C2W2L06}})},
  url = {https://www.youtube.com/watch?v=k8fTYJPd3_I},
  urldate = {2020-10-16},
  abstract = {Take the Deep Learning Specialization: http://bit.ly/2Tx5XGn Check out all our courses: https://www.deeplearning.ai Subscribe to The Batch, our weekly newsletter: https://www.deeplearning.ai/thebatch Follow us:  Twitter: https://twitter.com/deeplearningai\_ Facebook: https://www.facebook.com/deeplearningHQ/ Linkedin: https://www.linkedin.com/company/deep...}
}

@article{graffAcceleratingHighthroughputVirtual2021,
  title = {Accelerating High-Throughput Virtual Screening through Molecular Pool-Based Active Learning},
  author = {Graff, David E. and Shakhnovich, Eugene I. and Coley, Connor W.},
  date = {2021},
  journaltitle = {Chem. Sci.},
  volume = {12},
  number = {22},
  pages = {7866--7881},
  issn = {2041-6520, 2041-6539},
  doi = {10.1039/D0SC06805E},
  url = {http://xlink.rsc.org/?DOI=D0SC06805E},
  urldate = {2022-05-19},
  abstract = {Bayesian optimization can accelerate structure-based virtual screening campaigns by minimizing the total number of simulations performed while still identifying the vast majority of computational hits.           ,                             Structure-based virtual screening is an important tool in early stage drug discovery that scores the interactions between a target protein and candidate ligands. As virtual libraries continue to grow (in excess of 10               8               molecules), so too do the resources necessary to conduct exhaustive virtual screening campaigns on these libraries. However, Bayesian optimization techniques, previously employed in other scientific discovery problems, can aid in their exploration: a surrogate structure–property relationship model trained on the predicted affinities of a subset of the library can be applied to the remaining library members, allowing the least promising compounds to be excluded from evaluation. In this study, we explore the application of these techniques to computational docking datasets and assess the impact of surrogate model architecture, acquisition function, and acquisition batch size on optimization performance. We observe significant reductions in computational costs; for example, using a directed-message passing neural network we can identify 94.8\% or 89.3\% of the top-50\,000 ligands in a 100M member library after testing only 2.4\% of candidate ligands using an upper confidence bound or greedy acquisition strategy, respectively. Such model-guided searches mitigate the increasing computational costs of screening increasingly large virtual libraries and can accelerate high-throughput virtual screening campaigns with applications beyond docking.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\FWJ9XCL5\\Graff et al. - 2021 - Accelerating high-throughput virtual screening thr.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FYB9QD49\\Graff et al. - 2021 - Accelerating high-throughput virtual screening thr.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WPNRKVC5\\Graff et al. - 2021 - Accelerating high-throughput virtual screening thr.pdf}
}

@online{GraphconvolutionalNeuralNetwork,
  title = {A Graph-Convolutional Neural Network Model for the Prediction of Chemical Reactivity - {{Chemical Science}} ({{RSC Publishing}}) {{DOI}}:10.1039/{{C8SC04228D}}},
  url = {https://pubs.rsc.org/lv/content/articlehtml/2019/sc/c8sc04228d},
  urldate = {2021-04-08},
  file = {C:\Users\USEBPERP\Zotero\storage\JJWC8R4T\c8sc04228d.html}
}

@online{GraphVAEGenerationSmall,
  title = {{{GraphVAE}}: {{Towards Generation}} of {{Small Graphs Using Variational Autoencoders}} | {{SpringerLink}}},
  url = {https://link.springer.com/chapter/10.1007/978-3-030-01418-6_41},
  urldate = {2019-05-06},
  file = {C:\Users\USEBPERP\Zotero\storage\GEEUWZ2G\Simonovsky_Komodakis_2018_GraphVAE.pdf}
}

@unpublished{gravesGeneratingSequencesRecurrent2013,
  title = {Generating {{Sequences With Recurrent Neural Networks}}},
  author = {Graves, Alex},
  date = {2013-08-04},
  eprint = {1308.0850},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1308.0850},
  urldate = {2018-06-13},
  abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\8BQEXTSS\\Graves_2013_Generating Sequences With Recurrent Neural Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TY6JBSLC\\1308.html}
}

@article{greffLSTMSearchSpace2017,
  title = {{{LSTM}}: {{A Search Space Odyssey}}},
  shorttitle = {{{LSTM}}},
  author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutník, Jan and Steunebrink, Bas R. and Schmidhuber, Jürgen},
  date = {2017-10},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {28},
  number = {10},
  eprint = {1503.04069},
  eprinttype = {arxiv},
  pages = {2222--2232},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2016.2582924},
  url = {http://arxiv.org/abs/1503.04069},
  urldate = {2018-05-25},
  abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (\$\textbackslash approx 15\$ years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
  keywords = {68T10,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,H.5.5,I.2.6,I.2.7,I.5.1},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\SHQ2RPV7\\Greff et al_2017_LSTM.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\UUZUXZDD\\1503.html}
}

@unpublished{gregorDRAWRecurrentNeural2015,
  title = {{{DRAW}}: {{A Recurrent Neural Network For Image Generation}}},
  shorttitle = {{{DRAW}}},
  author = {Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
  date = {2015-02-16},
  eprint = {1502.04623},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1502.04623},
  urldate = {2018-10-10},
  abstract = {This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\K2TZSXNJ\\Gregor et al_2015_DRAW.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\W6GRGJVX\\1502.html}
}

@article{gresslingAutochemistryNewResearch2018,
  title = {Autochemistry: {{A New Research Paradigm Based}} on {{Artificial Intelligence}} and {{Big Data}}},
  shorttitle = {Autochemistry},
  author = {Gressling, Thorsten},
  date = {2018-10-29},
  doi = {10.26434/chemrxiv.7264103.v1},
  url = {https://chemrxiv.org/articles/Autochemistry_A_New_Research_Paradigm_Based_on_Artificial_Intelligence_and_Big_Data/7264103},
  urldate = {2019-10-14},
  abstract = {Artificial Intelligence technologies affect every domain and process in industry. Many solutions with different maturity levels have been created or are in development. With this paper we collect initiatives in the domain of chemical science and bring these resources together into a common process model. We define ten building blocks, analyse their role in the architecture and evaluate their impact to the current system. Finally we discuss the changes and the transition that occurs to the lab worker and the chemist.This paper introduces Autochemistry as a meme and for further development and discussion. We just can provide a first sketch to this exciting new area of scientific principles changing the anthropocentric fundament of chemistry research to a technocentric one.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\GQV7L4TM\\Gressling_2018_Autochemistry.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\IMUED2XS\\1.pdf}
}

@unpublished{griffithsConstrainedBayesianOptimization2017,
  title = {Constrained {{Bayesian Optimization}} for {{Automatic Chemical Design}}},
  author = {Griffiths, Ryan-Rhys and Hernández-Lobato, José Miguel},
  date = {2017-09-16},
  eprint = {1709.05501},
  eprinttype = {arxiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/1709.05501},
  urldate = {2019-05-06},
  abstract = {Automatic Chemical Design provides a framework for generating novel molecules with optimized molecular properties. The current model suffers from the pathology that it tends to produce invalid molecular structures. By reformulating the search procedure as a constrained Bayesian optimization problem, we showcase improvements in both the validity and quality of the generated molecules. We demonstrate that the model consistently produces novel molecules ranking above the 90th percentile of the distribution over training set scores across a range of objective functions. Importantly, our method suffers no degradation in the complexity or the diversity of the generated molecules.},
  keywords = {Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ECUNKVQK\\Griffiths_Hernández-Lobato_2017_Constrained Bayesian Optimization for Automatic Chemical Design.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\KJJ3KMGQ\\1709.html}
}

@unpublished{griffithsDatasetBiasNatural2021,
  title = {Dataset {{Bias}} in the {{Natural Sciences}}: {{A Case Study}} in {{Chemical Reaction Prediction}} and {{Synthesis Design}}},
  shorttitle = {Dataset {{Bias}} in the {{Natural Sciences}}},
  author = {Griffiths, Ryan-Rhys and Schwaller, Philippe and Lee, Alpha A.},
  date = {2021-05-06},
  eprint = {2105.02637},
  eprinttype = {arxiv},
  eprintclass = {physics},
  url = {http://arxiv.org/abs/2105.02637},
  urldate = {2021-05-11},
  abstract = {Datasets in the Natural Sciences are often curated with the goal of aiding scientific understanding and hence may not always be in a form that facilitates the application of machine learning. In this paper, we identify three trends within the fields of chemical reaction prediction and synthesis design that require a change in direction. First, the manner in which reaction datasets are split into reactants and reagents encourages testing models in an unrealistically generous manner. Second, we highlight the prevalence of mislabelled data, and suggest that the focus should be on outlier removal rather than data fitting only. Lastly, we discuss the problem of reagent prediction, in addition to reactant prediction, in order to solve the full synthesis design problem, highlighting the mismatch between what machine learning solves and what a lab chemist would need. Our critiques are also relevant to the burgeoning field of using machine learning to accelerate progress in experimental Natural Sciences, where datasets are often split in a biased way, are highly noisy, and contextual variables that are not evident from the data strongly influence the outcome of experiments.},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics}
}

@unpublished{guimaraesObjectiveReinforcedGenerativeAdversarial2017,
  title = {Objective-{{Reinforced Generative Adversarial Networks}} ({{ORGAN}}) for {{Sequence Generation Models}}},
  author = {Guimaraes, Gabriel Lima and Sanchez-Lengeling, Benjamin and Outeiral, Carlos and Farias, Pedro Luis Cunha and Aspuru-Guzik, Alán},
  date = {2017-05-30},
  eprint = {1705.10843},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1705.10843},
  urldate = {2018-09-17},
  abstract = {In unsupervised data generation tasks, besides the generation of a sample based on previous observations, one would often like to give hints to the model in order to bias the generation towards desirable metrics. We propose a method that combines Generative Adversarial Networks (GANs) and reinforcement learning (RL) in order to accomplish exactly that. While RL biases the data generation process towards arbitrary metrics, the GAN component of the reward function ensures that the model still remembers information learned from data. We build upon previous results that incorporated GANs and RL in order to generate sequence data and test this model in several settings for the generation of molecules encoded as text sequences (SMILES) and in the context of music generation, showing for each case that we can effectively bias the generation process towards desired metrics.},
  keywords = {Computer Science - Learning,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JFL2JJG2\\Guimaraes et al_2017_Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QRJN89DG\\Guimaraes et al_2017_Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\3QS37NCX\\1705.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\SJE85HUS\\1705.html}
}

@incollection{gunasekarImplicitRegularizationMatrix2017,
  title = {Implicit {{Regularization}} in {{Matrix Factorization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  pages = {6151--6159},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/7195-implicit-regularization-in-matrix-factorization.pdf},
  urldate = {2019-05-02},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JWKBZE2J\\Gunasekar et al_2017_Implicit Regularization in Matrix Factorization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\R83WSWDS\\7195-implicit-regularization-in-matrix-factorization.html}
}

@online{guoAugmentedMemoryCapitalizing2023,
  title = {Augmented {{Memory}}: {{Capitalizing}} on {{Experience Replay}} to {{Accelerate De Novo Molecular Design}}},
  shorttitle = {Augmented {{Memory}}},
  author = {Guo, Jeff and Schwaller, Philippe},
  date = {2023-05-10},
  eprint = {2305.16160},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2305.16160},
  urldate = {2023-09-11},
  abstract = {Sample efficiency is a fundamental challenge in de novo molecular design. Ideally, molecular generative models should learn to satisfy a desired objective under minimal oracle evaluations (computational prediction or wet-lab experiment). This problem becomes more apparent when using oracles that can provide increased predictive accuracy but impose a significant cost. Consequently, these oracles cannot be directly optimized under a practical budget. Molecular generative models have shown remarkable sample efficiency when coupled with reinforcement learning, as demonstrated in the Practical Molecular Optimization (PMO) benchmark. Here, we propose a novel algorithm called Augmented Memory that combines data augmentation with experience replay. We show that scores obtained from oracle calls can be reused to update the model multiple times. We compare Augmented Memory to previously proposed algorithms and show significantly enhanced sample efficiency in an exploitation task and a drug discovery case study requiring both exploration and exploitation. Our method achieves a new state-of-the-art in the PMO benchmark which enforces a computational budget, outperforming the previous best performing method on 19/23 tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Quantitative Biology - Quantitative Methods},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\VMAW3ZPA\\Guo and Schwaller - 2023 - Augmented Memory Capitalizing on Experience Repla.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\476ML8KJ\\2305.html}
}

@article{guoBayesianAlgorithmRetrosynthesis2020,
  ids = {guo2020bayesiana},
  title = {A {{Bayesian}} Algorithm for Retrosynthesis},
  author = {Guo, Zhongliang and Wu, Stephen and Ohno, Mitsuru and Yoshida, Ryo},
  date = {2020-10-26},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {60},
  number = {10},
  eprint = {2003.03190},
  eprinttype = {arxiv},
  pages = {4474--4486},
  issn = {1549-9596, 1549-960X},
  doi = {10.1021/acs.jcim.0c00320},
  url = {http://arxiv.org/abs/2003.03190},
  urldate = {2021-04-30},
  abstract = {The identification of synthetic routes that end with a desired product has been an inherently time-consuming process that is largely dependent on expert knowledge regarding a limited fraction of the entire reaction space. At present, emerging machine-learning technologies are overturning the process of retrosynthetic planning. The objective of this study is to discover synthetic routes backwardly from a given desired molecule to commercially available compounds. The problem is reduced to a combinatorial optimization task with the solution space subject to the combinatorial complexity of all possible pairs of purchasable reactants. We address this issue within the framework of Bayesian inference and computation. The workflow consists of two steps: a deep neural network is trained that forwardly predicts a product of the given reactants with a high level of accuracy, following which this forward model is inverted into the backward one via Bayes' law of conditional probability. Using the backward model, a diverse set of highly probable reaction sequences ending with a given synthetic target is exhaustively explored using a Monte Carlo search algorithm. The Bayesian retrosynthesis algorithm could successfully rediscover 80.3\% and 50.0\% of known synthetic routes of single-step and two-step reactions within top-10 accuracy, respectively, thereby outperforming state-of-the-art algorithms in terms of the overall accuracy. Remarkably, the Monte Carlo method, which was specifically designed for the presence of diverse multiple routes, often revealed a ranked list of hundreds of reaction routes to the same synthetic target. We investigated the potential applicability of such diverse candidates based on expert knowledge from synthetic organic chemistry.},
  issue = {10},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Statistics - Machine Learning}
}

@unpublished{guoCalibrationModernNeural2017,
  title = {On {{Calibration}} of {{Modern Neural Networks}}},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  date = {2017-06-14},
  eprint = {1706.04599},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.04599},
  urldate = {2019-04-16},
  abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\AUU9ZH79\\Guo et al_2017_On Calibration of Modern Neural Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\F2EHYLMD\\1706.html}
}

@article{guptaGenerativeRecurrentNetworks2018,
  title = {Generative {{Recurrent Networks}} for {{De Novo Drug Design}}},
  author = {Gupta, Anvita and Müller, Alex T. and Huisman, Berend J. H. and Fuchs, Jens A. and Schneider, Petra and Schneider, Gisbert},
  date = {2018-01},
  journaltitle = {Mol Inform},
  volume = {37},
  number = {1-2},
  eprint = {29095571},
  eprinttype = {pmid},
  issn = {1868-1751},
  doi = {10.1002/minf.201700111},
  abstract = {Generative artificial intelligence models present a fresh approach to chemogenomics and de novo drug design, as they provide researchers with the ability to narrow down their search of the chemical space and focus on regions of interest. We present a method for molecular de novo design that utilizes generative recurrent neural networks (RNN) containing long short-term memory (LSTM) cells. This computational model captured the syntax of molecular representation in terms of SMILES strings with close to perfect accuracy. The learned pattern probabilities can be used for de novo SMILES generation. This molecular design concept eliminates the need for virtual compound library enumeration. By employing transfer learning, we fine-tuned the RNN's predictions for specific molecular targets. This approach enables virtual compound design without requiring secondary or external activity prediction, which could introduce error or unwanted bias. The results obtained advocate this generative RNN-LSTM system for high-impact use cases, such as low-data drug discovery, fragment based molecular design, and hit-to-lead optimization for diverse drug targets.},
  langid = {english},
  pmcid = {PMC5836943},
  keywords = {Chemogenomics,deep learning,Drug Design,drug discovery,Drug Discovery,machine learning,medicinal chemistry,Models Chemical,Neural Networks (Computer),Quantitative Structure-Activity Relationship},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\E645Z9W9\\Gupta et al_2018_Generative Recurrent Networks for De Novo Drug Design2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LLZ6A972\\Gupta et al_2018_Generative Recurrent Networks for De Novo Drug Design.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\825E63LC\\minf.html}
}

@article{guptaOutlierDetectionTemporal2014,
  title = {Outlier {{Detection}} for {{Temporal Data}}: {{A Survey}}},
  shorttitle = {Outlier {{Detection}} for {{Temporal Data}}},
  author = {Gupta, Manish and Gao, Jing and Aggarwal, Charu C. and Han, Jiawei},
  date = {2014-09},
  journaltitle = {IEEE Trans. Knowl. Data Eng.},
  volume = {26},
  number = {9},
  pages = {2250--2267},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2013.184},
  url = {http://ieeexplore.ieee.org/document/6684530/},
  urldate = {2023-01-06},
  abstract = {In the statistics community, outlier detection for time series data has been studied for decades. Recently, with advances in hardware and software technology, there has been a large body of work on temporal outlier detection from a computational perspective within the computer science community. In particular, advances in hardware technology have enabled the availability of various forms of temporal data collection mechanisms, and advances in software technology have enabled a variety of data management mechanisms. This has fueled the growth of different kinds of data sets such as data streams, spatiotemporal data, distributed streams, temporal networks, and time series data, generated by a multitude of applications. There arises a need for an organized and detailed study of the work done in the area of outlier detection with respect to such temporal datasets. In this survey, we provide a comprehensive and structured overview of a large set of interesting outlier definitions for various forms of temporal data, novel techniques, and application scenarios in which specific definitions and techniques have been widely used.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\3XDFGESI\Gupta et al. - 2014 - Outlier Detection for Temporal Data A Survey.pdf}
}

@incollection{gurbuzbalabanWhenCyclicCoordinate2017,
  title = {When {{Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Gurbuzbalaban, Mert and Ozdaglar, Asuman and Parrilo, Pablo A and Vanli, Nuri},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  pages = {6999--7007},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/7275-when-cyclic-coordinate-descent-outperforms-randomized-coordinate-descent.pdf},
  urldate = {2019-08-26},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\9FPA4TKI\\Gurbuzbalaban et al_2017_When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZCVHY7PN\\7275-when-cyclic-coordinate-descent-outperforms-randomized-coordinate-descent.html}
}

@unpublished{gurbuzbalabanWhyRandomReshuffling2015,
  ids = {gurbuzbalabanWhyRandomReshuffling2015a},
  title = {Why {{Random Reshuffling Beats Stochastic Gradient Descent}}},
  author = {Gürbüzbalaban, Mert and Ozdaglar, Asu and Parrilo, Pablo},
  date = {2015-10-29},
  eprint = {1510.08560},
  eprinttype = {arxiv},
  eprintclass = {math},
  url = {http://arxiv.org/abs/1510.08560},
  urldate = {2019-08-26},
  abstract = {We analyze the convergence rate of the random reshuffling (RR) method, which is a randomized first-order incremental algorithm for minimizing a finite sum of convex component functions. RR proceeds in cycles, picking a uniformly random order (permutation) and processing the component functions one at a time according to this order, i.e., at each cycle, each component function is sampled without replacement from the collection. Though RR has been numerically observed to outperform its with-replacement counterpart stochastic gradient descent (SGD), characterization of its convergence rate has been a long standing open question. In this paper, we answer this question by showing that when the component functions are quadratics or smooth and the sum function is strongly convex, RR with iterate averaging and a diminishing stepsize \$\textbackslash alpha\_k=\textbackslash Theta(1/k\textasciicircum s)\$ for \$s\textbackslash in (1/2,1)\$ converges at rate \$\textbackslash Theta(1/k\textasciicircum\{2s\})\$ with probability one in the suboptimality of the objective value, thus improving upon the \$\textbackslash Omega(1/k)\$ rate of SGD. Our analysis draws on the theory of Polyak-Ruppert averaging and relies on decoupling the dependent cycle gradient error into an independent term over cycles and another term dominated by \$\textbackslash alpha\_k\textasciicircum 2\$. This allows us to apply law of large numbers to an appropriately weighted version of the cycle gradient errors, where the weights depend on the stepsize. We also provide high probability convergence rate estimates that shows decay rate of different terms and allows us to propose a modification of RR with convergence rate \$\{\textbackslash cal O\}(\textbackslash frac\{1\}\{k\textasciicircum 2\})\$.},
  keywords = {Mathematics - Optimization and Control},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BIDDUXT3\\Gürbüzbalaban et al_2015_Why Random Reshuffling Beats Stochastic Gradient Descent.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\USJ3QLQC\\Gürbüzbalaban et al_2015_Why Random Reshuffling Beats Stochastic Gradient Descent2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GMNLTZ95\\1510.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\PBV6CSDV\\1510.html}
}

@inproceedings{hadsellDimensionalityReductionLearning2006,
  title = {Dimensionality {{Reduction}} by {{Learning}} an {{Invariant Mapping}}},
  booktitle = {2006 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'06)},
  author = {Hadsell, R. and Chopra, S. and LeCun, Y.},
  date = {2006-06},
  volume = {2},
  pages = {1735--1742},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2006.100},
  abstract = {Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that 'similar" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distancemeasure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.},
  eventtitle = {2006 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'06)},
  keywords = {Astronomy,Biology,Data visualization,Extraterrestrial measurements,Feature extraction,Geoscience,Image analysis,Image generation,Manufacturing industries,Service robots},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\VNFQ46AZ\\Hadsell et al. - 2006 - Dimensionality Reduction by Learning an Invariant .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8G9BTNPB\\1640964.html}
}

@article{haidtCoddlingAmericanMind,
  entrysubtype = {magazine},
  title = {The {{Coddling}} of the {{American Mind}}},
  author = {Haidt, Story by Greg Lukianoff {and} Jonathan},
  journaltitle = {The Atlantic},
  issn = {1072-7825},
  url = {https://www.theatlantic.com/magazine/archive/2015/09/the-coddling-of-the-american-mind/399356/},
  urldate = {2020-11-04},
  abstract = {In the name of emotional well-being, college students are increasingly demanding protection from words and ideas they don’t like. Here’s why that’s disastrous for education—and mental health.},
  file = {C:\Users\USEBPERP\Zotero\storage\JRJKGIJT\399356.html}
}

@inproceedings{hamidiGenderRecognitionGender2018,
  title = {Gender {{Recognition}} or {{Gender Reductionism}}?: {{The Social Implications}} of {{Embedded Gender Recognition Systems}}},
  shorttitle = {Gender {{Recognition}} or {{Gender Reductionism}}?},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}  - {{CHI}} '18},
  author = {Hamidi, Foad and Scheuerman, Morgan Klaus and Branham, Stacy M.},
  date = {2018},
  pages = {1--13},
  publisher = {ACM Press},
  location = {Montreal QC, Canada},
  doi = {10.1145/3173574.3173582},
  url = {http://dl.acm.org/citation.cfm?doid=3173574.3173582},
  urldate = {2020-06-22},
  abstract = {Automatic Gender Recognition (AGR) refers to various computational methods that aim to identify an individual’s gender by extracting and analyzing features from images, video, and/or audio. Applications of AGR are increasingly being explored in domains such as security, marketing, and social robotics. However, little is known about stakeholders’ perceptions and attitudes towards AGR and how this technology might disproportionately affect vulnerable communities. To begin to address these gaps, we interviewed 13 transgender individuals, including three transgender technology designers, about their perceptions and attitudes towards AGR. We found that transgender individuals have overwhelmingly negative attitudes towards AGR and fundamentally question whether it can accurately recognize such a subjective aspect of their identity. They raised concerns about privacy and potential harms that can result from being incorrectly gendered, or misgendered, by technology. We present a series of recommendations on how to accommodate gender diversity when designing new digital systems.},
  eventtitle = {The 2018 {{CHI Conference}}},
  isbn = {978-1-4503-5620-6},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\N589LA6S\Hamidi et al. - 2018 - Gender Recognition or Gender Reductionism The So.pdf}
}

@incollection{hamiltonEmbeddingLogicalQueries2018,
  title = {Embedding {{Logical Queries}} on {{Knowledge Graphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Hamilton, Will and Bajaj, Payal and Zitnik, Marinka and Jurafsky, Dan and Leskovec, Jure},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {2026--2037},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/7473-embedding-logical-queries-on-knowledge-graphs.pdf},
  urldate = {2019-05-20},
  file = {C:\Users\USEBPERP\Zotero\storage\ZQ7K9M5F\Hamilton et al_2018_Embedding Logical Queries on Knowledge Graphs.pdf}
}

@online{handaDifficultyValidatingMolecular2023,
  title = {On {{The Difficulty}} of {{Validating Molecular Generative Models Realistically}}: {{A Case Study}} on {{Public}} and {{Proprietary Data}}},
  shorttitle = {On {{The Difficulty}} of {{Validating Molecular Generative Models Realistically}}},
  author = {Handa, Koichi and Thomas, Morgan and Kageyama, Michiharu and Iijima, Takeshi and Bender, Andreas},
  date = {2023-06-16},
  eprinttype = {ChemRxiv},
  doi = {10.26434/chemrxiv-2023-lbvgn},
  url = {https://chemrxiv.org/engage/chemrxiv/article-details/648bed3ae64f843f41dba601},
  urldate = {2023-07-20},
  abstract = {While a multitude of deep generative models have recently emerged there exists no best practice for their practically relevant validation. On the one hand, novel de novo-generated molecules cannot be refuted by retrospective validation (so that this type of validation is biased); but on the other hand prospective validation is expensive and then often biased by the human selection process. In this case study, we frame retrospective validation as the ability to mimic human drug design, by answering the following question: Can a generative model trained on early-stage project compounds generate middle/late-stage compounds de novo? To this end, we used experimental data that contains the elapsed time of a synthetic expansion following hit identification from five public (where the time series was pre-processed to better reflect realistic synthetic expansions) and six in-house project datasets, and used REINVENT as a widely adopted RNN-based generative model. After splitting the dataset and training REINVENT on early-stage compounds, we found that rediscovery of middle/late-stage compounds was much higher in public projects (at 1.60\%, 0.64\%, and 0.21\% of the top 100, 500, and 5,000 scored generated compounds) than in in-house projects (where the values were 0.00\%, 0.03\%, and 0.04\%, respectively). Similarly, average single nearest neighbour similarity between early- and middle/late-stage compounds in public projects was higher between active compounds than inactive compounds; however, for in-house projects the converse was true, which makes rediscovery (if so desired) more difficult. We hence show that the generative model recovers very few middle/late-stage compounds from real-world drug discovery projects, highlighting the fundamental difference between purely algorithmic design and drug discovery as a real-world process. Evaluating de novo compound design approaches appears, based on the current study, difficult or even impossible to do retrospectively.},
  langid = {english},
  pubstate = {preprint},
  keywords = {AI,Artificial Intelligence,De novo molecular generation,Generative models,LBDD,Ligand-based drug design,Recurrent neural network,Reinforcement learning,REINVENT,Validation},
  file = {C:\Users\USEBPERP\Zotero\storage\7NS2LJS6\Handa et al. - 2023 - On The Difficulty of Validating Molecular Generati.pdf}
}

@article{hardtTrainFasterGeneralize,
  title = {Train Faster, Generalize Better: {{Stability}} of Stochastic Gradient Descent},
  author = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  pages = {10},
  abstract = {We show that parametric models trained by a stochastic gradient method (SGM) with few iterations have vanishing generalization error. We prove our results by arguing that SGM is algorithmically stable in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex and continuous optimization. We derive stability bounds for both convex and non-convex optimization under standard Lipschitz and smoothness assumptions.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\4EXH9HTY\Hardt et al_Train faster, generalize better.pdf}
}

@article{harelAcceleratingPrototypeBasedDrug2018,
  title = {Accelerating {{Prototype-Based Drug Discovery}} Using {{Conditional Diversity Networks}}},
  author = {Harel, Shahar and Radinsky, Kira},
  date = {2018},
  journaltitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining  - KDD '18},
  eprint = {1804.02668},
  eprinttype = {arxiv},
  pages = {331--339},
  doi = {10.1145/3219819.3219882},
  url = {http://arxiv.org/abs/1804.02668},
  urldate = {2018-09-17},
  abstract = {Designing a new drug is a lengthy and expensive process. As the space of potential molecules is very large (10\textasciicircum 23-10\textasciicircum 60), a common technique during drug discovery is to start from a molecule which already has some of the desired properties. An interdisciplinary team of scientists generates hypothesis about the required changes to the prototype. In this work, we develop an algorithmic unsupervised-approach that automatically generates potential drug molecules given a prototype drug. We show that the molecules generated by the system are valid molecules and significantly different from the prototype drug. Out of the compounds generated by the system, we identified 35 FDA-approved drugs. As an example, our system generated Isoniazid - one of the main drugs for Tuberculosis. The system is currently being deployed for use in collaboration with pharmaceutical companies to further analyze the additional generated molecules.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6XNQK3FC\\Harel_Radinsky_2018_Accelerating Prototype-Based Drug Discovery using Conditional Diversity Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SKGQ3MWZ\\1804.html}
}

@article{harelPrototypeBasedCompoundDiscovery2018,
  title = {Prototype-{{Based Compound Discovery Using Deep Generative Models}}},
  author = {Harel, Shahar and Radinsky, Kira},
  date = {2018-10},
  journaltitle = {Mol. Pharmaceutics},
  volume = {15},
  number = {10},
  pages = {4406--4416},
  issn = {1543-8384, 1543-8392},
  doi = {10.1021/acs.molpharmaceut.8b00474},
  url = {http://pubs.acs.org/doi/10.1021/acs.molpharmaceut.8b00474},
  urldate = {2019-05-06},
  abstract = {Designing a new drug is a lengthy and expensive process. As the space of potential molecules is very large (Polishchuk, P. G.; Madzhidov, T. I.; Varnek, A. Estimation of the size of drug-like chemical space based on GDB-17 data. J. Comput.-Aided Mol. Des. 2013, 27, 675−679 10.1007/s10822-013-9672-4), a common technique during drug discovery is to start from a molecule which already has some of the desired properties. An interdisciplinary team of scientists generates hypothesis about the required changes to the prototype. In this work, we develop a deep-learning unsupervised-approach that automatically generates potential drug molecules given a prototype drug. We show that the molecules generated by the system are valid molecules and significantly different from the prototype drug. Out of the compounds generated by the system, we identified 35 known FDA-approved drugs. As an example, our system generated isoniazid, one of the main drugs for tuberculosis. We suggest several ranking functions for the generated molecules and present results that the top ten generated molecules per prototype drug contained in our retrospective experiments 23 known FDA-approved drugs.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\BFU2IQDY\Harel_Radinsky_2018_Prototype-Based Compound Discovery Using Deep Generative Models.pdf}
}

@article{harperDesignCompoundScreening2004,
  title = {Design of a Compound Screening Collection for Use in High Throughput Screening},
  author = {Harper, G. and Pickett, S. D. and Green, D. V. S.},
  date = {2004-02},
  journaltitle = {Comb Chem High Throughput Screen},
  volume = {7},
  number = {1},
  eprint = {14965262},
  eprinttype = {pmid},
  pages = {63--70},
  issn = {1386-2073},
  doi = {10.2174/138620704772884832},
  abstract = {In this paper we introduce a quantitative model that relates chemical structural similarity to biological activity, and in particular to the activity of lead series of compounds in high-throughput assays. From this model we derive the optimal screening collection make up for a given fixed size of screening collection, and identify the conditions under which a diverse collection of compounds or a collection focusing on particular regions of chemical space are appropriate strategies. We derive from the model a diversity function that may be used to assess compounds for acquisition or libraries for combinatorial synthesis by their ability to complement an existing screening collection. The diversity function is linked directly through the model to the goal of more frequent discovery of lead series from high-throughput screening. We show how the model may also be used to derive relationships between collection size and probabilities of lead discovery in high-throughput screening, and to guide the judicious application of structural filters.},
  langid = {english},
  keywords = {Chemistry Pharmaceutical,Combinatorial Chemistry Techniques,Drug Design,Models Chemical,Structure-Activity Relationship}
}

@article{hartenfellerCollectionRobustOrganic2011,
  title = {A Collection of Robust Organic Synthesis Reactions for in Silico Molecule Design},
  author = {Hartenfeller, Markus and Eberle, Martin and Meier, Peter and Nieto-Oberhuber, Cristina and Altmann, Karl-Heinz and Schneider, Gisbert and Jacoby, Edgar and Renner, Steffen},
  date = {2011-12-27},
  journaltitle = {J Chem Inf Model},
  volume = {51},
  number = {12},
  eprint = {22077721},
  eprinttype = {pmid},
  pages = {3093--3098},
  issn = {1549-960X},
  doi = {10.1021/ci200379p},
  abstract = {A focused collection of organic synthesis reactions for computer-based molecule construction is presented. It is inspired by real-world chemistry and has been compiled in close collaboration with medicinal chemists to achieve high practical relevance. Virtual molecules assembled from existing starting material connected by these reactions are supposed to have an enhanced chance to be amenable to real chemical synthesis. About 50\% of the reactions in the dataset are ring-forming reactions, which fosters the assembly of novel ring systems and innovative chemotypes. A comparison with a recent survey of the reactions used in early drug discovery revealed considerable overlaps with the collection presented here. The dataset is available encoded as computer-readable Reaction SMARTS expressions from the Supporting Information presented for this paper.},
  langid = {english},
  keywords = {Chemistry Techniques Synthetic,Computer-Aided Design,Databases Factual,Drug Design}
}

@article{hartenfellerDOGSReactionDrivenNovo2012,
  title = {{{DOGS}}: {{Reaction-Driven}} de Novo {{Design}} of {{Bioactive Compounds}}},
  shorttitle = {{{DOGS}}},
  author = {Hartenfeller, Markus and Zettl, Heiko and Walter, Miriam and Rupp, Matthias and Reisen, Felix and Proschak, Ewgenij and Weggen, Sascha and Stark, Holger and Schneider, Gisbert},
  date = {2012-02-16},
  journaltitle = {PLOS Computational Biology},
  volume = {8},
  number = {2},
  pages = {e1002380},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002380},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002380},
  urldate = {2020-05-29},
  abstract = {We present a computational method for the reaction-based de novo design of drug-like molecules. The software DOGS (Design of Genuine Structures) features a ligand-based strategy for automated ‘in silico’ assembly of potentially novel bioactive compounds. The quality of the designed compounds is assessed by a graph kernel method measuring their similarity to known bioactive reference ligands in terms of structural and pharmacophoric features. We implemented a deterministic compound construction procedure that explicitly considers compound synthesizability, based on a compilation of 25'144 readily available synthetic building blocks and 58 established reaction principles. This enables the software to suggest a synthesis route for each designed compound. Two prospective case studies are presented together with details on the algorithm and its implementation. De novo designed ligand candidates for the human histamine H4 receptor and γ-secretase were synthesized as suggested by the software. The computational approach proved to be suitable for scaffold-hopping from known ligands to novel chemotypes, and for generating bioactive molecules with drug-like properties.},
  langid = {english},
  keywords = {Dogs,Functional groups,Histamine,Reactants,Serine proteases,Software design,Software tools,Trypsin inhibitors},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6EIIKKBH\\Hartenfeller et al_2012_DOGS.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\IS4WT8LJ\\article.html}
}

@article{hartenfellerEnablingFutureDrug2011,
  title = {Enabling Future Drug Discovery by de Novo Design},
  author = {Hartenfeller, Markus and Schneider, Gisbert},
  date = {2011},
  journaltitle = {Wiley Interdisciplinary Reviews: Computational Molecular Science},
  volume = {1},
  number = {5},
  pages = {742--759}
}

@book{hartiganClusteringAlgorithms1975,
  title = {Clustering Algorithms},
  author = {Hartigan, John A.},
  date = {1975},
  series = {Wiley Series in Probability and Mathematical Statistics},
  publisher = {Wiley},
  location = {New York},
  isbn = {978-0-471-35645-5},
  pagetotal = {351},
  keywords = {Cluster analysis,Data processing},
  file = {C:\Users\USEBPERP\Zotero\storage\4L76IGTH\John A. Hartigan-Clustering Algorithms-John Wiley & Sons (1975).pdf}
}

@article{hasicSingleStepRetrosynthesisPrediction2021,
  title = {Single-{{Step Retrosynthesis Prediction Based}} on the {{Identification}} of {{Potential Disconnection Sites Using Molecular Substructure Fingerprints}}},
  author = {Hasic, Haris and Ishida, Takashi},
  date = {2021-02-22},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {61},
  number = {2},
  pages = {641--652},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.0c01100},
  url = {https://doi.org/10.1021/acs.jcim.0c01100},
  urldate = {2021-05-10},
  abstract = {The proper application of retrosynthesis to identify possible transformations for a given target compound requires a lot of chemistry knowledge and experience. However, because the complexity of this technique scales together with the complexity of the target, efficient application on compounds with intricate molecular structures becomes almost impossible for human chemists. The idea of using computers in such situations has existed for a long time, but the accuracy was not sufficient for practical applications. Nevertheless, with the steady improvement of machine learning and artificial intelligence in recent years, computer-assisted retrosynthesis has been gaining research attention again. Because of the overall lack of chemical reaction data, the main challenge for the recent retrosynthesis methods is low exploration ability during the analysis of target and intermediate compounds. The main goal of this research is to develop a novel, template-free approach to address this issue. Only individual molecular substructures of the target are used to determine potential disconnection sites, without relying on additional information such as chemical reaction class. The model for the identification of potential disconnection sites is trained on novel molecular substructure fingerprint representations. For each of the disconnections suggested using the model, a simple structural similarity-based reactant retrieval and scoring method is applied, and the suggestions are completed. This method achieves 47.2\% top-1 accuracy for the single-step retrosynthesis task on the processed United States Patent Office dataset. Furthermore, if the predicted reaction class is used to narrow down the reactant candidate search space, the performance is improved to 61.4\% top-1 accuracy.},
  issue = {2}
}

@article{hassanOptimizationVisualizationMolecular1996,
  title = {Optimization and Visualization of Molecular Diversity of Combinatorial Libraries},
  author = {Hassan, Moises and Bielawski, Jan P. and Hempel, Judith C. and Waldman, Marvin},
  date = {1996-10-01},
  journaltitle = {Mol Divers},
  volume = {2},
  number = {1},
  pages = {64--74},
  issn = {1573-501X},
  doi = {10.1007/BF01718702},
  url = {https://doi.org/10.1007/BF01718702},
  urldate = {2022-06-09},
  abstract = {One of the major goals of rational design of combinatorial libraries is to design libraries with maximum diversity to enhance the potential of finding active compounds in the initial rounds of high-throughput screening programs. We present strategies to visualize and optimize the structural diversity of sets of molecules, which can be either potential substituents to be attached at specific positions of the library scaffold, or entire molecules corresponding to enumerated libraries. The selection of highly diverse subsets of molecules from the library is based on the stochastic optimization of ‘Diversity’ functions using a single-point-mutation Monte Carlo technique. The Diversity functions are defined in terms of the distances among molecules in multidimensional property space resulting from the calculation of 2D and 3D molecular descriptors. Several Diversity functions, including an implementation of D-Optimal design, are applied to select diverse subsets and the results are compared. The diversity of the selected subsets of molecules is visualized by embedding the intermolecular distances, defined by the molecules in multidimensional property space, into a three-dimensional space.},
  langid = {english},
  keywords = {Combinatorial libraries,Experimental design,Molecular descriptors},
  file = {C:\Users\USEBPERP\Zotero\storage\BCQR2B9E\Hassan et al. - 1996 - Optimization and visualization of molecular divers.pdf}
}

@unpublished{heDataAugmentationRevisited2019,
  title = {Data {{Augmentation Revisited}}: {{Rethinking}} the {{Distribution Gap}} between {{Clean}} and {{Augmented Data}}},
  shorttitle = {Data {{Augmentation Revisited}}},
  author = {He, Zhuoxun and Xie, Lingxi and Chen, Xin and Zhang, Ya and Wang, Yanfeng and Tian, Qi},
  date = {2019-09-19},
  eprint = {1909.09148},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1909.09148},
  urldate = {2019-09-27},
  abstract = {Data augmentation has been widely applied as an effective methodology to prevent over-fitting in particular when training very deep neural networks. The essential benefit comes from introducing additional priors in visual invariance, and thus generate images in different appearances but containing the same semantics. Recently, researchers proposed a few powerful data augmentation techniques which indeed improved accuracy, yet we notice that these methods have also caused a considerable gap between clean and augmented data. This paper revisits this problem from an analytical perspective, for which we estimate the upper-bound of testing loss using two terms, named empirical risk and generalization error, respectively. Data augmentation significantly reduces the generalization error, but meanwhile leads to a larger empirical risk, which can be alleviated by a simple algorithm, i.e. using less-augmented data to refine the model trained on fully-augmented data. We validate our approach on a few popular image classification datasets including CIFAR and ImageNet, and demonstrate consistent accuracy gain. We also conjecture that this simple strategy implies a generalized approach to circumvent local minima, which is of value to future research on model optimization.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\N8EISYCP\\He et al_2019_Data Augmentation Revisited.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QGKB9SP5\\1909.html}
}

@unpublished{heekBayesianInferenceLarge2019,
  title = {Bayesian {{Inference}} for {{Large Scale Image Classification}}},
  author = {Heek, Jonathan and Kalchbrenner, Nal},
  date = {2019-08-09},
  eprint = {1908.03491},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1908.03491},
  urldate = {2019-10-21},
  abstract = {Bayesian inference promises to ground and improve the performance of deep neural networks. It promises to be robust to overfitting, to simplify the training procedure and the space of hyperparameters, and to provide a calibrated measure of uncertainty that can enhance decision making, agent exploration and prediction fairness. Markov Chain Monte Carlo (MCMC) methods enable Bayesian inference by generating samples from the posterior distribution over model parameters. Despite the theoretical advantages of Bayesian inference and the similarity between MCMC and optimization methods, the performance of sampling methods has so far lagged behind optimization methods for large scale deep learning tasks. We aim to fill this gap and introduce ATMC, an adaptive noise MCMC algorithm that estimates and is able to sample from the posterior of a neural network. ATMC dynamically adjusts the amount of momentum and noise applied to each parameter update in order to compensate for the use of stochastic gradients. We use a ResNet architecture without batch normalization to test ATMC on the Cifar10 benchmark and the large scale ImageNet benchmark and show that, despite the absence of batch normalization, ATMC outperforms a strong optimization baseline in terms of both classification accuracy and test log-likelihood. We show that ATMC is intrinsically robust to overfitting on the training data and that ATMC provides a better calibrated measure of uncertainty compared to the optimization baseline.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\IKTYKHSI\\Heek_Kalchbrenner_2019_Bayesian Inference for Large Scale Image Classification.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SF2KLIUT\\1908.html}
}

@online{heimAdaptiveAnomalyDetection2019,
  title = {Adaptive {{Anomaly Detection}} in {{Chaotic Time Series}} with a {{Spatially Aware Echo State Network}}},
  author = {Heim, Niklas and Avery, James E.},
  date = {2019-09-02},
  eprint = {1909.01709},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1909.01709},
  url = {http://arxiv.org/abs/1909.01709},
  urldate = {2023-01-06},
  abstract = {This work builds an automated anomaly detection method for chaotic time series, and more concretely for turbulent, high-dimensional, ocean simulations. We solve this task by extending the Echo State Network by spatially aware input maps, such as convolutions, gradients, cosine transforms, et cetera, as well as a spatially aware loss function. The spatial ESN is used to create predictions which reduce the detection problem to thresholding of the prediction error. We benchmark our detection framework on different tasks of increasing difficulty to show the generality of the framework before applying it to raw climate model output in the region of the Japanese ocean current Kuroshio, which exhibits a bimodality that is not easily detected by the naked eye. The code is available as an open source Python package, Torsk, available at https://github.com/nmheim/torsk, where we also provide supplementary material and programs that reproduce the results shown in this paper.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\K3P4DQ2T\\Heim and Avery - 2019 - Adaptive Anomaly Detection in Chaotic Time Series .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZU7DWACG\\1909.html}
}

@inproceedings{heMomentumContrastUnsupervised2020,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  date = {2020},
  pages = {9729--9738},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html},
  urldate = {2021-04-08},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2UEW9D7R\\He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\NMW995YT\\He_Momentum_Contrast_for_Unsupervised_Visual_Representation_Learning_CVPR_2020_paper.html}
}

@unpublished{henaffDataEfficientImageRecognition2019,
  title = {Data-{{Efficient Image Recognition}} with {{Contrastive Predictive Coding}}},
  author = {Hénaff, Olivier J. and Srinivas, Aravind and De Fauw, Jeffrey and Razavi, Ali and Doersch, Carl and Eslami, S. M. Ali and family=Oord, given=Aaron, prefix=van den, useprefix=false},
  date = {2019-12-06},
  eprint = {1905.09272},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1905.09272},
  urldate = {2019-12-19},
  abstract = {Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on PASCAL VOC-2007, surpassing fully supervised pre-trained ImageNet classifiers.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\KXAHIJIH\\Hénaff et al_2019_Data-Efficient Image Recognition with Contrastive Predictive Coding.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VIQ74ZBG\\1905.html}
}

@article{henaffMODELPREDICTIVEPOLICYLEARNING2019,
  title = {{{MODEL-PREDICTIVE POLICY LEARNING WITH UNCERTAINTY REGULARIZATION FOR DRIVING IN DENSE TRAFFIC}}},
  author = {Henaff, Mikael and Canziani, Alfredo and LeCun, Yann},
  date = {2019},
  pages = {20},
  abstract = {Learning a policy using only observational data is challenging because the distribution of states it induces at execution time may differ from the distribution observed during training. We propose to train a policy by unrolling a learned model of the environment dynamics over multiple time steps while explicitly penalizing two costs: the original cost the policy seeks to optimize, and an uncertainty cost which represents its divergence from the states it is trained on. We measure this second cost by using the uncertainty of the dynamics model about its own predictions, using recent ideas from uncertainty estimation for deep networks. We evaluate our approach using a large-scale observational dataset of driving behavior recorded from traffic cameras, and show that we are able to learn effective driving policies from purely observational data, with no environment interaction.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\N22A2XIS\Henaff et al_2019_MODEL-PREDICTIVE POLICY LEARNING WITH UNCERTAINTY REGULARIZATION FOR DRIVING IN.pdf}
}

@article{hendrycksBaselineDetectingMisclassified2016,
  title = {A {{Baseline}} for {{Detecting Misclassified}} and {{Out-of-Distribution Examples}} in {{Neural Networks}}},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  date = {2016-11-04},
  url = {https://openreview.net/forum?id=Hkg4TI9xl},
  urldate = {2019-04-30},
  abstract = {We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions....},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\25VWDV47\\Hendrycks_Gimpel_2016_A Baseline for Detecting Misclassified and Out-of-Distribution Examples in.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\F3WC37L6\\Hendrycks_Gimpel_2016_A Baseline for Detecting Misclassified and Out-of-Distribution Examples in2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GUDUFGUV\\forum.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\ME9M9EAS\\1610.html}
}

@unpublished{hendrycksDeepAnomalyDetection2018,
  title = {Deep {{Anomaly Detection}} with {{Outlier Exposure}}},
  author = {Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas},
  date = {2018-12-11},
  eprint = {1812.04606},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1812.04606},
  urldate = {2019-05-02},
  abstract = {It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\H2PETIAL\\Hendrycks et al_2018_Deep Anomaly Detection with Outlier Exposure.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7K7VQWY9\\1812.html}
}

@unpublished{hendrycksGaussianErrorLinear2016,
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  date = {2016-06-27},
  eprint = {1606.08415},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1606.08415},
  urldate = {2019-03-05},
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map to a neuron's input. The GELU nonlinearity weights inputs by their magnitude, rather than gates inputs by their sign as in ReLUs. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\UWPMEAJ2\\Hendrycks_Gimpel_2016_Gaussian Error Linear Units (GELUs).pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\5TUX2Z7J\\1606.html}
}

@book{herrmannDeutschlandWirtschaftsmaerchenWarum2019,
  title = {Deutschland, ein Wirtschaftsmärchen Warum es kein Wunder ist, dass wir reich wurden},
  author = {Herrmann, Ulrike and {Westend Verlag GmbH}},
  date = {2019},
  abstract = {Deutschland ist reich, aber die gängigen Erklärungen sind falsch. So soll Ludwig Erhard der "Vater" des Wirtschaftswunders gewesen sein - in Wahrheit war er ein unfähiger Ökonom, ein Profiteur im Dritten Reich und ein Lügner. Die Bundesbank war angeblich die unbestechliche "Hüterin der D-Mark" - tatsächlich hat sie Millionen in die Arbeitslosigkeit geschickt und die deutsche Einheit fast ruiniert. "Soziale Marktwirtschaft" klingt nach sozialem Ausgleich, doch begünstigt werden die Reichen. Auch die permanenten Exportüberschüsse haben Deutschland nicht voran gebracht, sondern geschadet. Umgekehrt werden echte Erfolge nicht gesehen: Die Wiedervereinigung war angeblich wahnsinnig teuer. Tatsächlich hat sie keinen einzigen Cent gekostet. Es ist Zeit, sich von den Legenden zu verabschieden. Sonst verpassen wir unsere Zukunft.},
  isbn = {978-3-86489-263-9},
  langid = {ngerman},
  annotation = {OCLC: 1096294625},
  file = {C:\Users\USEBPERP\Zotero\storage\J2VIRXCN\Herrmann and Westend Verlag GmbH - 2019 - Deutschland, ein Wirtschaftsmärchen Warum es kein .pdf}
}

@article{herronNovoOriginsMulticellularity2019,
  title = {De Novo Origins of Multicellularity in Response to Predation},
  author = {Herron, Matthew D. and Borin, Joshua M. and Boswell, Jacob C. and Walker, Jillian and Chen, I.-Chen Kimberly and Knox, Charles A. and Boyd, Margrethe and Rosenzweig, Frank and Ratcliff, William C.},
  date = {2019-02-20},
  journaltitle = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {2328},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-39558-8},
  url = {https://www.nature.com/articles/s41598-019-39558-8},
  urldate = {2019-03-01},
  abstract = {The transition from unicellular to multicellular life was one of a few major events in the history of life that created new opportunities for more complex biological systems to evolve. Predation is hypothesized as one selective pressure that may have driven the evolution of multicellularity. Here we show that de novo origins of simple multicellularity can evolve in response to predation. We subjected outcrossed populations of the unicellular green alga Chlamydomonas reinhardtii to selection by the filter-feeding predator Paramecium tetraurelia. Two of five experimental populations evolved multicellular structures not observed in unselected control populations within \textasciitilde 750 asexual generations. Considerable variation exists in the evolved multicellular life cycles, with both cell number and propagule size varying among isolates. Survival assays show that evolved multicellular traits provide effective protection against predation. These results support the hypothesis that selection imposed by predators may have played a role in some origins of multicellularity.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LBHLWKU2\\Herron et al_2019_De novo origins of multicellularity in response to predation.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\W3XDUS6G\\s41598-019-39558-8.html}
}

@article{hertQuantifyingBiogenicBias2009,
  title = {Quantifying {{Biogenic Bias}} in {{Screening Libraries}}},
  author = {Hert, Jérôme and Irwin, John J. and Laggner, Christian and Keiser, Michael J. and Shoichet, Brian K.},
  date = {2009-07},
  journaltitle = {Nat Chem Biol},
  volume = {5},
  number = {7},
  eprint = {19483698},
  eprinttype = {pmid},
  pages = {479--483},
  issn = {1552-4450},
  doi = {10.1038/nchembio.180},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2783405/},
  urldate = {2020-07-09},
  abstract = {In lead discovery, libraries of 106 molecules are screened for biological activity. Given the over 1060 drug-like molecules thought possible, such screens might never succeed. That they do, even occasionally, implies a biased selection of library molecules. Here a method is developed to quantify the bias in screening libraries towards biogenic molecules. With this approach, we consider what is missing from screening libraries and how they can be optimized.},
  pmcid = {PMC2783405},
  file = {C:\Users\USEBPERP\Zotero\storage\X5P7YCFM\Hert et al_2009_Quantifying Biogenic Bias in Screening Libraries.pdf}
}

@article{hespeScalableKernelizationMaximum2019,
  title = {Scalable {{Kernelization}} for {{Maximum Independent Sets}}},
  author = {Hespe, Demian and Schulz, Christian and Strash, Darren},
  date = {2019-09-23},
  journaltitle = {ACM J. Exp. Algorithmics},
  volume = {24},
  pages = {1.16:1--1.16:22},
  issn = {1084-6654},
  doi = {10.1145/3355502},
  url = {https://doi.org/10.1145/3355502},
  urldate = {2023-09-02},
  abstract = {The most efficient algorithms for finding maximum independent sets in both theory and practice use reduction rules to obtain a much smaller problem instance called a kernel. The kernel can then be solved quickly using exact or heuristic algorithms—or by repeatedly kernelizing recursively in the branch-and-reduce paradigm. Current algorithms are either slow but produce a small kernel or fast and give a large kernel. Yet it is of critical importance for these algorithms that kernelization is fast and returns a small kernel. We attempt to accomplish both of these goals simultaneously by giving an efficient parallel kernelization algorithm based on graph partitioning and parallel bipartite maximum matching. We combine our parallelization techniques with two techniques to accelerate kernelization further: dependency checking that prunes reductions that cannot be applied, and reduction tracking that allows us to stop kernelization when reductions become less fruitful. Our algorithm produces kernels that are orders of magnitude smaller than the fastest kernelization methods while having a similar execution time. Furthermore, our algorithm is able to compute kernels with size comparable to the smallest known kernels but up to two orders of magnitude faster than possible previously. Finally, we show that our kernelization algorithm can be used to accelerate existing state-of-the-art heuristic algorithms, allowing us to find larger independent sets faster on large real-world networks and synthetic instances.},
  keywords = {graphs,kernelization,Maximum independent set,parallelization,shared-memory},
  file = {C:\Users\USEBPERP\Zotero\storage\48G2QZXG\Hespe et al. - 2019 - Scalable Kernelization for Maximum Independent Set.pdf}
}

@article{hesslerArtificialIntelligenceDrug2018,
  title = {Artificial {{Intelligence}} in {{Drug Design}}},
  author = {Hessler, Gerhard and Baringhaus, Karl-Heinz and Hessler, Gerhard and Baringhaus, Karl-Heinz},
  date = {2018-10-02},
  journaltitle = {Molecules},
  volume = {23},
  number = {10},
  pages = {2520},
  doi = {10.3390/molecules23102520},
  url = {https://www.mdpi.com/1420-3049/23/10/2520},
  urldate = {2018-10-10},
  abstract = {Artificial Intelligence (AI) plays a pivotal role in drug discovery. In particular artificial neural networks such as deep neural networks or recurrent networks drive this area. Numerous applications in property or activity predictions like physicochemical and ADMET properties have recently appeared and underpin the strength of this technology in quantitative structure-property relationships (QSPR) or quantitative structure-activity relationships (QSAR). Artificial intelligence in de novo design drives the generation of meaningful new biologically active molecules towards desired properties. Several examples establish the strength of artificial intelligence in this field. Combination with synthesis planning and ease of synthesis is feasible and more and more automated drug discovery by computers is expected in the near future.},
  langid = {english},
  keywords = {artificial intelligence,de novo design,deep learning,neural networks,property prediction,quantitative structure-activity relationship (QSAR),quantitative structure-property prediction (QSPR)},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\8DV48Y4D\\Hessler et al_2018_Artificial Intelligence in Drug Design.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\W8276JLR\\2520.html}
}

@article{heTemporalConvolutionalNetworks2019,
  title = {Temporal {{Convolutional Networks}} for {{Anomaly Detection}} in {{Time Series}}},
  author = {He, Yangdong and Zhao, Jiabao},
  date = {2019-06-01},
  journaltitle = {J. Phys.: Conf. Ser.},
  volume = {1213},
  number = {4},
  pages = {042050},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/1213/4/042050},
  url = {https://iopscience.iop.org/article/10.1088/1742-6596/1213/4/042050},
  urldate = {2023-01-06},
  abstract = {Convolutional Networks have been demonstrated to be particularly useful for extracting high level feature in structural data. Temporal convolutional network (TCN) is a framework which employs casual convolutions and dilations so that it is adaptive for sequential data with its temporality and large receptive fields. In this paper, we apply TCN for anomaly detection in time series. We train the TCN on normal sequences and use it to predict trend in a number of time steps. Prediction errors are fitted by a multivariate Gaussian distribution and used to calculate the anomaly scores of points. In addition, a multi-scale feature mixture method is raised to promote performance. The validity of this method is confirmed on three real-world datasets.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\I3EGRKKU\He and Zhao - 2019 - Temporal Convolutional Networks for Anomaly Detect.pdf}
}

@unpublished{heUnsupervisedLearningSyntactic2018,
  title = {Unsupervised {{Learning}} of {{Syntactic Structure}} with {{Invertible Neural Projections}}},
  author = {He, Junxian and Neubig, Graham and Berg-Kirkpatrick, Taylor},
  date = {2018-08-28},
  eprint = {1808.09111},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1808.09111},
  urldate = {2019-12-19},
  abstract = {Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\PVGY3H2H\\He et al_2018_Unsupervised Learning of Syntactic Structure with Invertible Neural Projections.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\3KCH2X6P\\1808.html}
}

@unpublished{heuselGANsTrainedTwo2017,
  title = {{{GANs Trained}} by a {{Two Time-Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  date = {2017-06-26},
  eprint = {1706.08500},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.08500},
  urldate = {2018-05-08},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\textbackslash 'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\YJN5VHBC\\Heusel et al_2017_GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ICZTW7TG\\1706.html}
}

@unpublished{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  date = {2015-03-09},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1503.02531},
  urldate = {2021-09-09},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2Y2ZIETH\\Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\UMFFS2H6\\1503.html}
}

@unpublished{hintonImprovingNeuralNetworks2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  date = {2012-07-03},
  eprint = {1207.0580},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1207.0580},
  urldate = {2019-03-28},
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\UUCYCB5G\\Hinton et al_2012_Improving neural networks by preventing co-adaptation of feature detectors.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VHV8XEWD\\1207.html}
}

@online{HistogrambasedOutlierScore,
  title = {Histogram-Based Outlier Score (Hbos): A Fast Unsupervised Anomaly Detection Algorithm - {{Google Suche}}},
  url = {https://www.google.com/search?q=Histogram-based+outlier+score+(hbos)%3A+a+fast+unsupervised+anomaly+detection+algorithm&rlz=1C5GCEM_enDE1028DE1028&sourceid=chrome&ie=UTF-8},
  urldate = {2023-04-11},
  file = {C:\Users\USEBPERP\Zotero\storage\5LGXCUTX\search.html}
}

@article{hochreiterBioinformaticsSequenceAnalysis,
  title = {Bioinformatics {{I}}: {{Sequence Analysis}} and {{Phylogenetics}}},
  author = {Hochreiter, Sepp},
  pages = {184},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\MIUG4JCB\Hochreiter_Bioinformatics I.pdf}
}

@book{hochreiterGradientFlowRecurrent2001,
  title = {Gradient Flow in Recurrent Nets: The Difficulty of Learning Long-Term Dependencies},
  shorttitle = {Gradient Flow in Recurrent Nets},
  author = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, Jürgen},
  date = {2001},
  publisher = {A field guide to dynamical recurrent neural networks. IEEE Press},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2VBUQ5KK\\Hochreiter et al_2001_Gradient flow in recurrent nets.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7EVE7PE2\\Hochreiter et al_2001_Gradient flow in recurrent nets2.pdf}
}

@article{hochreiterLongShorttermMemory1997,
  title = {Long {{Short-term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-12-01},
  journaltitle = {Neural computation},
  volume = {9},
  pages = {1735--80},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {C:\Users\USEBPERP\Zotero\storage\QDDWN6T7\Hochreiter_Schmidhuber_1997_Long Short-term Memory.pdf}
}

@article{hochreiterMachineLearningDrug2018,
  title = {Machine {{Learning}} in {{Drug Discovery}}},
  author = {Hochreiter, Sepp and Klambauer, Guenter and Rarey, Matthias},
  date = {2018-09-24},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {58},
  number = {9},
  pages = {1723--1724},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.8b00478},
  url = {https://doi.org/10.1021/acs.jcim.8b00478},
  urldate = {2020-09-08},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\VWPP6HZC\\Hochreiter et al_2018_Machine Learning in Drug Discovery.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DSH4RJ4Z\\acs.jcim.html}
}

@article{hochreiterVanishingGradientProblem1998,
  title = {The {{Vanishing Gradient Problem During Learning Recurrent Neural Nets}} and {{Problem Solutions}}},
  author = {Hochreiter, Sepp},
  date = {1998-04},
  journaltitle = {Int. J. Uncertain. Fuzziness Knowl.-Based Syst.},
  volume = {6},
  number = {2},
  pages = {107--116},
  issn = {0218-4885},
  doi = {10.1142/S0218488598000094},
  url = {http://dx.doi.org/10.1142/S0218488598000094},
  urldate = {2018-05-25},
  keywords = {long short-term memory,long-term dependencies,recurrent neural nets,vanishing gradient}
}

@article{hodgeSurveyOutlierDetection2004,
  title = {A {{Survey}} of {{Outlier Detection Methodologies}}},
  author = {Hodge, Victoria J. and Austin, Jim},
  date = {2004-10-01},
  journaltitle = {Artif Intell Rev},
  volume = {22},
  number = {2},
  pages = {85--126},
  issn = {1573-7462},
  doi = {10.1007/s10462-004-4304-y},
  url = {https://doi.org/10.1007/s10462-004-4304-y},
  urldate = {2023-01-06},
  abstract = {Outlier detection has been used for centuries to detect and, where appropriate, remove anomalous observations from data. Outliers arise due to mechanical faults, changes in system behaviour, fraudulent behaviour, human error, instrument error or simply through natural deviations in populations. Their detection can identify system faults and fraud before they escalate with potentially catastrophic consequences. It can identify errors and remove their contaminating effect on the data set and as such to purify the data for processing. The original outlier detection methods were arbitrary but now, principled and systematic techniques are used, drawn from the full gamut of Computer Science and Statistics. In this paper, we introduce a survey of contemporary techniques for outlier detection. We identify their respective motivations and distinguish their advantages and disadvantages in a comparative review.},
  langid = {english},
  keywords = {anomaly,detection,deviation,noise,novelty,outlier,recognition},
  file = {C:\Users\USEBPERP\Zotero\storage\P2KKSVGD\Hodge and Austin - 2004 - A Survey of Outlier Detection Methodologies.pdf}
}

@inproceedings{hofferAugmentYourBatch2020,
  title = {Augment {{Your Batch}}: {{Improving Generalization Through Instance Repetition}}},
  shorttitle = {Augment {{Your Batch}}},
  author = {Hoffer, Elad and Ben-Nun, Tal and Hubara, Itay and Giladi, Niv and Hoefler, Torsten and Soudry, Daniel},
  date = {2020},
  pages = {8129--8138},
  url = {http://openaccess.thecvf.com/content_CVPR_2020/html/Hoffer_Augment_Your_Batch_Improving_Generalization_Through_Instance_Repetition_CVPR_2020_paper.html},
  urldate = {2020-06-19},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\HM4D9BBT\\Hoffer et al_2020_Augment Your Batch.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DM2DN6US\\Hoffer_Augment_Your_Batch_Improving_Generalization_Through_Instance_Repetition_CVPR_2020_paper.html}
}

@incollection{hofferTrainLongerGeneralize2017,
  title = {Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks},
  shorttitle = {Train Longer, Generalize Better},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  pages = {1731--1741},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks.pdf},
  urldate = {2020-02-03},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\G9ERX45V\\Hoffer et al_2017_Train longer, generalize better.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BUZT2YWY\\6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-n.html}
}

@article{hoffmannBuildingBridgesInorganic1982,
  title = {Building {{Bridges Between Inorganic}} and {{Organic Chemistry}} ({{Nobel Lecture}})},
  author = {Hoffmann, Roald},
  date = {1982},
  journaltitle = {Angewandte Chemie International Edition in English},
  volume = {21},
  number = {10},
  pages = {711--724},
  issn = {1521-3773},
  doi = {10.1002/anie.198207113},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.198207113},
  urldate = {2021-04-08},
  abstract = {Robert B. Woodward, a supreme patterner of chaos, was one of my teachers. I dedicate this lecture to him, for it is our collaboration on orbital symmetry conservation, the electronic factors which govern the course of chemical reactions, which is recognized by half of the 1981 Nobel Prize in Chemistry. From Woodward I learned much: the significance of the experimental stimulus to theory, the craft of constructing explanations, and the importance of asethetics in science. I will try to show you how these characteristics of chemical theory may be applied to the construction of conceptual bridges between inorganic and organic chemistry.},
  langid = {english},
  keywords = {Isolobal relationship,Nobel lecture,Orbital symmetry},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\UJIURKCI\\Hoffmann - 1982 - Building Bridges Between Inorganic and Organic Che.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\4WABLUJ8\\anie.html}
}

@online{hofmarcherLargescaleLigandbasedVirtual2020,
  title = {Large-Scale Ligand-Based Virtual Screening for {{SARS-CoV-2}} Inhibitors Using Deep Neural Networks},
  author = {Hofmarcher, Markus and Mayr, Andreas and Rumetshofer, Elisabeth and Ruch, Peter and Renz, Philipp and Schimunek, Johannes and Seidl, Philipp and Vall, Andreu and Widrich, Michael and Hochreiter, Sepp and Klambauer, Günter},
  date = {2020-08-17},
  eprint = {2004.00979},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio, stat},
  doi = {10.48550/arXiv.2004.00979},
  url = {http://arxiv.org/abs/2004.00979},
  urldate = {2022-09-16},
  abstract = {Due to the current severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) pandemic, there is an urgent need for novel therapies and drugs. We conducted a large-scale virtual screening for small molecules that are potential CoV-2 inhibitors. To this end, we utilized "ChemAI", a deep neural network trained on more than 220M data points across 3.6M molecules from three public drug-discovery databases. With ChemAI, we screened and ranked one billion molecules from the ZINC database for favourable effects against CoV-2. We then reduced the result to the 30,000 top-ranked compounds, which are readily accessible and purchasable via the ZINC database. Additionally, we screened the DrugBank using ChemAI to allow for drug repurposing, which would be a fast way towards a therapy. We provide these top-ranked compounds of ZINC and DrugBank as a library for further screening with bioassays at https://github.com/ml-jku/sars-cov-inhibitors-chemai.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\36DW9QUP\\Hofmarcher et al. - 2020 - Large-scale ligand-based virtual screening for SAR.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\EHBQFUY2\\2004.html}
}

@article{hokszaMolpherSoftwareFramework2014,
  title = {Molpher: A Software Framework for Systematic Chemical Space Exploration},
  shorttitle = {Molpher},
  author = {Hoksza, David and Škoda, Petr and Voršilák, Milan and Svozil, Daniel},
  date = {2014-03-21},
  journaltitle = {Journal of Cheminformatics},
  volume = {6},
  number = {1},
  pages = {7},
  issn = {1758-2946},
  doi = {10.1186/1758-2946-6-7},
  url = {https://doi.org/10.1186/1758-2946-6-7},
  urldate = {2018-10-05},
  abstract = {Chemical space is virtual space occupied by all chemically meaningful organic compounds. It is an important concept in contemporary chemoinformatics research, and its systematic exploration is vital to the discovery of either novel drugs or new tools for chemical biology.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\RT5I2XQA\\Hoksza et al_2014_Molpher.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\D97XW3IF\\1758-2946-6-7.html}
}

@unpublished{holtzmanCuriousCaseNeural2019,
  title = {The {{Curious Case}} of {{Neural Text Degeneration}}},
  author = {Holtzman, Ari and Buys, Jan and Forbes, Maxwell and Choi, Yejin},
  date = {2019-04-22},
  eprint = {1904.09751},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1904.09751},
  urldate = {2019-09-30},
  abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NUERNSU2\\Holtzman et al_2019_The Curious Case of Neural Text Degeneration.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LSJ8XYUV\\1904.html}
}

@unpublished{hondaGraphResidualFlow2019,
  title = {Graph {{Residual Flow}} for {{Molecular Graph Generation}}},
  author = {Honda, Shion and Akita, Hirotaka and Ishiguro, Katsuhiko and Nakanishi, Toshiki and Oono, Kenta},
  date = {2019-09-30},
  eprint = {1909.13521},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1909.13521},
  urldate = {2019-12-19},
  abstract = {Statistical generative models for molecular graphs attract attention from many researchers from the fields of bio- and chemo-informatics. Among these models, invertible flow-based approaches are not fully explored yet. In this paper, we propose a powerful invertible flow for molecular graphs, called graph residual flow (GRF). The GRF is based on residual flows, which are known for more flexible and complex non-linear mappings than traditional coupling flows. We theoretically derive non-trivial conditions such that GRF is invertible, and present a way of keeping the entire flows invertible throughout the training and sampling. Experimental results show that a generative model based on the proposed GRF achieves comparable generation performance, with much smaller number of trainable parameters compared to the existing flow-based model.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\RR3C3NJ6\\Honda et al_2019_Graph Residual Flow for Molecular Graph Generation.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\4BELVC2B\\1909.html}
}

@unpublished{hoogeboomIntegerDiscreteFlows2019,
  title = {Integer {{Discrete Flows}} and {{Lossless Compression}}},
  author = {Hoogeboom, Emiel and Peters, Jorn W. T. and family=Berg, given=Rianne, prefix=van den, useprefix=false and Welling, Max},
  date = {2019-05-17},
  eprint = {1905.07376},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.07376},
  urldate = {2019-05-27},
  abstract = {Lossless compression methods shorten the expected representation size of data without loss of information, using a statistical model. Flow-based models are attractive in this setting because they admit exact likelihood optimization, which is equivalent to minimizing the expected number of bits per message. However, conventional flows assume continuous data, which may lead to reconstruction errors when quantized for compression. For that reason, we introduce a flow-based generative model for ordinal discrete data called Integer Discrete Flow (IDF): a bijective integer map that can learn rich transformations on high-dimensional data. As building blocks for IDFs, we introduce a flexible transformation layer called integer discrete coupling. Our experiments show that IDFs are competitive with other flow-based generative models. Furthermore, we demonstrate that IDF based compression achieves state-of-the-art lossless compression rates on CIFAR10, ImageNet32, and ImageNet64. To the best of our knowledge, this is the first lossless compression method that uses invertible neural networks.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\78PKFVC9\\Hoogeboom et al_2019_Integer Discrete Flows and Lossless Compression.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SBWIWIEC\\1905.html}
}

@article{hoonakkerCondensedGraphReaction,
  title = {Condensed {{Graph}} of {{Reaction}}: {{Considering}} a {{Chemical Reaction As}} One {{Single Pseudo Molecule}}},
  author = {Hoonakker, Frank and Lachiche, Nicolas and Varnek, Alexandre and Wagner, Alain},
  pages = {6},
  abstract = {Chemical reactions always involve several molecules of two types reactants and products. On the other hand, Quantitative Structure Activity Relationship (QSAR) methods are used to predict physicchemical or biological properties of individual molecules. In this article, we propose to use Condensed Graph of Reaction (CGR) approach merging all molecules involved in a reaction into one molecular graph. This allows one to consider reactions as pseudo-molecules and to develop QSAR models based on fragment descriptors. Here Substructure Molecular Fragment descriptors calculated from CGRs have been used to build quantitative models for the rate constant of SN2 reactions in water. Three common attribute-value regression algorithms (linear regression, support vector machine, and regression trees) have been evaluated.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\G6LDJ543\Hoonakker et al. - Condensed Graph of Reaction Considering a Chemica.pdf}
}

@article{horrobinModernBiomedicalResearch2003,
  title = {Modern Biomedical Research: An Internally Self-Consistent Universe with Little Contact with Medical Reality?},
  shorttitle = {Modern Biomedical Research},
  author = {Horrobin, David F.},
  date = {2003-02},
  journaltitle = {Nature Reviews Drug Discovery},
  volume = {2},
  number = {2},
  pages = {151--154},
  publisher = {Nature Publishing Group},
  issn = {1474-1784},
  doi = {10.1038/nrd1012},
  url = {https://www.nature.com/articles/nrd1012},
  urldate = {2020-07-09},
  abstract = {Congruence between in vitro and animal models of disease and the corresponding human condition is a fundamental assumption of much biomedical research, but it is one that is rarely critically assessed. In the absence of such critical assessment, the assumption of congruence may be invalid for most models. Much more open discussion of this issue is required if biomedical research is to be clinically productive.},
  issue = {2},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\JQA5NT76\nrd1012.html}
}

@article{horwoodMolecularDesignSynthetically2020,
  title = {Molecular {{Design}} in {{Synthetically Accessible Chemical Space}} via {{Deep Reinforcement Learning}}},
  author = {Horwood, Julien and Noutahi, Emmanuel},
  date = {2020-12-29},
  journaltitle = {ACS Omega},
  volume = {5},
  number = {51},
  eprint = {2004.14308},
  eprinttype = {arxiv},
  pages = {32984--32994},
  issn = {2470-1343, 2470-1343},
  doi = {10.1021/acsomega.0c04153},
  url = {http://arxiv.org/abs/2004.14308},
  urldate = {2021-02-03},
  abstract = {The fundamental goal of generative drug design is to propose optimized molecules that meet predefined activity, selectivity, and pharmacokinetic criteria. Despite recent progress, we argue that existing generative methods are limited in their ability to favourably shift the distributions of molecular properties during optimization. We instead propose a novel Reinforcement Learning framework for molecular design in which an agent learns to directly optimize through a space of synthetically-accessible drug-like molecules. This becomes possible by defining transitions in our Markov Decision Process as chemical reactions, and allows us to leverage synthetic routes as an inductive bias. We validate our method by demonstrating that it outperforms existing state-of the art approaches in the optimization of pharmacologically-relevant objectives, while results on multi-objective optimization tasks suggest increased scalability to realistic pharmaceutical design problems.},
  issue = {51},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Statistics - Machine Learning}
}

@article{howardFastaiLayeredAPI2020,
  title = {Fastai: {{A Layered API}} for {{Deep Learning}}},
  shorttitle = {Fastai},
  author = {Howard, Jeremy and Gugger, Sylvain},
  date = {2020-02-16},
  journaltitle = {Information},
  volume = {11},
  number = {2},
  eprint = {2002.04688},
  eprinttype = {arxiv},
  pages = {108},
  issn = {2078-2489},
  doi = {10.3390/info11020108},
  url = {http://arxiv.org/abs/2002.04688},
  urldate = {2020-08-13},
  abstract = {fastai is a deep learning library which provides practitioners with high-level components that can quickly and easily provide state-of-the-art results in standard deep learning domains, and provides researchers with low-level components that can be mixed and matched to build new approaches. It aims to do both things without substantial compromises in ease of use, flexibility, or performance. This is possible thanks to a carefully layered architecture, which expresses common underlying patterns of many deep learning and data processing techniques in terms of decoupled abstractions. These abstractions can be expressed concisely and clearly by leveraging the dynamism of the underlying Python language and the flexibility of the PyTorch library. fastai includes: a new type dispatch system for Python along with a semantic type hierarchy for tensors; a GPU-optimized computer vision library which can be extended in pure Python; an optimizer which refactors out the common functionality of modern optimizers into two basic pieces, allowing optimization algorithms to be implemented in 4-5 lines of code; a novel 2-way callback system that can access any part of the data, model, or optimizer and change it at any point during training; a new data block API; and much more. We have used this library to successfully create a complete deep learning course, which we were able to write more quickly than using previous approaches, and the code was more clear. The library is already in wide use in research, industry, and teaching. NB: This paper covers fastai v2, which is currently in pre-release at http://dev.fast.ai/},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\B3KY7YSP\\Howard_Gugger_2020_fastai.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\L9J9FJFG\\2002.html}
}

@unpublished{howardUniversalLanguageModel2018,
  title = {Universal {{Language Model Fine-tuning}} for {{Text Classification}}},
  author = {Howard, Jeremy and Ruder, Sebastian},
  date = {2018-01-18},
  eprint = {1801.06146},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1801.06146},
  urldate = {2019-10-16},
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ERDSPYUU\\Howard_Ruder_2018_Universal Language Model Fine-tuning for Text Classification.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DYHF7BTI\\1801.html}
}

@online{HowDiverseAre,
  title = {How {{Diverse Are Diversity Assessment Methods}}? {{A Comparative Analysis}} and {{Benchmarking}} of {{Molecular Descriptor Space}} | {{Journal}} of {{Chemical Information}} and {{Modeling}}},
  url = {https://pubs.acs.org/doi/abs/10.1021/ci400469u},
  urldate = {2023-09-01},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JGMRQSKA\\How Diverse Are Diversity Assessment Methods A Co.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ARIHGNNX\\ci400469u.html}
}

@article{huangCurrentDevelopmentsComputeraided2010,
  title = {Current Developments of Computer-Aided Drug Design},
  author = {Huang, Hung-Jin and Yu, Hsin Wei and Chen, Chien-Yu and Hsu, Chih-Ho and Chen, Hsin-Yi and Lee, Kuei-Jen and Tsai, Fuu-Jen and Chen, Calvin Yu-Chian},
  date = {2010},
  journaltitle = {Journal of the Taiwan Institute of Chemical Engineers},
  volume = {41},
  number = {6},
  pages = {623--635},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\84JQ8KPU\\Huang et al_2010_Current developments of computer-aided drug design.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VQARXXYL\\S1876107010000556.html}
}

@unpublished{huangSnapshotEnsemblesTrain2017,
  title = {Snapshot {{Ensembles}}: {{Train}} 1, Get {{M}} for Free},
  shorttitle = {Snapshot {{Ensembles}}},
  author = {Huang, Gao and Li, Yixuan and Pleiss, Geoff and Liu, Zhuang and Hopcroft, John E. and Weinberger, Kilian Q.},
  date = {2017-03-31},
  eprint = {1704.00109},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1704.00109},
  urldate = {2019-09-10},
  abstract = {Ensembles of neural networks are known to be much more robust and accurate than individual networks. However, training multiple deep networks for model averaging is computationally expensive. In this paper, we propose a method to obtain the seemingly contradictory goal of ensembling multiple neural networks at no additional training cost. We achieve this goal by training a single neural network, converging to several local minima along its optimization path and saving the model parameters. To obtain repeated rapid convergence, we leverage recent work on cyclic learning rate schedules. The resulting technique, which we refer to as Snapshot Ensembling, is simple, yet surprisingly effective. We show in a series of experiments that our approach is compatible with diverse network architectures and learning tasks. It consistently yields lower error rates than state-of-the-art single models at no additional training cost, and compares favorably with traditional network ensembles. On CIFAR-10 and CIFAR-100 our DenseNet Snapshot Ensembles obtain error rates of 3.4\% and 17.4\% respectively.},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DMZFDE5X\\Huang et al_2017_Snapshot Ensembles.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\27C5TN2V\\1704.html}
}

@article{hudsonParameterBasedMethods1996,
  title = {Parameter {{Based Methods}} for {{Compound Selection}} from {{Chemical Databases}}},
  author = {Hudson, Brian D. and Hyde, Richard M. and Rahr, Elizabeth and Wood, John and Osman, Julian},
  date = {1996},
  journaltitle = {Quantitative Structure-Activity Relationships},
  volume = {15},
  number = {4},
  pages = {285--289},
  issn = {1521-3838},
  doi = {10.1002/qsar.19960150402},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qsar.19960150402},
  urldate = {2022-06-10},
  abstract = {Two algorithms for the selection of subsets of compounds from chemical databases are presented and discussed. The first is designed to select representative subsets whilst the second is intended to select compounds which cover the available property space. Both make use of calculated physicochemical parameters in contrast to more common methods based on molecular fingerprints. This is an approach to molecular similarity which has proved successful in the past. The methods are illustrated with examples and discussed.},
  langid = {english},
  keywords = {chemical databases,Compound selection,high throughput screening,representative sets},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NF2B5KTZ\\Hudson et al. - 1996 - Parameter Based Methods for Compound Selection fro.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\PCVBWHMY\\qsar.html}
}

@article{hughesPrinciplesEarlyDrug2011,
  title = {Principles of Early Drug Discovery},
  author = {family=Hughes, given=JP, given-i=JP and Rees, S and family=Kalindjian, given=SB, given-i=SB and family=Philpott, given=KL, given-i=KL},
  date = {2011-03},
  journaltitle = {Br J Pharmacol},
  volume = {162},
  number = {6},
  eprint = {21091654},
  eprinttype = {pmid},
  pages = {1239--1249},
  issn = {0007-1188},
  doi = {10.1111/j.1476-5381.2010.01127.x},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3058157/},
  urldate = {2019-05-20},
  abstract = {Developing a new drug from original idea to the launch of a finished product is a complex process which can take 12–15 years and cost in excess of \$1 billion. The idea for a target can come from a variety of sources including academic and clinical research and from the commercial sector. It may take many years to build up a body of supporting evidence before selecting a target for a costly drug discovery programme. Once a target has been chosen, the pharmaceutical industry and more recently some academic centres have streamlined a number of early processes to identify molecules which possess suitable characteristics to make acceptable drugs. This review will look at key preclinical stages of the drug discovery process, from initial target identification and validation, through assay development, high throughput screening, hit identification, lead optimization and finally the selection of a candidate molecule for clinical development.},
  pmcid = {PMC3058157},
  file = {C:\Users\USEBPERP\Zotero\storage\G55UNG4Q\Hughes et al_2011_Principles of early drug discovery.pdf}
}

@article{huLEAPPfizerGlobal2011,
  title = {{{LEAP}} into the {{Pfizer Global Virtual Library}} ({{PGVL}}) Space: Creation of Readily Synthesizable Design Ideas Automatically},
  shorttitle = {{{LEAP}} into the {{Pfizer Global Virtual Library}} ({{PGVL}}) Space},
  author = {Hu, Qiyue and Peng, Zhengwei and Kostrowicki, Jaroslav and Kuki, Atsuo},
  date = {2011},
  journaltitle = {Methods Mol. Biol.},
  volume = {685},
  eprint = {20981528},
  eprinttype = {pmid},
  pages = {253--276},
  issn = {1940-6029},
  doi = {10.1007/978-1-60761-931-4_13},
  abstract = {Pfizer Global Virtual Library (PGVL) of 10(13) readily synthesizable molecules offers a tremendous opportunity for lead optimization and scaffold hopping in drug discovery projects. However, mining into a chemical space of this size presents a challenge for the concomitant design informatics due to the fact that standard molecular similarity searches against a collection of explicit molecules cannot be utilized, since no chemical information system could create and manage more than 10(8) explicit molecules. Nevertheless, by accepting a tolerable level of false negatives in search results, we were able to bypass the need for full 10(13) enumeration and enabled the efficient similarity search and retrieval into this huge chemical space for practical usage by medicinal chemists. In this report, two search methods (LEAP1 and LEAP2) are presented. The first method uses PGVL reaction knowledge to disassemble the incoming search query molecule into a set of reactants and then uses reactant-level similarities into actual available starting materials to focus on a much smaller sub-region of the full virtual library compound space. This sub-region is then explicitly enumerated and searched via a standard similarity method using the original query molecule. The second method uses a fuzzy mapping onto candidate reactions and does not require exact disassembly of the incoming query molecule. Instead Basis Products (or capped reactants) are mapped into the query molecule and the resultant asymmetric similarity scores are used to prioritize the corresponding reactions and reactant sets. All sets of Basis Products are inherently indexed to specific reactions and specific starting materials. This again allows focusing on a much smaller sub-region for explicit enumeration and subsequent standard product-level similarity search. A set of validation studies were conducted. The results have shown that the level of false negatives for the disassembly-based method is acceptable when the query molecule can be recognized for exact disassembly, and the fuzzy reaction mapping method based on Basis Products has an even better performance in terms of lower false-negative rate because it is not limited by the requirement that the query molecule needs to be recognized by any disassembly algorithm. Both search methods have been implemented and accessed through a powerful desktop molecular design tool (see ref. (33) for details). The chapter will end with a comparison of published search methods against large virtual chemical space.},
  langid = {english},
  keywords = {Algorithms,Automation,Data Mining,Drug Discovery,Drug Industry,False Negative Reactions,Models Molecular,Molecular Conformation,Reproducibility of Results,Small Molecule Libraries,Time Factors,User-Computer Interface}
}

@inproceedings{hundmanDetectingSpacecraftAnomalies2018,
  title = {Detecting {{Spacecraft Anomalies Using LSTMs}} and {{Nonparametric Dynamic Thresholding}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Hundman, Kyle and Constantinou, Valentino and Laporte, Christopher and Colwell, Ian and Soderstrom, Tom},
  date = {2018-07-19},
  eprint = {1802.04431},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {387--395},
  doi = {10.1145/3219819.3219845},
  url = {http://arxiv.org/abs/1802.04431},
  urldate = {2023-01-27},
  abstract = {As spacecraft send back increasing amounts of telemetry data, improved anomaly detection systems are needed to lessen the monitoring burden placed on operations engineers and reduce operational risk. Current spacecraft monitoring systems only target a subset of anomaly types and often require costly expert knowledge to develop and maintain due to challenges involving scale and complexity. We demonstrate the effectiveness of Long Short-Term Memory (LSTMs) networks, a type of Recurrent Neural Network (RNN), in overcoming these issues using expert-labeled telemetry anomaly data from the Soil Moisture Active Passive (SMAP) satellite and the Mars Science Laboratory (MSL) rover, Curiosity. We also propose a complementary unsupervised and nonparametric anomaly thresholding approach developed during a pilot implementation of an anomaly detection system for SMAP, and offer false positive mitigation strategies along with other key improvements and lessons learned during development.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\XKWCEIHD\\Hundman et al. - 2018 - Detecting Spacecraft Anomalies Using LSTMs and Non.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\F6MCM3S6\\1802.html}
}

@unpublished{hungOptimizingAgentBehavior2018,
  title = {Optimizing {{Agent Behavior}} over {{Long Time Scales}} by {{Transporting Value}}},
  author = {Hung, Chia-Chun and Lillicrap, Timothy and Abramson, Josh and Wu, Yan and Mirza, Mehdi and Carnevale, Federico and Ahuja, Arun and Wayne, Greg},
  date = {2018-10-15},
  eprint = {1810.06721},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1810.06721},
  urldate = {2019-06-06},
  abstract = {Humans spend a remarkable fraction of waking life engaged in acts of "mental time travel". We dwell on our actions in the past and experience satisfaction or regret. More than merely autobiographical storytelling, we use these event recollections to change how we will act in similar scenarios in the future. This process endows us with a computationally important ability to link actions and consequences across long spans of time, which figures prominently in addressing the problem of long-term temporal credit assignment; in artificial intelligence (AI) this is the question of how to evaluate the utility of the actions within a long-duration behavioral sequence leading to success or failure in a task. Existing approaches to shorter-term credit assignment in AI cannot solve tasks with long delays between actions and consequences. Here, we introduce a new paradigm for reinforcement learning where agents use recall of specific memories to credit actions from the past, allowing them to solve problems that are intractable for existing algorithms. This paradigm broadens the scope of problems that can be investigated in AI and offers a mechanistic account of behaviors that may inspire computational models in neuroscience, psychology, and behavioral economics.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2TKMX2RX\\Hung et al_2018_Optimizing Agent Behavior over Long Time Scales by Transporting Value.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LTP6FC2F\\1810.html}
}

@online{HuntingOrganicMolecules,
  title = {Hunting for {{Organic Molecules}} with {{Artificial Intelligence}}: {{Molecules Optimized}} for {{Desired Excitation Energies}} - {{ACS Central Science}} ({{ACS Publications}})},
  url = {https://pubs.acs.org/doi/10.1021/acscentsci.8b00213},
  urldate = {2019-05-06},
  file = {C:\Users\USEBPERP\Zotero\storage\7HC85VPK\acscentsci.html}
}

@unpublished{huoLargeBatchTraining2020,
  title = {Large {{Batch Training Does Not Need Warmup}}},
  author = {Huo, Zhouyuan and Gu, Bin and Huang, Heng},
  date = {2020-02-04},
  eprint = {2002.01576},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.01576},
  urldate = {2020-06-19},
  abstract = {Training deep neural networks using a large batch size has shown promising results and benefits many real-world applications. However, the optimizer converges slowly at early epochs and there is a gap between large-batch deep learning optimization heuristics and theoretical underpinnings. In this paper, we propose a novel Complete Layer-wise Adaptive Rate Scaling (CLARS) algorithm for large-batch training. We also analyze the convergence rate of the proposed method by introducing a new fine-grained analysis of gradient-based methods. Based on our analysis, we bridge the gap and illustrate the theoretical insights for three popular large-batch training techniques, including linear learning rate scaling, gradual warmup, and layer-wise adaptive rate scaling. Extensive experiments demonstrate that the proposed algorithm outperforms gradual warmup technique by a large margin and defeats the convergence of the state-of-the-art large-batch optimizer in training advanced deep neural networks (ResNet, DenseNet, MobileNet) on ImageNet dataset.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\42FN928A\\Huo et al_2020_Large Batch Training Does Not Need Warmup.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\IVMKEHIY\\2002.html}
}

@article{huPfizerGlobalVirtual2012,
  title = {Pfizer {{Global Virtual Library}} ({{PGVL}}): {{A Chemistry Design Tool Powered}} by {{Experimentally Validated Parallel Synthesis Information}}},
  shorttitle = {Pfizer {{Global Virtual Library}} ({{PGVL}})},
  author = {Hu, Qiyue and Peng, Zhengwei and Sutton, Scott C. and Na, Jim and Kostrowicki, Jaroslav and Yang, Bo and Thacher, Thomas and Kong, Xianjun and Mattaparti, Sarathy and Zhou, Joe Zhongxiang and Gonzalez, Javier and Ramirez-Weinhouse, Michele and Kuki, Atsuo},
  date = {2012-11-12},
  journaltitle = {ACS Comb. Sci.},
  volume = {14},
  number = {11},
  pages = {579--589},
  issn = {2156-8952, 2156-8944},
  doi = {10.1021/co300096q},
  url = {https://pubs.acs.org/doi/10.1021/co300096q},
  urldate = {2020-06-03},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\FHPQNQBL\Hu et al_2012_Pfizer Global Virtual Library (PGVL).pdf}
}

@article{huReinforcementLearningMolecule2021,
  title = {Reinforcement {{Learning}} of {{Molecule Optimization}} with {{Bayesian Neural Networks}}},
  author = {Hu, Wei},
  date = {2021-12-24},
  journaltitle = {Computational Molecular Bioscience},
  volume = {11},
  number = {4},
  pages = {69--83},
  publisher = {Scientific Research Publishing},
  doi = {10.4236/cmb.2021.114005},
  url = {https://www.scirp.org/journal/paperinformation.aspx?paperid=114158},
  urldate = {2023-09-29},
  abstract = {Creating new molecules with desired properties is a fundamental and challenging problem in chemistry. Reinforcement learning (RL) has shown its utility in this area where the target chemical property values can serve as a reward signal. At each step of making a new molecule, the RL agent learns selecting an action from a list of many chemically valid actions for a given molecule, implying a great uncertainty associated with its learning. In a traditional implementation of deep RL algorithms, deterministic neural networks are typically employed, thus allowing the agent to choose one action from one sampled action at each step. In this paper, we proposed a new strategy of applying Bayesian neural networks to RL to reduce uncertainty so that the agent can choose one action from a pool of sampled actions at each step, and investigated its benefits in molecule design. Our experiments suggested the Bayesian approach could create molecules of desirable chemical quality while maintained their diversity, a very difficult goal to achieve in machine learning of molecules. We further exploited their diversity by using them to train a generative model to yield more novel drug-like molecules, which were absent in the training molecules as we know novelty is essential for drug candidate molecules. In conclusion, Bayesian approach could offer a balance between exploitation and exploration in RL, and a balance between optimization and diversity in molecule design.},
  issue = {4},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\HFKGNVXH\Hu - 2021 - Reinforcement Learning of Molecule Optimization wi.pdf}
}

@inproceedings{hwangYouKnowExisting2022,
  title = {"{{Do}} You Know Existing Accuracy Metrics Overrate Time-Series Anomaly Detections?"},
  shorttitle = {"{{Do}} You Know Existing Accuracy Metrics Overrate Time-Series Anomaly Detections?},
  booktitle = {Proceedings of the 37th {{ACM}}/{{SIGAPP Symposium}} on {{Applied Computing}}},
  author = {Hwang, Won-Seok and Yun, Jeong-Han and Kim, Jonguk and Min, Byung Gil},
  date = {2022-05-06},
  series = {{{SAC}} '22},
  pages = {403--412},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3477314.3507024},
  url = {https://doi.org/10.1145/3477314.3507024},
  urldate = {2023-03-14},
  abstract = {An anomaly coincides with a time range in time-series data, and a detection method usually detects part of this range. Existing works assume that an expert can detect the whole anomaly range by analyzing its detected part. Based on this expert scenario, a detection method achieves a higher score if the expert can find more anomalies through predictions given by the detection method as clues. However, the existing metrics overrate imprecise and insufficient cases. The expert cannot detect any anomalies if a prediction indicates a wrong range that is unrelated with any anomalies (i.e., called an imprecise case). Moreover, they fail to detect an anomaly if a prediction indicates an insufficient anomaly range (i.e., called an insufficient case). For instance, it is difficult to understand an anomaly using a small part of the range if the anomaly has steadily changed from its original pattern over a period of time. Moreover, the existing metrics do not consider the length of incorrect predictions in their score though a prolonged incorrect prediction incoveniences the expert more than a shorter one. We deal with these problems through two concepts of cross-referencing and a weighting scheme. The cross-referencing verifies anomalies and predictions involved in imprecise and insufficient cases, preventing them from getting scores. By adopting the weighting scheme, we consider the weighted sum of scores given to predictions, wherein lengthy incorrect predictions are penalized. Based on these concepts, we propose novel metrics (i.e., eTaV and eTaff). We verify that the proposed metrics are more reasonable compared to existing metrics via evaluations using two real-world datasets, Secure Water Treatment and Hardware-in-the-Loop-based Augmented Industrial control system as well as some hypothetical datasets. Furthermore, our metrics have been verified in practice because they are employed in a anomaly detection competition (i.e., HAICon'211).},
  isbn = {978-1-4503-8713-2},
  keywords = {accuracy metric,anomaly detection,precision,recall,time-series}
}

@article{ihlenfeldtComputerAssistedPlanningOrganic1996,
  title = {Computer-{{Assisted Planning}} of {{Organic Syntheses}}: {{The Second Generation}} of {{Programs}}},
  shorttitle = {Computer-{{Assisted Planning}} of {{Organic Syntheses}}},
  author = {Ihlenfeldt, Wolf-Dietrich and Gasteiger, Johann},
  date = {1996},
  journaltitle = {Angewandte Chemie International Edition in English},
  volume = {34},
  number = {23-24},
  pages = {2613--2633},
  issn = {1521-3773},
  doi = {10.1002/anie.199526131},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.199526131},
  urldate = {2019-11-06},
  abstract = {The planning of syntheses in organic chemistry has continuously been given more solid foundations during the last decades. Widely applicable rules have been formulated. Nearly parallel with the systematization of this field the potential for the automation of synthesis planning by the use of computers has been promulgated. However, after more than two decades of continuing efforts by a number of groups, computer-assisted synthesis planning, which relies on large libraries of synthons and transforms, has failed to establish itself firmly. This is in marked contrast to the acceptance of reaction databases. Their use has become routine with a surprisingly short time. Apparently the classical approaches to computer-assisted synthesis planning do not satisfy the needs of the preparative chemist. However, this lack of success does not yet mean the complete demise of the whole field. The conceptual shortcomings and problems of the first generation of programs are both of technical and psychological nature and need to be critically analyzed. Meanwhile, work has begun on systems of the second generation, which try to support the chemist in the synthesis laboratory with new methods in novel ways. Care is taken to mimic and support the typical planning style of the human chemist, who often on the spur of the moment switches direction in planning as well as his (sub) goals and methods. New tools that correspond better to the thinking patterns and working habits of chemists empower the user to plan syntheses of organic molecules in an interactive and innovative fashion. We feel that this field is far from dead and continue our research activities. In this article we introduce a system with a number of novel concepts and demonstrate its capabilities with some practical examples.},
  langid = {english},
  keywords = {computer chemistry,Computer chemistry,Synthesis design,synthesis planning,Synthetic methods,WODCA},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\EVGEA9PG\\Ihlenfeldt_Gasteiger_1996_Computer-Assisted Planning of Organic Syntheses.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZQ9WBMEW\\anie.html}
}

@article{ikebataBayesianMolecularDesign2017,
  title = {Bayesian Molecular Design with a Chemical Language Model},
  author = {Ikebata, Hisaki and Hongo, Kenta and Isomura, Tetsu and Maezono, Ryo and Yoshida, Ryo},
  date = {2017-04-01},
  journaltitle = {J Comput Aided Mol Des},
  volume = {31},
  number = {4},
  pages = {379--391},
  issn = {1573-4951},
  doi = {10.1007/s10822-016-0008-z},
  url = {https://doi.org/10.1007/s10822-016-0008-z},
  urldate = {2018-09-18},
  abstract = {The aim of computational molecular design is the identification of promising hypothetical molecules with a predefined set of desired properties. We address the issue of accelerating the material discovery with state-of-the-art machine learning techniques. The method involves two different types of prediction; the forward and backward predictions. The objective of the forward prediction is to create a set of machine learning models on various properties of a given molecule. Inverting the trained forward models through Bayes’ law, we derive a posterior distribution for the backward prediction, which is conditioned by a desired property requirement. Exploring high-probability regions of the posterior with a sequential Monte Carlo technique, molecules that exhibit the desired properties can computationally be created. One major difficulty in the computational creation of molecules is the exclusion of the occurrence of chemically unfavorable structures. To circumvent this issue, we derive a chemical language model that acquires commonly occurring patterns of chemical fragments through natural language processing of ASCII strings of existing compounds, which follow the SMILES chemical language notation. In the backward prediction, the trained language model is used to refine chemical strings such that the properties of the resulting structures fall within the desired property region while chemically unfavorable structures are successfully removed. The present method is demonstrated through the design of small organic molecules with the property requirements on HOMO-LUMO gap and internal energy. The R package iqspr is available at the CRAN repository.},
  langid = {english},
  keywords = {Bayesian analysis,Inverse-QSPR,Molecular design,Natural language processing,Small organic molecules,SMILES},
  file = {C:\Users\USEBPERP\Zotero\storage\CTSRPCH6\Ikebata et al_2017_Bayesian molecular design with a chemical language model.pdf}
}

@unpublished{ilyasAdversarialExamplesAre2019,
  title = {Adversarial {{Examples Are Not Bugs}}, {{They Are Features}}},
  author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  date = {2019-05-06},
  eprint = {1905.02175},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.02175},
  urldate = {2019-07-25},
  abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\HWFAPBW8\\Ilyas et al_2019_Adversarial Examples Are Not Bugs, They Are Features.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VGFUK9RD\\1905.html}
}

@inreference{IndependentSetGraph2020,
  title = {Independent Set (Graph Theory)},
  booktitle = {Wikipedia},
  date = {2020-12-03T18:35:30Z},
  url = {https://en.wikipedia.org/w/index.php?title=Independent_set_(graph_theory)&oldid=992139884},
  urldate = {2020-12-16},
  abstract = {In graph theory, an independent set, stable set, coclique or anticlique is a set of vertices in a graph, no two of which are adjacent. That is, it is a set                         S                 \{\textbackslash displaystyle S\}    of vertices such that for every two vertices in                         S                 \{\textbackslash displaystyle S\}   , there is no edge connecting the two. Equivalently, each edge in the graph has at most one endpoint in                         S                 \{\textbackslash displaystyle S\}   . The size of an independent set is the number of vertices it contains. Independent sets have also been called internally stable sets.A maximal independent set is an independent set that is not a proper subset of any other independent set. A maximum independent set is an independent set of largest possible size for a given graph                         G                 \{\textbackslash displaystyle G\}   . This size is called the independence number of                         G                 \{\textbackslash displaystyle G\}   , and denoted                         α         (         G         )                 \{\textbackslash displaystyle \textbackslash alpha (G)\}   . The optimization problem of finding such a set is called the maximum independent set problem. It is a strongly NP-hard. As such, it is unlikely that there exists an efficient algorithm for finding a maximum independent set of a graph. Every maximum independent set also is maximal, but the converse implication does not necessarily hold.},
  langid = {english},
  annotation = {Page Version ID: 992139884},
  file = {C:\Users\USEBPERP\Zotero\storage\B3BVIAYV\index.html}
}

@incollection{ingrahamGenerativeModelsGraphBased2019,
  title = {Generative {{Models}} for {{Graph-Based Protein Design}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Ingraham, John and Garg, Vikas and Barzilay, Regina and Jaakkola, Tommi},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Alché-Buc, given=F., prefix=d\textbackslash textquotesingle, useprefix=false and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {15794--15805},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/9711-generative-models-for-graph-based-protein-design.pdf},
  urldate = {2019-12-19},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\W6XRMXXN\\Ingraham et al_2019_Generative Models for Graph-Based Protein Design.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\UZTPHU3G\\9711-generative-models-for-graph-based-protein-design.html}
}

@article{IntroducingCommunicationsChemistry2018,
  title = {Introducing {{Communications Chemistry}}},
  date = {2018-03-08},
  journaltitle = {Communications Chemistry},
  volume = {1},
  number = {1},
  pages = {1--2},
  publisher = {Nature Publishing Group},
  issn = {2399-3669},
  doi = {10.1038/s42004-018-0011-5},
  url = {https://www.nature.com/articles/s42004-018-0011-5},
  urldate = {2020-04-21},
  abstract = {Today we launch Communications Chemistry, an open access, multidisciplinary chemistry journal from Nature Research publishing articles, reviews and commentary across the chemical sciences.},
  issue = {1},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BQQC25ZH\\2018_Introducing Communications Chemistry.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FBP2NGJU\\s42004-018-0011-5.html}
}

@article{ishidaPredictionInterpretableVisualization2019,
  title = {Prediction and {{Interpretable Visualization}} of {{Retrosynthetic Reactions Using Graph Convolutional Networks}}},
  author = {Ishida, Shoichi and Terayama, Kei and Kojima, Ryosuke and Takasu, Kiyosei and Okuno, Yasushi},
  date = {2019-12-23},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {59},
  number = {12},
  pages = {5026--5033},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.9b00538},
  url = {https://doi.org/10.1021/acs.jcim.9b00538},
  urldate = {2021-05-10},
  abstract = {Recently, many research groups have been addressing data-driven approaches for (retro)synthetic reaction prediction and retrosynthetic analysis. Although the performances of the data-driven approach have progressed because of recent advances of machine learning and deep learning techniques, problems such as improving capability of reaction prediction and the black-box problem of neural networks persist for practical use by chemists. To spread data-driven approaches to chemists, we focused on two challenges: improvement of retrosynthetic reaction prediction and interpretability of the prediction. In this paper, we propose an interpretable prediction framework using graph convolutional networks (GCN) for retrosynthetic reaction prediction and integrated gradients (IG) for visualization of contributions to the prediction to address these challenges. As a result, from the viewpoint of balanced accuracies, our model showed better performances than the approach using an extended-connectivity fingerprint. Furthermore, IG-based visualization of the GCN prediction successfully highlighted reaction-related atoms.},
  issue = {12}
}

@article{ishiguroDataTransferApproaches2020,
  title = {Data {{Transfer Approaches}} to {{Improve Seq-to-Seq Retrosynthesis}}},
  author = {Ishiguro, Katsuhiko and Ujihara, Kazuya and Sawada, R. and Akita, Hirotaka and Kotera, Masaaki},
  date = {2020},
  journaltitle = {ArXiv},
  abstract = {Retrosynthesis is a problem to infer reactant compounds to synthesize a given product compound through chemical reactions. Recent studies on retrosynthesis focus on proposing more sophisticated prediction models, but the dataset to feed the models also plays an essential role in achieving the best generalizing models. Generally, a dataset that is best suited for a specific task tends to be small. In such a case, it is the standard solution to transfer knowledge from a large or clean dataset in the same domain. In this paper, we conduct a systematic and intensive examination of data transfer approaches on end-to-end generative models, in application to retrosynthesis. Experimental results show that typical data transfer methods can improve test prediction scores of an off-the-shelf Transformer baseline model. Especially, the pre-training plus fine-tuning approach boosts the accuracy scores of the baseline, achieving the new state-of-the-art. In addition, we conduct a manual inspection for the erroneous prediction results. The inspection shows that the pre-training plus fine-tuning models can generate chemically appropriate or sensible proposals in almost all cases.}
}

@online{IWD2023UK,
  title = {{{IWD}} 2023 ({{UK}}) {{Day}} 3 {{PM Live Stream}} | {{Amazon Livestreaming}}},
  url = {https://livestreaming.corp.amazon.com/live/IWD(UK)_2023?eventId=44877},
  urldate = {2023-03-08},
  file = {C:\Users\USEBPERP\Zotero\storage\TGEW4NNR\IWD(UK)_2023.html}
}

@article{jacobsGasTurbineEngine2018,
  title = {Gas Turbine Engine Condition Monitoring Using {{Gaussian}} Mixture and Hidden {{Markov}} Models},
  author = {Jacobs, W. R. and Edwards, H. and Li, P. and Kadirkamanathan, V. and Mills, A. R.},
  date = {2018-08-16},
  journaltitle = {International Journal of Prognostics and Health Management},
  volume = {9},
  publisher = {{The Prognostics and Health Management Society}},
  issn = {2153-2648},
  url = {https://www.phmsociety.org/node/2455},
  urldate = {2023-01-25},
  abstract = {This paper investigates the problem of condition monitoring of complex dynamic systems, specifically the detection, localisation and quantification of transient faults. A data driven approach is developed for fault detection where the multidimensional data sequence is viewed as a stochastic process whose behaviour can be described by a hidden Markov model with two hidden states --- i.e. `healthy / nominal' and `unhealthy / faulty'. The fault detection is performed by first clustering in a multidimensional data space to define normal operating behaviour using a Gaussian-Uniform mixture model. The health status of the system at each data point is then determined by evaluating the posterior probabilities of the hidden states of a hidden Markov model. This allows the temporal relationship between sequential data points to be incorporated into the fault detection scheme. The proposed scheme is robust to noise and requires minimal tuning. A real-world case study is performed based on the detection of transient faults in the variable stator vane actuator of a gas turbine engine to demonstrate the successful application of the scheme. The results are used to demonstrate the generation of simple and easily interpretable analytics that can be used to monitor the evolution of the fault across time.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4HKDHPB4\\Jacobs et al. - 2018 - Gas turbine engine condition monitoring using Gaus.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HIZXF4UK\\135531.html}
}

@unpublished{jainAcceleratingStochasticGradient2018,
  title = {Accelerating {{Stochastic Gradient Descent For Least Squares Regression}}},
  author = {Jain, Prateek and Kakade, Sham M. and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  date = {2018-07-31},
  eprint = {1704.08227},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1704.08227},
  urldate = {2020-10-21},
  abstract = {There is widespread sentiment that it is not possible to effectively utilize fast gradient methods (e.g. Nesterov's acceleration, conjugate gradient, heavy ball) for the purposes of stochastic optimization due to their instability and error accumulation, a notion made precise in d'Aspremont 2008 and Devolder, Glineur, and Nesterov 2014. This work considers these issues for the special case of stochastic approximation for the least squares regression problem, and our main result refutes the conventional wisdom by showing that acceleration can be made robust to statistical errors. In particular, this work introduces an accelerated stochastic gradient method that provably achieves the minimax optimal statistical risk faster than stochastic gradient descent. Critical to the analysis is a sharp characterization of accelerated stochastic gradient descent as a stochastic process. We hope this characterization gives insights towards the broader question of designing simple and effective accelerated stochastic methods for more general convex and non-convex optimization problems.},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\SWBN3MT2\\Jain et al_2018_Accelerating Stochastic Gradient Descent For Least Squares Regression.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\5WLQZXVB\\1704.html}
}

@article{jainParallelizingStochasticGradient2018,
  title = {Parallelizing {{Stochastic Gradient Descent}} for {{Least Squares Regression}}: {{Mini-batching}}, {{Averaging}}, and {{Model Misspecification}}},
  shorttitle = {Parallelizing {{Stochastic Gradient Descent}} for {{Least Squares Regression}}},
  author = {Jain, Prateek and Kakade, Sham M. and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  date = {2018},
  journaltitle = {Journal of Machine Learning Research},
  volume = {18},
  number = {223},
  pages = {1--42},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v18/16-595.html},
  urldate = {2019-09-02},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\M4BLUZM8\\Jain et al. - 2018 - Parallelizing Stochastic Gradient Descent for Leas.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BLLDNM5N\\16-595.html}
}

@unpublished{jaiswalUnsupervisedAdversarialInvariance2018,
  title = {Unsupervised {{Adversarial Invariance}}},
  author = {Jaiswal, Ayush and Wu, Yue and AbdAlmageed, Wael and Natarajan, Premkumar},
  date = {2018-09-26},
  eprint = {1809.10083},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1809.10083},
  urldate = {2018-11-06},
  abstract = {Data representations that contain all the information about target variables but are invariant to nuisance factors benefit supervised learning algorithms by preventing them from learning associations between these factors and the targets, thus reducing overfitting. We present a novel unsupervised invariance induction framework for neural networks that learns a split representation of data through competitive training between the prediction task and a reconstruction task coupled with disentanglement, without needing any labeled information about nuisance factors or domain knowledge. We describe an adversarial instantiation of this framework and provide analysis of its working. Our unsupervised model outperforms state-of-the-art methods, which are supervised, at inducing invariance to inherent nuisance factors, effectively using synthetic data augmentation to learn invariance, and domain adaptation. Our method can be applied to any prediction task, eg., binary/multi-class classification or regression, without loss of generality.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\79868ZHU\\Jaiswal et al_2018_Unsupervised Adversarial Invariance.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AFCVUR3X\\1809.html}
}

@article{janetQuantitativeUncertaintyMetric2019,
  title = {A Quantitative Uncertainty Metric Controls Error in Neural Network-Driven Chemical Discovery},
  author = {Janet, Jon Paul and Duan, Chenru and Yang, Tzuhsiung and Nandy, Aditya and Kulik, Heather J.},
  date = {2019-07-11},
  journaltitle = {Chem. Sci.},
  issn = {2041-6539},
  doi = {10.1039/C9SC02298H},
  url = {https://pubs.rsc.org/en/content/articlelanding/2019/sc/c9sc02298h},
  urldate = {2019-08-14},
  abstract = {Machine learning (ML) models, such as artificial neural networks, have emerged as a complement to high-throughput screening, enabling characterization of new compounds in seconds instead of hours. The promise of ML models to enable large-scale chemical space exploration can only be realized if it is straightforward to identify when molecules and materials are outside the model's domain of applicability. Established uncertainty metrics for neural network models are either costly to obtain (e.g., ensemble models) or rely on feature engineering (e.g., feature space distances), and each has limitations in estimating prediction errors for chemical space exploration. We introduce the distance to available data in the latent space of a neural network ML model as a low-cost, quantitative uncertainty metric that works for both inorganic and organic chemistry. The calibrated performance of this approach exceeds widely used uncertainty metrics and is readily applied to models of increasing complexity at no additional cost. Tightening latent distance cutoffs systematically drives down predicted model errors below training errors, thus enabling predictive error control in chemical discovery or identification of useful data points for active learning.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ZF7P8ZT5\\Janet et al_2019_A quantitative uncertainty metric controls error in neural network-driven.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WJ5EXGQE\\c9sc02298h.html}
}

@unpublished{janzLearningGenerativeModel2017,
  title = {Learning a {{Generative Model}} for {{Validity}} in {{Complex Discrete Structures}}},
  author = {Janz, David and family=Westhuizen, given=Jos, prefix=van der, useprefix=true and Paige, Brooks and Kusner, Matt J. and Hernandez-Labato, Jose Miguel},
  date = {2017-12-05},
  eprint = {1712.01664},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1712.01664},
  urldate = {2018-09-19},
  abstract = {Deep generative models have been successfully used to learn representations for high-dimensional discrete spaces by representing discrete objects as sequences and employing powerful sequence-based deep models. Unfortunately, these sequence-based models often produce invalid sequences: sequences which do not represent any underlying discrete structure; invalid sequences hinder the utility of such models. As a step towards solving this problem, we propose to learn a deep recurrent validator model, which can estimate whether a partial sequence can function as the beginning of a full, valid sequence. This validator provides insight as to how individual sequence elements influence the validity of the overall sequence, and can be used to constrain sequence based models to generate valid sequences -- and thus faithfully model discrete objects. Our approach is inspired by reinforcement learning, where an oracle which can evaluate validity of complete sequences provides a sparse reward signal. We demonstrate its effectiveness as a generative model of Python 3 source code for mathematical expressions, and in improving the ability of a variational autoencoder trained on SMILES strings to decode valid molecular structures.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\YNH87EFD\\Janz et al_2017_Learning a Generative Model for Validity in Complex Discrete Structures.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QFTMQ58Q\\1712.html}
}

@unpublished{jaquesSequenceTutorConservative2016,
  title = {Sequence {{Tutor}}: {{Conservative Fine-Tuning}} of {{Sequence Generation Models}} with {{KL-control}}},
  shorttitle = {Sequence {{Tutor}}},
  author = {Jaques, Natasha and Gu, Shixiang and Bahdanau, Dzmitry and Hernández-Lobato, José Miguel and Turner, Richard E. and Eck, Douglas},
  date = {2016-11-08},
  eprint = {1611.02796},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.02796},
  urldate = {2018-09-18},
  abstract = {This paper proposes a general method for improving the structure and quality of sequences generated by a recurrent neural network (RNN), while maintaining information originally learned from data, as well as sample diversity. An RNN is first pre-trained on data using maximum likelihood estimation (MLE), and the probability distribution over the next token in the sequence learned by this model is treated as a prior policy. Another RNN is then trained using reinforcement learning (RL) to generate higher-quality outputs that account for domain-specific incentives while retaining proximity to the prior policy of the MLE RNN. To formalize this objective, we derive novel off-policy RL methods for RNNs from KL-control. The effectiveness of the approach is demonstrated on two applications; 1) generating novel musical melodies, and 2) computational molecular generation. For both problems, we show that the proposed method improves the desired properties and structure of the generated sequences, while maintaining information learned from data.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\EWN2ULM3\\Jaques et al_2016_Sequence Tutor2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LDUCWTCM\\Jaques et al_2016_Sequence Tutor.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2QU7UPGM\\1611.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\7PVSFKSC\\1611.html}
}

@article{jasialActivityrelevantSimilarityValues2016,
  title = {Activity-Relevant Similarity Values for Fingerprints and Implications for Similarity Searching},
  author = {Jasial, Swarit and Hu, Ye and Vogt, Martin and Bajorath, Jürgen},
  date = {2016-04-28},
  journaltitle = {F1000Res},
  volume = {5},
  eprint = {27127620},
  eprinttype = {pmid},
  pages = {Chem Inf Sci-591},
  issn = {2046-1402},
  doi = {10.12688/f1000research.8357.2},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4830209/},
  urldate = {2023-09-04},
  abstract = {A largely unsolved problem in chemoinformatics is the issue of how calculated compound similarity relates to activity similarity, which is central to many applications. In general, activity relationships are predicted from calculated similarity values. However, there is no solid scientific foundation to bridge between calculated molecular and observed activity similarity. Accordingly, the success rate of identifying new active compounds by similarity searching is limited. Although various attempts have been made to establish relationships between calculated fingerprint similarity values and biological activities, none of these has yielded generally applicable rules for similarity searching. In this study, we have addressed the question of molecular versus activity similarity in a more fundamental way. First, we have evaluated if activity-relevant similarity value ranges could in principle be identified for standard fingerprints and distinguished from similarity resulting from random compound comparisons. Then, we have analyzed if activity-relevant similarity values could be used to guide typical similarity search calculations aiming to identify active compounds in databases. It was found that activity-relevant similarity values can be identified as a characteristic feature of fingerprints. However, it was also shown that such values cannot be reliably used as thresholds for practical similarity search calculations. In addition, the analysis presented herein helped to rationalize differences in fingerprint search performance.},
  pmcid = {PMC4830209},
  file = {C:\Users\USEBPERP\Zotero\storage\V3GS5SRG\Jasial et al. - 2016 - Activity-relevant similarity values for fingerprin.pdf}
}

@article{jaworskiAutomaticMappingAtoms2019,
  title = {Automatic Mapping of Atoms across Both Simple and Complex Chemical Reactions},
  author = {Jaworski, Wojciech and Szymkuć, Sara and Mikulak-Klucznik, Barbara and Piecuch, Krzysztof and Klucznik, Tomasz and Kaźmierowski, Michał and Rydzewski, Jan and Gambin, Anna and Grzybowski, Bartosz A.},
  date = {2019-03-29},
  journaltitle = {Nat Commun},
  volume = {10},
  number = {1},
  pages = {1--11},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-09440-2},
  url = {https://www.nature.com/articles/s41467-019-09440-2},
  urldate = {2019-09-30},
  abstract = {Mapping atoms across chemical reactions represents a challenging computational task. Here the authors show via a combination of graph theory and combinatorics with expert chemical knowledge the possibility to map very complex organic reactions.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6JULLFHH\\Jaworski et al_2019_Automatic mapping of atoms across both simple and complex chemical reactions.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LWCWAFBF\\Jaworski et al_2019_Automatic mapping of atoms across both simple and complex chemical reactions2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\57MJ2MNU\\s41467-019-09440-2.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\JWQ7IMIB\\s41467-019-09440-2.html}
}

@article{jensenAutonomousDiscoveryChemical,
  title = {Autonomous Discovery in the Chemical Sciences Part {{I}}: {{Progress}}},
  shorttitle = {Autonomous Discovery in the Chemical Sciences Part {{I}}},
  author = {Jensen, Klavs F. and Coley, Connor W. and Eyke, Natalie S.},
  journaltitle = {Angewandte Chemie International Edition},
  volume = {n/a},
  number = {n/a},
  issn = {1521-3773},
  doi = {10.1002/anie.201909987},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.201909987},
  urldate = {2020-05-29},
  abstract = {This two-part review examines how automation has contributed to different aspects of discovery in the chemical sciences. In this first part, we describe a classification for discoveries of physical matter (molecules, materials, devices), processes, and models and how they are unified as search problems. We then introduce a set of questions and considerations relevant to assessing the extent of autonomy. Finally, we describe many case studies of discoveries accelerated by or resulting from computer assistance and automation from the domains of synthetic chemistry, drug discovery, inorganic chemistry, and materials science. These illustrate how rapid advancements in hardware automation and machine learning continue to transform the nature of experimentation and modelling. Part two reflects on these case studies and identifies a set of open challenges for the field.},
  langid = {english},
  keywords = {automation,Autonomous,chemoinformatics,data analysis,Discovery,drug discovery,machine learning,materials science},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BAJD8G2S\\Coley et al. - 2020 - Autonomous Discovery in the Chemical Sciences Part.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XDJSPDS3\\Jensen et al_Autonomous discovery in the chemical sciences part I.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QW8XZUIG\\anie.html}
}

@article{jensenGraphbasedGeneticAlgorithm2019,
  title = {A Graph-Based Genetic Algorithm and Generative Model/{{Monte Carlo}} Tree Search for the Exploration of Chemical Space},
  author = {Jensen, Jan H.},
  date = {2019-03-20},
  journaltitle = {Chem. Sci.},
  volume = {10},
  number = {12},
  pages = {3567--3572},
  publisher = {The Royal Society of Chemistry},
  issn = {2041-6539},
  doi = {10.1039/C8SC05372C},
  url = {https://pubs.rsc.org/en/content/articlelanding/2019/sc/c8sc05372c},
  urldate = {2020-03-17},
  abstract = {This paper presents a comparison of a graph-based genetic algorithm (GB-GA) and machine learning (ML) results for the optimization of log P values with a constraint for synthetic accessibility and shows that the GA is as good as or better than the ML approaches for this particular property. The molecules found by the GB-GA bear little resemblance to the molecules used to construct the initial mating pool, indicating that the GB-GA approach can traverse a relatively large distance in chemical space using relatively few (50) generations. The paper also introduces a new non-ML graph-based generative model (GB-GM) that can be parameterized using very small data sets and combined with a Monte Carlo tree search (MCTS) algorithm. The results are comparable to previously published results (Sci. Technol. Adv. Mater., 2017, 18, 972–976) using a recurrent neural network (RNN) generative model, and the GB-GM-based method is several orders of magnitude faster. The MCTS results seem more dependent on the composition of the training set than the GA approach for this particular property. Our results suggest that the performance of new ML-based generative models should be compared to that of more traditional, and often simpler, approaches such a GA.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LBLPA5ZE\\Jensen_2019_A graph-based genetic algorithm and generative model-Monte Carlo tree search.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HZQTCL7Q\\c8sc05372c.html}
}

@article{jetleyFriendsTheseWho2018,
  title = {With {{Friends Like These}}, {{Who Needs Adversaries}}?},
  author = {Jetley, Saumya and Lord, Nicholas A. and Torr, Philip H. S.},
  date = {2018-07-11},
  url = {https://arxiv.org/abs/1807.04200},
  urldate = {2018-11-06},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LEMIN9B9\\Jetley et al_2018_With Friends Like These, Who Needs Adversaries.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LBSEIR4F\\1807.html}
}

@unpublished{jiaTransferLearningSpeaker2019,
  title = {Transfer {{Learning}} from {{Speaker Verification}} to {{Multispeaker Text-To-Speech Synthesis}}},
  author = {Jia, Ye and Zhang, Yu and Weiss, Ron J. and Wang, Quan and Shen, Jonathan and Ren, Fei and Chen, Zhifeng and Nguyen, Patrick and Pang, Ruoming and Moreno, Ignacio Lopez and Wu, Yonghui},
  date = {2019-01-02},
  eprint = {1806.04558},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1806.04558},
  urldate = {2019-11-13},
  abstract = {We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of many different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech from thousands of speakers without transcripts, to generate a fixed-dimensional embedding vector from seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2, which generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder that converts the mel spectrogram into a sequence of time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the new task, and is able to synthesize natural speech from speakers that were not seen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NTU4LZ2D\\Jia et al_2019_Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JKEYA4CB\\1806.html}
}

@unpublished{jinJunctionTreeVariational2018,
  title = {Junction {{Tree Variational Autoencoder}} for {{Molecular Graph Generation}}},
  author = {Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  date = {2018-02-12},
  eprint = {1802.04364},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.04364},
  urldate = {2019-05-06},
  abstract = {We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6C7K8MUX\\Jin et al_2018_Junction Tree Variational Autoencoder for Molecular Graph Generation3.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GQKA5FBM\\Jin et al_2018_Junction Tree Variational Autoencoder for Molecular Graph Generation.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\Z9NDXTJB\\Jin et al_2018_Junction Tree Variational Autoencoder for Molecular Graph Generation2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6Q9BVNVY\\1802.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\9MATFIMN\\1802.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\MKMDH2ZH\\1802.html}
}

@article{jinLearningMultimodalGraphtoGraph2018,
  title = {Learning {{Multimodal Graph-to-Graph Translation}} for {{Molecule Optimization}}},
  author = {Jin, Wengong and Yang, Kevin and Barzilay, Regina and Jaakkola, Tommi},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=B1xJAsA5F7},
  urldate = {2019-04-24},
  abstract = {We view molecule optimization as a graph-to-graph translation problem. The goal is to learn to map from one molecular graph to another with better properties based on an available corpus of paired...},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BP3XAZN6\\Jin et al_2018_Learning Multimodal Graph-to-Graph Translation for Molecule Optimization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\V6KMJIS6\\forum.html}
}

@online{jinMultiObjectiveMoleculeGeneration2020,
  title = {Multi-{{Objective Molecule Generation}} Using {{Interpretable Substructures}}},
  author = {Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  date = {2020-07-02},
  eprint = {2002.03244},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.03244},
  urldate = {2023-07-19},
  abstract = {Drug discovery aims to find novel compounds with specified chemical property profiles. In terms of generative modeling, the goal is to learn to sample molecules in the intersection of multiple property constraints. This task becomes increasingly challenging when there are many property constraints. We propose to offset this complexity by composing molecules from a vocabulary of substructures that we call molecular rationales. These rationales are identified from molecules as substructures that are likely responsible for each property of interest. We then learn to expand rationales into a full molecule using graph generative models. Our final generative model composes molecules as mixtures of multiple rationale completions, and this mixture is fine-tuned to preserve the properties of interest. We evaluate our model on various drug design tasks and demonstrate significant improvements over state-of-the-art baselines in terms of accuracy, diversity, and novelty of generated compounds.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\WLETD8VZ\\Jin et al. - 2020 - Multi-Objective Molecule Generation using Interpre.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TSP8VZCR\\2002.html}
}

@online{JohnMaynardKeynes,
  title = {John {{Maynard Keynes}}: {{Newton}}, the {{Man}}},
  shorttitle = {John {{Maynard Keynes}}},
  url = {https://mathshistory.st-andrews.ac.uk/Extras/Keynes_Newton/},
  urldate = {2020-07-06},
  abstract = {John Maynard Keynes: Newton, the Man},
  langid = {english},
  organization = {Maths History}
}

@incollection{johnsonPerceptualLossesRealTime2016,
  title = {Perceptual {{Losses}} for {{Real-Time Style Transfer}} and {{Super-Resolution}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2016},
  author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  date = {2016},
  volume = {9906},
  pages = {694--711},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-46475-6_43},
  url = {http://link.springer.com/10.1007/978-3-319-46475-6_43},
  urldate = {2020-06-22},
  abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a per-pixel loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing perceptual loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al. in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
  isbn = {978-3-319-46474-9 978-3-319-46475-6},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\33KXR7ZS\Johnson et al. - 2016 - Perceptual Losses for Real-Time Style Transfer and.pdf}
}

@incollection{jonasDeepImitationLearning2019,
  title = {Deep Imitation Learning for Molecular Inverse Problems},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Jonas, Eric},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Alché-Buc, given=F., prefix=d\textbackslash textquotesingle, useprefix=false and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {4991--5001},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/8744-deep-imitation-learning-for-molecular-inverse-problems.pdf},
  urldate = {2019-12-19},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\RYA46WAS\\Jonas_2019_Deep imitation learning for molecular inverse problems.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8NMVCFGJ\\8744-deep-imitation-learning-for-molecular-inverse-problems.html}
}

@article{jorgensenDeepGenerativeModels2018,
  title = {Deep {{Generative Models}} for {{Molecular Science}}},
  author = {Jørgensen, Peter B. and Schmidt, Mikkel N. and Winther, Ole},
  date = {2018-01-01},
  journaltitle = {Molecular Informatics},
  volume = {37},
  number = {1-2},
  pages = {1700133},
  issn = {1868-1751},
  doi = {10.1002/minf.201700133},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/minf.201700133},
  urldate = {2018-09-18},
  abstract = {Generative deep machine learning models now rival traditional quantum-mechanical computations in predicting properties of new structures, and they come with a significantly lower computational cost, opening new avenues in computational molecular science. In the last few years, a variety of deep generative models have been proposed for modeling molecules, which differ in both their model structure and choice of input features. We review these recent advances within deep generative models for predicting molecular properties, with particular focus on models based on the probabilistic autoencoder (or variational autoencoder, VAE) approach in which the molecular structure is embedded in a latent vector space from which its properties can be predicted and its structure can be restored.},
  langid = {english},
  keywords = {deep learning,generative modeling,molecular science,variational auto-encoders,variational inference},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\486CB9S7\\Jørgensen et al_2018_Deep Generative Models for Molecular Science.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QHIMC6IS\\minf.html}
}

@inproceedings{jozefowiczEmpiricalExplorationRecurrent2015,
  title = {An {{Empirical Exploration}} of {{Recurrent Network Architectures}}},
  booktitle = {Proceedings of the {{32Nd International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  date = {2015},
  series = {{{ICML}}'15},
  pages = {2342--2350},
  publisher = {JMLR.org},
  location = {Lille, France},
  url = {http://dl.acm.org/citation.cfm?id=3045118.3045367},
  urldate = {2018-07-02},
  abstract = {The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.}
}

@article{kadurinCornucopiaMeaningfulLeads2016,
  title = {The Cornucopia of Meaningful Leads: {{Applying}} Deep Adversarial Autoencoders for New Molecule Development in Oncology},
  shorttitle = {The Cornucopia of Meaningful Leads},
  author = {Kadurin, Artur and Aliper, Alexander and Kazennov, Andrey and Mamoshina, Polina and Vanhaelen, Quentin and Khrabrov, Kuzma and Zhavoronkov, Alex},
  date = {2016-12-22},
  journaltitle = {Oncotarget},
  volume = {8},
  number = {7},
  eprint = {28029644},
  eprinttype = {pmid},
  pages = {10883--10890},
  issn = {1949-2553},
  doi = {10.18632/oncotarget.14073},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5355231/},
  urldate = {2018-09-18},
  abstract = {Recent advances in deep learning and specifically in generative adversarial networks have demonstrated surprising results in generating new images and videos upon request even using natural language as input. In this paper we present the first application of generative adversarial autoencoders (AAE) for generating novel molecular fingerprints with a defined set of parameters. We developed a 7-layer AAE architecture with the latent middle layer serving as a discriminator. As an input and output the AAE uses a vector of binary fingerprints and concentration of the molecule. In the latent layer we also introduced a neuron responsible for growth inhibition percentage, which when negative indicates the reduction in the number of tumor cells after the treatment. To train the AAE we used the NCI-60 cell line assay data for 6252 compounds profiled on MCF-7 cell line. The output of the AAE was used to screen 72 million compounds in PubChem and select candidate molecules with potential anti-cancer properties. This approach is a proof of concept of an artificially-intelligent drug discovery engine, where AAEs are used to generate new molecular fingerprints with the desired molecular properties.},
  pmcid = {PMC5355231},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7Z9QPSUP\\Kadurin et al_2016_The cornucopia of meaningful leads.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\KCL35C2T\\Kadurin et al_2016_The cornucopia of meaningful leads3.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QC28LWQZ\\Kadurin et al_2016_The cornucopia of meaningful leads2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JQBCN7NV\\index.html}
}

@article{kadurinDruGANAdvancedGenerative2017,
  title = {{{druGAN}}: {{An Advanced Generative Adversarial Autoencoder Model}} for de {{Novo Generation}} of {{New Molecules}} with {{Desired Molecular Properties}} in {{Silico}}},
  shorttitle = {{{druGAN}}},
  author = {Kadurin, Artur and Nikolenko, Sergey and Khrabrov, Kuzma and Aliper, Alex and Zhavoronkov, Alex},
  date = {2017-09-05},
  journaltitle = {Mol. Pharmaceutics},
  volume = {14},
  number = {9},
  pages = {3098--3104},
  issn = {1543-8384},
  doi = {10.1021/acs.molpharmaceut.7b00346},
  url = {https://doi.org/10.1021/acs.molpharmaceut.7b00346},
  urldate = {2019-05-06},
  abstract = {Deep generative adversarial networks (GANs) are the emerging technology in drug discovery and biomarker development. In our recent work, we demonstrated a proof-of-concept of implementing deep generative adversarial autoencoder (AAE) to identify new molecular fingerprints with predefined anticancer properties. Another popular generative model is the variational autoencoder (VAE), which is based on deep neural architectures. In this work, we developed an advanced AAE model for molecular feature extraction problems, and demonstrated its advantages compared to VAE in terms of (a) adjustability in generating molecular fingerprints; (b) capacity of processing very large molecular data sets; and (c) efficiency in unsupervised pretraining for regression model. Our results suggest that the proposed AAE model significantly enhances the capacity and efficiency of development of the new molecules with specific anticancer properties using the deep generative models.},
  keywords = {adversarial autoencoder,Artificial Intelligence,Computer Simulation,Concept Formation,deep learning,drug discovery,generative adversarial network,Learning,Models Theoretical,Neural Networks (Computer),Neural Networks Computer,variational autoencoder},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\3GLG6KXJ\\Kadurin et al_2017_druGAN.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\V7M4J56Z\\Kadurin et al_2017_druGAN2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\Z75WCQNJ\\acs.molpharmaceut.html}
}

@incollection{kahnemanProspectTheoryAnalysis2013,
  title = {Prospect Theory: {{An}} Analysis of Decision under Risk},
  shorttitle = {Prospect Theory},
  booktitle = {Handbook of the Fundamentals of Financial Decision Making: {{Part I}}},
  author = {Kahneman, Daniel and Tversky, Amos},
  date = {2013},
  pages = {99--127},
  publisher = {World Scientific},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\S9KMIJ6H\\Kahneman_Tversky_2013_Prospect theory.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6MVACA3G\\9789814417358_0006.html}
}

@unpublished{kajinoMolecularHypergraphGrammar2018,
  title = {Molecular {{Hypergraph Grammar}} with Its {{Application}} to {{Molecular Optimization}}},
  author = {Kajino, Hiroshi},
  date = {2018-09-07},
  eprint = {1809.02745},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1809.02745},
  urldate = {2019-05-06},
  abstract = {Molecular optimization aims to discover novel molecules with desirable properties. Two fundamental challenges are: (i) it is not trivial to generate valid molecules in a controllable way due to hard chemical constraints such as the valency conditions, and (ii) it is often costly to evaluate a property of a novel molecule, and therefore, the number of property evaluations is limited. These challenges are to some extent alleviated by a combination of a variational autoencoder (VAE) and Bayesian optimization (BO). VAE converts a molecule into/from its latent continuous vector, and BO optimizes a latent continuous vector (and its corresponding molecule) within a limited number of property evaluations. While the most recent work, for the first time, achieved 100\% validity, its architecture is rather complex due to auxiliary neural networks other than VAE, making it difficult to train. This paper presents a molecular hypergraph grammar variational autoencoder (MHG-VAE), which uses a single VAE to achieve 100\% validity. Our idea is to develop a graph grammar encoding the hard chemical constraints, called molecular hypergraph grammar (MHG), which guides VAE to always generate valid molecules. We also present an algorithm to construct MHG from a set of molecules.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BKKB9BMX\\Kajino_2018_Molecular Hypergraph Grammar with its Application to Molecular Optimization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\MF66C4BD\\1809.html}
}

@unpublished{kalchbrennerNeuralMachineTranslation2016,
  title = {Neural {{Machine Translation}} in {{Linear Time}}},
  author = {Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and family=Oord, given=Aaron, prefix=van den, useprefix=false and Graves, Alex and Kavukcuoglu, Koray},
  date = {2016-10-31},
  eprint = {1610.10099},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1610.10099},
  urldate = {2019-10-21},
  abstract = {We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efficient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive field. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We find that the latent alignment structure contained in the representations reflects the expected alignment between the tokens.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LYMMC6QB\\Kalchbrenner et al_2016_Neural Machine Translation in Linear Time.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TRTLZR5E\\1610.html}
}

@unpublished{kalchbrennerVideoPixelNetworks2016,
  title = {Video {{Pixel Networks}}},
  author = {Kalchbrenner, Nal and family=Oord, given=Aaron, prefix=van den, useprefix=false and Simonyan, Karen and Danihelka, Ivo and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
  date = {2016-10-03},
  eprint = {1610.00527},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1610.00527},
  urldate = {2019-10-21},
  abstract = {We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel values in a video. The model and the neural architecture reflect the time, space and color structure of video tensors and encode it as a four-dimensional dependency chain. The VPN approaches the best possible performance on the Moving MNIST benchmark, a leap over the previous state of the art, and the generated videos show only minor deviations from the ground truth. The VPN also produces detailed samples on the action-conditional Robotic Pushing benchmark and generalizes to the motion of novel objects.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\B8H5H7C3\\Kalchbrenner et al_2016_Video Pixel Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XMNFGBGP\\1610.html}
}

@article{kangConditionalMolecularDesign2019,
  title = {Conditional {{Molecular Design}} with {{Deep Generative Models}}},
  author = {Kang, Seokho and Cho, Kyunghyun},
  date = {2019-01-28},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {59},
  number = {1},
  pages = {43--52},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.8b00263},
  url = {https://doi.org/10.1021/acs.jcim.8b00263},
  urldate = {2019-05-06},
  abstract = {Although machine learning has been successfully used to propose novel molecules that satisfy desired properties, it is still challenging to explore a large chemical space efficiently. In this paper, we present a conditional molecular design method that facilitates generating new molecules with desired properties. The proposed model, which simultaneously performs both property prediction and molecule generation, is built as a semisupervised variational autoencoder trained on a set of existing molecules with only a partial annotation. We generate new molecules with desired properties by sampling from the generative distribution estimated by the model. We demonstrate the effectiveness of the proposed model by evaluating it on drug-like molecules. The model improves the performance of property prediction by exploiting unlabeled molecules and efficiently generates novel molecules fulfilling various target conditions.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\IESYPLJD\\Kang_Cho_2019_Conditional Molecular Design with Deep Generative Models3.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LYMKQXFR\\Kang_Cho_2019_Conditional Molecular Design with Deep Generative Models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XNJGPYEB\\Kang_Cho_2019_Conditional Molecular Design with Deep Generative Models2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FS4XWRGH\\acs.jcim.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\JB2ICRBL\\acs.jcim.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\XXID6LJU\\acs.jcim.html}
}

@unpublished{karampatsisMaybeDeepNeural2019,
  title = {Maybe {{Deep Neural Networks}} Are the {{Best Choice}} for {{Modeling Source Code}}},
  author = {Karampatsis, Rafael-Michael and Sutton, Charles},
  date = {2019-03-13},
  eprint = {1903.05734},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1903.05734},
  urldate = {2019-03-15},
  abstract = {Statistical language modeling techniques have successfully been applied to source code, yielding a variety of new software development tools, such as tools for code suggestion and improving readability. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. But traditional language models limit the vocabulary to a fixed set of common words. For code, this strong assumption has been shown to have a significant negative effect on predictive performance. But the open vocabulary version of the neural network language models for code have not been introduced in the literature. We present a new open-vocabulary neural language model for code that is not limited to a fixed vocabulary of identifier names. We employ a segmentation into subword units, subsequences of tokens chosen based on a compression criterion, following previous work in machine translation. Our network achieves best in class performance, outperforming even the state-of-the-art methods of Hellendoorn and Devanbu that are designed specifically to model code. Furthermore, we present a simple method for dynamically adapting the model to a new test project, resulting in increased performance. We showcase our methodology on code corpora in three different languages of over a billion tokens each, hundreds of times larger than in previous work. To our knowledge, this is the largest neural language model for code that has been reported.},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2S3X3ZNF\\Karampatsis_Sutton_2019_Maybe Deep Neural Networks are the Best Choice for Modeling Source Code.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\Q49CMNWM\\1903.html}
}

@inproceedings{karpovTransformerModelRetrosynthesis2019,
  title = {A {{Transformer Model}} for {{Retrosynthesis}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} – {{ICANN}} 2019: {{Workshop}} and {{Special Sessions}}},
  author = {Karpov, Pavel and Godin, Guillaume and Tetko, Igor V.},
  editor = {Tetko, Igor V. and Kůrková, Věra and Karpov, Pavel and Theis, Fabian},
  date = {2019},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {817--830},
  publisher = {Springer International Publishing},
  abstract = {We describe a Transformer model for a retrosynthetic reaction prediction task. The model is trained on 45 033 experimental reaction examples extracted from USA patents. It can successfully predict the reactants set for 42.7\% of cases on the external test set. During the training procedure, we applied different learning rate schedules and snapshot learning. These techniques can prevent overfitting and thus can be a reason to get rid of internal validation dataset that is advantageous for deep models with millions of parameters. We thoroughly investigated different approaches to train Transformer models and found that snapshot learning with averaging weights on learning rates minima works best. While decoding the model output probabilities there is a strong influence of the temperature that improves at T=1.3T=1.3\textbackslash text \{T\}=1.3 the accuracy of models up to 1–2\%.},
  isbn = {978-3-030-30493-5},
  langid = {english},
  keywords = {Character-based models,Computer aided synthesis planning,Retrosynthesis prediction,Transformer},
  file = {C:\Users\USEBPERP\Zotero\storage\Y8AMXJ8M\Karpov et al_2019_A Transformer Model for Retrosynthesis.pdf}
}

@article{kayalaReactionPredictorPredictionComplex2012,
  title = {{{ReactionPredictor}}: {{Prediction}} of {{Complex Chemical Reactions}} at the {{Mechanistic Level Using Machine Learning}}},
  shorttitle = {{{ReactionPredictor}}},
  author = {Kayala, Matthew A. and Baldi, Pierre},
  date = {2012-10-22},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {52},
  number = {10},
  pages = {2526--2540},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/ci3003039},
  url = {https://doi.org/10.1021/ci3003039},
  urldate = {2021-04-08},
  abstract = {Proposing reasonable mechanisms and predicting the course of chemical reactions is important to the practice of organic chemistry. Approaches to reaction prediction have historically used obfuscating representations and manually encoded patterns or rules. Here we present ReactionPredictor, a machine learning approach to reaction prediction that models elementary, mechanistic reactions as interactions between approximate molecular orbitals (MOs). A training data set of productive reactions known to occur at reasonable rates and yields and verified by inclusion in the literature or textbooks is derived from an existing rule-based system and expanded upon with manual curation from graduate level textbooks. Using this training data set of complex polar, hypervalent, radical, and pericyclic reactions, a two-stage machine learning prediction framework is trained and validated. In the first stage, filtering models trained at the level of individual MOs are used to reduce the space of possible reactions to consider. In the second stage, ranking models over the filtered space of possible reactions are used to order the reactions such that the productive reactions are the top ranked. The resulting model, ReactionPredictor, perfectly ranks polar reactions 78.1\% of the time and recovers all productive reactions 95.7\% of the time when allowing for small numbers of errors. Pericyclic and radical reactions are perfectly ranked 85.8\% and 77.0\% of the time, respectively, rising to {$>$}93\% recovery for both reaction types with a small number of allowed errors. Decisions about which of the polar, pericyclic, or radical reaction type ranking models to use can be made with {$>$}99\% accuracy. Finally, for multistep reaction pathways, we implement the first mechanistic pathway predictor using constrained tree-search to discover a set of reasonable mechanistic steps from given reactants to given products. Webserver implementations of both the single step and pathway versions of ReactionPredictor are available via the chemoinformatics portal http://cdb.ics.uci.edu/.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7K3RNLXS\\Kayala and Baldi - 2012 - ReactionPredictor Prediction of Complex Chemical .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SHNEM8D2\\ci3003039.html}
}

@article{kearnesMolecularGraphConvolutions2016,
  title = {Molecular {{Graph Convolutions}}: {{Moving Beyond Fingerprints}}},
  shorttitle = {Molecular {{Graph Convolutions}}},
  author = {Kearnes, Steven and McCloskey, Kevin and Berndl, Marc and Pande, Vijay and Riley, Patrick},
  date = {2016-08},
  journaltitle = {Journal of Computer-Aided Molecular Design},
  volume = {30},
  number = {8},
  eprint = {1603.00856},
  eprinttype = {arxiv},
  pages = {595--608},
  issn = {0920-654X, 1573-4951},
  doi = {10.1007/s10822-016-9938-8},
  url = {http://arxiv.org/abs/1603.00856},
  urldate = {2018-09-17},
  abstract = {Molecular "fingerprints" encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make data-driven decisions. We describe molecular "graph convolutions", a machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph---atoms, bonds, distances, etc.---which allows the model to take greater advantage of information in the graph structure. Although graph convolutions do not outperform all fingerprint-based methods, they (along with other graph-based methods) represent a new paradigm in ligand-based virtual screening with exciting opportunities for future improvement.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JNURQFF7\\Kearnes et al_2016_Molecular Graph Convolutions.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HGVZ2WNG\\1603.html}
}

@unpublished{kendallBayesianSegNetModel2015,
  title = {Bayesian {{SegNet}}: {{Model Uncertainty}} in {{Deep Convolutional Encoder-Decoder Architectures}} for {{Scene Understanding}}},
  shorttitle = {Bayesian {{SegNet}}},
  author = {Kendall, Alex and Badrinarayanan, Vijay and Cipolla, Roberto},
  date = {2015-11-09},
  eprint = {1511.02680},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1511.02680},
  urldate = {2019-04-15},
  abstract = {We present a deep learning framework for probabilistic pixel-wise semantic segmentation, which we term Bayesian SegNet. Semantic segmentation is an important tool for visual scene understanding and a meaningful measure of uncertainty is essential for decision making. Our contribution is a practical system which is able to predict pixel-wise class labels with a measure of model uncertainty. We achieve this by Monte Carlo sampling with dropout at test time to generate a posterior distribution of pixel class labels. In addition, we show that modelling uncertainty improves segmentation performance by 2-3\% across a number of state of the art architectures such as SegNet, FCN and Dilation Network, with no additional parametrisation. We also observe a significant improvement in performance for smaller datasets where modelling uncertainty is more effective. We benchmark Bayesian SegNet on the indoor SUN Scene Understanding and outdoor CamVid driving scenes datasets.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\J358ZUL6\\Kendall et al_2015_Bayesian SegNet.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZS2LF2P6\\1511.html}
}

@unpublished{kendallWhatUncertaintiesWe2017,
  title = {What {{Uncertainties Do We Need}} in {{Bayesian Deep Learning}} for {{Computer Vision}}?},
  author = {Kendall, Alex and Gal, Yarin},
  date = {2017-03-15},
  eprint = {1703.04977},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.04977},
  urldate = {2018-12-28},
  abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model -- uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\82FA8DA4\\Kendall_Gal_2017_What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\EZLD785M\\1703.html}
}

@article{kennardComputerAidedDesign1969,
  title = {Computer {{Aided Design}} of {{Experiments}}},
  author = {Kennard, R. W. and Stone, L. A.},
  date = {1969-02-01},
  journaltitle = {Technometrics},
  volume = {11},
  number = {1},
  pages = {137--148},
  publisher = {Taylor \& Francis},
  issn = {0040-1706},
  doi = {10.1080/00401706.1969.10490666},
  url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1969.10490666},
  urldate = {2022-08-23},
  abstract = {A computer oriented method which assists in the construction of response surface type experimental plans is described. It takes into account constraints met in practice that standard procedures do not consider explicitly. The method is a sequential one and each step covers the experimental region uniformly. Applications to well-known situations are given to demonstrate the reasonableness of the procedure. Application to a ‘messy” design situation is given to demonstrate its novelty.}
}

@article{kennardComputerAidedDesign1969a,
  title = {Computer {{Aided Design}} of {{Experiments}}},
  author = {Kennard, R. W. and Stone, L. A.},
  date = {1969},
  journaltitle = {Technometrics},
  volume = {11},
  number = {1},
  eprint = {1266770},
  eprinttype = {jstor},
  pages = {137--148},
  publisher = {[Taylor \& Francis, Ltd., American Statistical Association, American Society for Quality]},
  issn = {0040-1706},
  doi = {10.2307/1266770},
  url = {https://www.jstor.org/stable/1266770},
  urldate = {2022-06-07},
  abstract = {A computer oriented method which assists in the construction of response surface type experimental plans is described. It takes into account constraints met in practice that standard procedures do not consider explicitly. The method is a sequential one and each step covers the experimental region uniformly. Applications to well-known situations are given to demonstrate the reasonableness of the procedure. Application to a "messy" design situation is given to demonstrate its novelty.},
  file = {C:\Users\USEBPERP\Zotero\storage\7WMZVPQR\Kennard and Stone - 1969 - Computer Aided Design of Experiments.pdf}
}

@article{kerstjensLEADDLamarckianEvolutionary2022,
  title = {{{LEADD}}: {{Lamarckian}} Evolutionary Algorithm for de Novo Drug Design},
  shorttitle = {{{LEADD}}},
  author = {Kerstjens, Alan and De Winter, Hans},
  date = {2022-01-15},
  journaltitle = {Journal of Cheminformatics},
  volume = {14},
  number = {1},
  pages = {3},
  issn = {1758-2946},
  doi = {10.1186/s13321-022-00582-y},
  url = {https://doi.org/10.1186/s13321-022-00582-y},
  urldate = {2022-05-19},
  abstract = {Given an objective function that predicts key properties of a molecule, goal-directed de novo molecular design is a useful tool to identify molecules that maximize or minimize said objective function. Nonetheless, a common drawback of these methods is that they tend to design synthetically unfeasible molecules. In this paper we describe a Lamarckian evolutionary algorithm for de novo drug design (LEADD). LEADD attempts to strike a balance between optimization power, synthetic accessibility of designed molecules and computational efficiency. To increase the likelihood of designing synthetically accessible molecules, LEADD represents molecules as graphs of molecular fragments, and limits the bonds that can be formed between them through knowledge-based pairwise atom type compatibility rules. A reference library of drug-like molecules is used to extract fragments, fragment preferences and compatibility rules. A novel set of genetic operators that enforce these rules in a computationally efficient manner is presented. To sample chemical space more efficiently we also explore a Lamarckian evolutionary mechanism that adapts the reproductive behavior of molecules. LEADD has been compared to both standard virtual screening and a comparable evolutionary algorithm using a standardized benchmark suite and was shown to be able to identify fitter molecules more efficiently. Moreover, the designed molecules are predicted to be easier to synthesize than those designed by other evolutionary algorithms.},
  keywords = {De novo drug design,Evolutionary algorithm,Fragment-based,Graph-based,Synthetic accessibility},
  file = {C:\Users\USEBPERP\Zotero\storage\SCLF3EIJ\Kerstjens and De Winter - 2022 - LEADD Lamarckian evolutionary algorithm for de no.pdf}
}

@unpublished{keskarLargeBatchTrainingDeep2017,
  ids = {keskar2017largebatcha},
  title = {On {{Large-Batch Training}} for {{Deep Learning}}: {{Generalization Gap}} and {{Sharp Minima}}},
  shorttitle = {On {{Large-Batch Training}} for {{Deep Learning}}},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  date = {2017-02-09},
  eprint = {1609.04836},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/1609.04836},
  urldate = {2020-02-03},
  abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\3JAYFMHD\\Keskar et al_2017_On Large-Batch Training for Deep Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DGVXEHJD\\Keskar et al_2017_On Large-Batch Training for Deep Learning2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\L5N6MQB8\\Keskar et al_2017_On Large-Batch Training for Deep Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZATUPLQE\\Keskar et al_2017_On Large-Batch Training for Deep Learning3.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\EPSXT8BX\\1609.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\JI8VPS7C\\1609.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\U39M9A6S\\1609.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\VAKIWN2X\\1609.html}
}

@inproceedings{kidambiInsufficiencyExistingMomentum2018,
  ids = {kidambi2018insufficiencya},
  title = {On the {{Insufficiency}} of {{Existing Momentum Schemes}} for {{Stochastic Optimization}}},
  booktitle = {2018 {{Information Theory}} and {{Applications Workshop}} ({{ITA}})},
  author = {Kidambi, Rahul and Netrapalli, Praneeth and Jain, Prateek and Kakade, Sham},
  date = {2018-02},
  pages = {1--9},
  publisher = {IEEE},
  location = {San Diego, CA},
  doi = {10.1109/ITA.2018.8503173},
  url = {https://ieeexplore.ieee.org/document/8503173/},
  urldate = {2020-02-05},
  abstract = {Momentum based stochastic gradient methods such as heavy ball (HB) and Nesterov’s accelerated gradient descent (NAG) method are widely used in practice for training deep networks and other supervised learning models, as they often provide significant improvements over stochastic gradient descent (SGD). Rigorously speaking, “fast gradient” methods have provable improvements over gradient descent only for the deterministic case, where the gradients are exact. In the stochastic case, the popular explanations for their wide applicability is that when these fast gradient methods are applied in the stochastic case, they partially mimic their exact gradient counterparts, resulting in some practical gain. This work provides a counterpoint to this belief by proving that there exist simple problem instances where these methods cannot outperform SGD despite the best setting of its parameters. These negative problem instances are, in an informal sense, generic; they do not look like carefully constructed pathological instances. These results suggest (along with empirical evidence) that HB or NAG’s practical performance gains are a by-product of mini-batching.},
  eventtitle = {2018 {{Information Theory}} and {{Applications Workshop}} ({{ITA}})},
  isbn = {978-1-72810-124-8},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\CZ8CYGKA\\Kidambi et al. - 2018 - On the Insufficiency of Existing Momentum Schemes .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\MIMQWMKC\\Kidambi et al_2018_On the insufficiency of existing momentum schemes for Stochastic Optimization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VCRVMKFB\\Kidambi et al. - 2018 - On the Insufficiency of Existing Momentum Schemes .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\STEC9YQL\\forum.html}
}

@article{kidambiStochasticGradientDescent,
  title = {Stochastic {{Gradient Descent For Modern Machine Learning}}: {{Theory}}, {{Algorithms And Applications}}},
  author = {Kidambi, Rahul},
  pages = {256},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\GRXBAKCA\Kidambi - Stochastic Gradient Descent For Modern Machine Lea.pdf}
}

@article{kimDeepNAPDeepNeural2018,
  title = {{{DeepNAP}}: {{Deep}} Neural Anomaly Pre-Detection in a Semiconductor Fab},
  shorttitle = {{{DeepNAP}}},
  author = {Kim, Chunggyeom and Lee, Jinhyuk and Kim, Raehyun and Park, Youngbin and Kang, Jaewoo},
  date = {2018-08-01},
  journaltitle = {Information Sciences},
  volume = {457--458},
  pages = {1--11},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2018.05.020},
  url = {https://www.sciencedirect.com/science/article/pii/S002002551830375X},
  urldate = {2023-01-27},
  abstract = {Anomaly detection in an industrial process is crucial for preventing unexpected economic loss. Among various signals, multivariate time series signals are one of the most difficult signals to analyze for detecting anomalies. Moreover, labels for anomalous signals are often unavailable in many fields. To tackle this problem, we present DeepNAP which is an anomaly pre-detection model based on recurrent neural networks. Without any annotated data, DeepNAP successfully learns to detect anomalies using partial reconstruction. Furthermore, detecting anomalies in advance is essential for preventing catastrophic events. While previous studies focused mainly on capturing anomalies after they have occurred, DeepNAP is able to pre-detect anomalies. We evaluate DeepNAP and other baseline models on a real multivariate dataset generated from a semiconductor manufacturing fab. Compared with other baseline models, DeepNAP achieves the best performance on both the detection and pre-detection of anomalies.},
  langid = {english},
  keywords = {Anomaly detection,Long short term memory,Multivariate,Time series data},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\UQ4LYPQA\\Kim et al. - 2018 - DeepNAP Deep neural anomaly pre-detection in a se.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\W89L3VRC\\S002002551830375X.html}
}

@article{kimValidPlausibleDiverse2021,
  title = {Valid, {{Plausible}}, and {{Diverse Retrosynthesis Using Tied Two-Way Transformers}} with {{Latent Variables}}},
  author = {Kim, Eunji and Lee, Dongseon and Kwon, Youngchun and Park, Min Sik and Choi, Youn-Suk},
  date = {2021-01-07},
  journaltitle = {J. Chem. Inf. Model.},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.0c01074},
  url = {https://doi.org/10.1021/acs.jcim.0c01074},
  urldate = {2021-01-22},
  abstract = {Retrosynthesis is an essential task in organic chemistry for identifying the synthesis pathways of newly discovered materials, and with the recent advances in deep learning, there have been growing attempts to solve the retrosynthesis problem through transformer models, which are the state-of-the-art in neural machine translation, by converting the problem into a machine translation problem. However, the pure transformer provides unsatisfactory results that lack grammatical validity, chemical plausibility, and diversity in reactant candidates. In this study, we develop tied two-way transformers with latent modeling to solve those problems using cycle consistency checks, parameter sharing, and multinomial latent variables. Experimental results obtained using public and in-house datasets demonstrate that the proposed model improves the retrosynthesis accuracy, grammatical error, and diversity, and qualitative evaluation results verify its ability to suggest valid and plausible results.}
}

@unpublished{kingmaAdamMethodStochastic2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  date = {2014-12-22},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1412.6980},
  urldate = {2018-05-20},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  keywords = {Computer Science - Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\SJ77KYA5\\Kingma_Ba_2014_Adam.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JCQGYH4J\\1412.html}
}

@online{kingmaAutoEncodingVariationalBayes2022,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2022-12-10},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2023-04-11},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\CEI4M794\\Kingma and Welling - 2022 - Auto-Encoding Variational Bayes.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2N5JQDFG\\1312.html}
}

@incollection{kingmaGlowGenerativeFlow2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}},
  shorttitle = {Glow},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Kingma, Durk P and Dhariwal, Prafulla},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {10215--10224},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions.pdf},
  urldate = {2019-12-19},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LG95ACG4\\Kingma_Dhariwal_2018_Glow2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\S4Q32V7F\\8224-glow-generative-flow-with-invertible-1x1-convolutions.html}
}

@unpublished{kingmaIntroductionVariationalAutoencoders2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2019-06-06},
  eprint = {1906.02691},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.02691},
  urldate = {2019-06-12},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\V7N7X3PI\\Kingma_Welling_2019_An Introduction to Variational Autoencoders.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BIHAJAI8\\1906.html}
}

@unpublished{kipfSemiSupervisedClassificationGraph2016,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2016-09-09},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1609.02907},
  urldate = {2018-09-17},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NNRNP3W9\\Kipf_Welling_2016_Semi-Supervised Classification with Graph Convolutional Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZAPNI22P\\1609.html}
}

@article{kiureghianAleatoryEpistemicDoes2009,
  title = {Aleatory or Epistemic? {{Does}} It Matter?},
  shorttitle = {Aleatory or Epistemic?},
  author = {Kiureghian, Armen Der and Ditlevsen, Ove},
  date = {2009-03},
  journaltitle = {Structural Safety},
  volume = {31},
  number = {2},
  pages = {105--112},
  issn = {01674730},
  doi = {10.1016/j.strusafe.2008.06.020},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167473008000556},
  urldate = {2019-04-15},
  abstract = {The sources and characters of uncertainties in engineering modeling for risk and reliability analyses are discussed. While many sources of uncertainty may exist, they are generally categorized as either aleatory or epistemic. Uncertainties are characterized as epistemic, if the modeler sees a possibility to reduce them by gathering more data or by refining models. Uncertainties are categorized as aleatory if the modeler does not foresee the possibility of reducing them. From a pragmatic standpoint, it is useful to thus categorize the uncertainties within a model, since it then becomes clear as to which uncertainties have the potential of being reduced. More importantly, epistemic uncertainties may introduce dependence among random events, which may not be properly noted if the character of uncertainties is not correctly modeled. Influences of the two types of uncertainties in reliability assessment, codified design, performance-based engineering and risk-based decision-making are discussed. Two simple examples demonstrate the influence of statistical dependence arising from epistemic uncertainties on systems and time-variant reliability problems.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\8TMI3TX6\Kiureghian_Ditlevsen_2009_Aleatory or epistemic.pdf}
}

@article{klambauerMachineLearningDrug2019,
  title = {Machine {{Learning}} in {{Drug Discovery}}},
  author = {Klambauer, Günter and Hochreiter, Sepp and Rarey, Matthias},
  date = {2019-03-25},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {59},
  number = {3},
  pages = {945--946},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.9b00136},
  url = {https://pubs.acs.org/doi/abs/10.1021/acs.jcim.9b00136},
  urldate = {2020-09-08},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\FKU9H7GX\\Klambauer et al_2019_Machine Learning in Drug Discovery.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\68CD4HBS\\acs.jcim.html}
}

@unpublished{klambauerSelfNormalizingNeuralNetworks2017,
  title = {Self-{{Normalizing Neural Networks}}},
  author = {Klambauer, Günter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  date = {2017-06-08},
  eprint = {1706.02515},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.02515},
  urldate = {2018-05-24},
  abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\S3UKDM7A\\Klambauer et al_2017_Self-Normalizing Neural Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZLPNK829\\1706.html}
}

@incollection{klasUncertaintyMachineLearning2018,
  title = {Uncertainty in {{Machine Learning Applications}}: {{A Practice-Driven Classification}} of {{Uncertainty}}},
  shorttitle = {Uncertainty in {{Machine Learning Applications}}},
  booktitle = {Developments in {{Language Theory}}},
  author = {Kläs, Michael and Vollmer, Anna Maria},
  editor = {Hoshi, Mizuho and Seki, Shinnosuke},
  date = {2018},
  volume = {11088},
  pages = {431--438},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-99229-7_36},
  url = {http://link.springer.com/10.1007/978-3-319-99229-7_36},
  urldate = {2019-03-12},
  abstract = {Software-intensive systems that rely on machine learning (ML) and artificial intelligence (AI) are increasingly becoming part of our daily life, e.g., in recommendation systems or semi-autonomous vehicles. However, the use of ML and AI is accompanied by uncertainties regarding their outcomes. Dealing with such uncertainties is particularly important when the actions of these systems can harm humans or the environment, such as in the case of a medical product or self-driving car. To enable a system to make informed decisions when confronted with the uncertainty of embedded AI/ML models and possible safetyrelated consequences, these models do not only have to provide a defined functionality but must also describe as precisely as possible the likelihood of their outcome being wrong or outside a given range of accuracy. Thus, this paper proposes a classification of major uncertainty sources that is usable and useful in practice: scope compliance, data quality, and model fit. In particular, we highlight the implications of these classes in the development and testing of ML and AI models by establishing links to specific activities during development and testing and means for quantifying and dealing with these different sources of uncertainty.},
  isbn = {978-3-319-98653-1 978-3-319-98654-8},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\MU4X2JH4\Kläs_Vollmer_2018_Uncertainty in Machine Learning Applications.pdf}
}

@article{klebeRecentDevelopmentsStructurebased2000,
  title = {Recent Developments in Structure-Based Drug Design},
  author = {Klebe, Gerhard},
  date = {2000-07-01},
  journaltitle = {J Mol Med},
  volume = {78},
  number = {5},
  pages = {269--281},
  issn = {0946-2716, 1432-1440},
  doi = {10.1007/s001090000084},
  url = {https://link.springer.com/article/10.1007/s001090000084},
  urldate = {2018-05-21},
  abstract = {. Structure-based design has emerged as a new tool in medicinal chemistry. A prerequisite for this new approach is an understanding of the principles of molecular recognition in protein-ligand complexes. If the three-dimensional structure of a given protein is known, this information can be directly exploited for the retrieval and design of new ligands. Structure-based ligand design is an iterative approach. First of all, it requires the crystal structure or a model derived from the crystal structure of a closely related homolog of the target protein, preferentially complexed with a ligand. This complex unravels the binding mode and conformation of a ligand under investigation and indicates the essential aspects determining its binding affinity. It is then used to generate new ideas about ways of improving an existing ligand or of developing new alternative bonding skeletons. Computational methods supplemented by molecular graphics are applied to assist this step of hypothesis generation. The features of the protein binding pocket can be translated into queries used for virtual computer screening of large compound libraries or to design novel ligands de novo. These initial proposals must be confirmed experimentally. Subsequently they are optimized toward higher affinity and better selectivity. The latter aspect is of utmost importance in defining and controlling the pharmacological profile of a ligand. A prerequisite to tailoring selectivity by rational design is a detailed understanding of molecular parameters determining selectivity. Taking examples from current drug development programs (HIV proteinase, t-RNA transglycosylase, thymidylate synthase, thrombin and, related serine proteinases), we describe recent advances in lead discovery via computer screening, iterative design, and understanding of selectivity discrimination.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\FJ6G4BW7\\Klebe_2000_Recent developments in structure-based drug design.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\KFKFZS7S\\10.html}
}

@online{KlettCottaAufsteigerEdgar,
  title = {Klett-Cotta :: Der Aufsteiger - Edgar Wolfrum},
  shorttitle = {Klett-Cotta},
  url = {https://www.klett-cotta.de/buch/Geschichte/Der_Aufsteiger/112146},
  urldate = {2020-07-06},
  langid = {ngerman},
  file = {C:\Users\USEBPERP\Zotero\storage\Y6V5FJZY\112146.html}
}

@article{klucznikEfficientSynthesesDiverse2018,
  title = {Efficient {{Syntheses}} of {{Diverse}}, {{Medicinally Relevant Targets Planned}} by {{Computer}} and {{Executed}} in the {{Laboratory}}},
  author = {Klucznik, Tomasz and Mikulak-Klucznik, Barbara and McCormack, Michael P. and Lima, Heather and Szymkuć, Sara and Bhowmick, Manishabrata and Molga, Karol and Zhou, Yubai and Rickershauser, Lindsey and Gajewska, Ewa P. and Toutchkine, Alexei and Dittwald, Piotr and Startek, Michał P. and Kirkovits, Gregory J. and Roszak, Rafał and Adamski, Ariel and Sieredzińska, Bianka and Mrksich, Milan and Trice, Sarah L. J. and Grzybowski, Bartosz A.},
  date = {2018-03-08},
  journaltitle = {Chem},
  volume = {4},
  number = {3},
  pages = {522--532},
  issn = {2451-9294},
  doi = {10.1016/j.chempr.2018.02.002},
  url = {http://www.sciencedirect.com/science/article/pii/S2451929418300639},
  urldate = {2019-10-02},
  abstract = {Summary The Chematica program was used to autonomously design synthetic pathways to eight structurally diverse targets, including seven commercially valuable bioactive substances and one natural product. All of these computer-planned routes were successfully executed in the laboratory and offer significant yield improvements and cost savings over previous approaches, provide alternatives to patented routes, or produce targets that were not synthesized previously.},
  keywords = {artificial intelligence,Chematica,chemical networks and graphs,computer-assisted synthesis,large-scale calculations,organic synthesis,SDG9: Industry innovation and infrastructure},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LQTNPXRH\\Klucznik et al_2018_Efficient Syntheses of Diverse, Medicinally Relevant Targets Planned by.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\K8FTBANP\\S2451929418300639.html}
}

@incollection{kocsisBanditBasedMonteCarlo2006,
  title = {Bandit {{Based Monte-Carlo Planning}}},
  booktitle = {Machine {{Learning}}: {{ECML}} 2006},
  author = {Kocsis, Levente and Szepesvári, Csaba},
  editor = {Fürnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
  editora = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
  editoratype = {redactor},
  date = {2006},
  volume = {4212},
  pages = {282--293},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/11871842_29},
  url = {http://link.springer.com/10.1007/11871842_29},
  urldate = {2020-06-05},
  abstract = {For large state-space Markovian Decision Problems MonteCarlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.},
  isbn = {978-3-540-45375-8 978-3-540-46056-5},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\CLKJTVEH\Kocsis and Szepesvári - 2006 - Bandit Based Monte-Carlo Planning.pdf}
}

@unpublished{korovinaChemBOBayesianOptimization2019,
  ids = {korovina2019chemboa},
  title = {{{ChemBO}}: {{Bayesian Optimization}} of {{Small Organic Molecules}} with {{Synthesizable Recommendations}}},
  shorttitle = {{{ChemBO}}},
  author = {Korovina, Ksenia and Xu, Sailun and Kandasamy, Kirthevasan and Neiswanger, Willie and Poczos, Barnabas and Schneider, Jeff and Xing, Eric P.},
  date = {2019-10-21},
  eprint = {1908.01425},
  eprinttype = {arxiv},
  eprintclass = {physics, stat},
  url = {http://arxiv.org/abs/1908.01425},
  urldate = {2020-06-03},
  abstract = {In applications such as molecule design or drug discovery, it is desirable to have an algorithm which recommends new candidate molecules based on the results of past tests. These molecules first need to be synthesized and then tested for objective properties. We describe ChemBO, a Bayesian optimization framework for generating and optimizing organic molecules for desired molecular properties. While most existing data-driven methods for this problem do not account for sample efficiency or fail to enforce realistic constraints on synthesizability, our approach explores the synthesis graph in a sample-efficient way and produces synthesizable candidates. We implement ChemBO as a Gaussian process model and explore existing molecular kernels for it. Moreover, we propose a novel optimal-transport based distance and kernel that accounts for graphical information explicitly. In our experiments, we demonstrate the efficacy of the proposed approach on several molecular optimization problems.},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\WZMAUP5C\\Korovina et al_2019_ChemBO.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YRRGADPU\\Korovina et al_2019_ChemBO.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FF82DK6I\\1908.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\QEJ444ZZ\\1908.html}
}

@article{kotsiasDirectSteeringNovo2019,
  title = {Direct {{Steering}} of de Novo {{Molecular Generation}} Using {{Descriptor Conditional Recurrent Neural Networks}} ({{cRNNs}})},
  author = {Kotsias, P. and Arús-Pous, J. and Chen, H. and Engkvist, O. and Tyrchan, C. and Bjerrum, E. J.},
  date = {2019},
  doi = {10.26434/chemrxiv.9860906},
  url = {http://europepmc.org/abstract/ppr/ppr93386},
  urldate = {2019-09-30},
  abstract = {Abstract: {$<$}p{$>$}Deep learning has acquired considerable momentum over the past couple of years in the domain of \emph{de-novo} drug design. Particularly,...},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LVVTZ9A8\\Kotsias et al_2019_Direct Steering of de novo Molecular Generation using Descriptor Conditional.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ADEV4CX4\\ppr93386.html}
}

@online{kottasPeterKottasAnswer2017,
  title = {Peter {{Kottas}}'s Answer to {{How}} Does Deep Learning Work and How Is It Different from Normal Neural Networks Applied with {{SVM}}? {{How}} Does One Go about Starting to Understand Them (Papers/Blogs/Articles)? - {{Quora}}},
  author = {Kottas, Peter},
  date = {2017-09-28},
  url = {https://www.quora.com/How-does-deep-learning-work-and-how-is-it-different-from-normal-neural-networks-applied-with-SVM-How-does-one-go-about-starting-to-understand-them-papers-blogs-articles/answer/Peter-Kottas?share=bcc2bc51&srid=HMSK},
  urldate = {2018-07-03}
}

@article{koutsoyiannisBluecatLocalUncertainty2022,
  title = {Bluecat: {{A Local Uncertainty Estimator}} for {{Deterministic Simulations}} and {{Predictions}}},
  shorttitle = {Bluecat},
  author = {Koutsoyiannis, D. and Montanari, A.},
  date = {2022},
  journaltitle = {Water Resources Research},
  volume = {58},
  number = {1},
  pages = {e2021WR031215},
  issn = {1944-7973},
  doi = {10.1029/2021WR031215},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2021WR031215},
  urldate = {2022-07-19},
  abstract = {We present a new method for simulating and predicting hydrologic variables with uncertainty assessment and provide example applications to river flows. The method is identified with the acronym “Bluecat” and is based on the use of a deterministic model which is subsequently converted to a stochastic formulation. The latter provides an adjustment on statistical basis of the deterministic prediction along with its confidence limits. The distinguishing features of the proposed approach are the ability to infer the probability distribution of the prediction without requiring strong hypotheses on the statistical characterization of the prediction error (e.g., normality, homoscedasticity), and its transparent and intuitive use of the observations. Bluecat makes use of a rigorous theory to estimate the probability distribution of the predictand conditioned by the deterministic model output, by inferring the conditional statistics of observations. Therefore Bluecat bridges the gaps between deterministic (possibly physically based, or deep learning-based) and stochastic models, as well as between rigorous theory and transparent use of data with an innovative and user oriented approach. We present two examples of application to the case studies of the Arno river at Subbiano and Sieve river at Fornacina. The results confirm the distinguishing features of the method along with its technical soundness. We provide an open software working in the R environment, along with help facilities and detailed instructions to reproduce the case studies presented here.},
  langid = {english},
  keywords = {modeling,prediction,rainfall-runoff,uncertainty},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\QLCNBRRF\\Koutsoyiannis and Montanari - 2022 - Bluecat A Local Uncertainty Estimator for Determi.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\K4NM7RAB\\2021WR031215.html}
}

@online{KrebsforschungIstErklaerungsnot2022,
  title = {Die Krebsforschung ist in Erklärungsnot},
  date = {2022-05-21T08:43+00:00},
  url = {https://www.infosperber.ch/gesundheit/die-krebsforschung-ist-in-erklaerungsnot/},
  urldate = {2022-05-24},
  abstract = {Viele Befunde sind nicht replizierbar. «Wir haben in der Biomedizin ein grosses Problem», sagt ein Wissenschaftler. (Teil 1)…},
  langid = {nswissgerman},
  organization = {infosperber}
}

@article{krennSELFIESFutureMolecular2022,
  title = {{{SELFIES}} and the Future of Molecular String Representations},
  author = {Krenn, Mario and Ai, Qianxiang and Barthel, Senja and Carson, Nessa and Frei, Angelo and Frey, Nathan C. and Friederich, Pascal and Gaudin, Théophile and Gayle, Alberto Alexander and Jablonka, Kevin Maik and Lameiro, Rafael F. and Lemm, Dominik and Lo, Alston and Moosavi, Seyed Mohamad and Nápoles-Duarte, José Manuel and Nigam, AkshatKumar and Pollice, Robert and Rajan, Kohulan and Schatzschneider, Ulrich and Schwaller, Philippe and Skreta, Marta and Smit, Berend and Strieth-Kalthoff, Felix and Sun, Chong and Tom, Gary and Falk Von Rudorff, Guido and Wang, Andrew and White, Andrew D. and Young, Adamo and Yu, Rose and Aspuru-Guzik, Alán},
  date = {2022-10},
  journaltitle = {Patterns},
  volume = {3},
  number = {10},
  pages = {100588},
  issn = {26663899},
  doi = {10.1016/j.patter.2022.100588},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2666389922002069},
  urldate = {2024-01-05},
  abstract = {Artificial intelligence (AI) and machine learning (ML) are expanding in popularity for broad applications to challenging tasks in chemistry and materials science. Examples include the prediction of properties, the discovery of new reaction pathways, or the design of new molecules. The machine needs to read and write fluently in a chemical language for each of these tasks. Strings are a common tool to represent molecular graphs, and the most popular molecular string representation, SMILES, has powered cheminformatics since the late 1980s. However, in the context of AI and ML in chemistry, SMILES has several shortcomings—most pertinently, most combinations of symbols lead to invalid results with no valid chemical interpretation. To overcome this issue, a new language for molecules was introduced in 2020 that guarantees 100\% robustness: SELF-referencing embedded string (SELFIES). SELFIES has since simplified and enabled numerous new applications in chemistry. In this perspective, we look to the future and discuss molecular string representations, along with their respective opportunities and challenges. We propose 16 concrete future projects for robust molecular representations. These involve the extension toward new chemical domains, exciting questions at the interface of AI and robust languages, and interpretability for both humans and machines. We hope that these proposals will inspire several follow-up works exploiting the full potential of molecular string representations for the future of AI in chemistry and materials science.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\ZKR7KYJR\Krenn et al. - 2022 - SELFIES and the future of molecular string represe.pdf}
}

@unpublished{krennSelfReferencingEmbeddedStrings2020,
  title = {Self-{{Referencing Embedded Strings}} ({{SELFIES}}): {{A}} 100\% Robust Molecular String Representation},
  shorttitle = {Self-{{Referencing Embedded Strings}} ({{SELFIES}})},
  author = {Krenn, Mario and Häse, Florian and Nigam, AkshatKumar and Friederich, Pascal and Aspuru-Guzik, Alán},
  date = {2020-03-04},
  eprint = {1905.13741},
  eprinttype = {arxiv},
  eprintclass = {physics, physics:quant-ph, stat},
  url = {http://arxiv.org/abs/1905.13741},
  urldate = {2020-04-15},
  abstract = {The discovery of novel materials and functional molecules can help to solve some of society's most urgent challenges, ranging from efficient energy harvesting and storage to uncovering novel pharmaceutical drug candidates. Traditionally matter engineering -- generally denoted as inverse design -- was based massively on human intuition and high-throughput virtual screening. The last few years have seen the emergence of significant interest in computer-inspired designs based on evolutionary or deep learning methods. The major challenge here is that the standard strings molecular representation SMILES shows substantial weaknesses in that task because large fractions of strings do not correspond to valid molecules. Here, we solve this problem at a fundamental level and introduce SELFIES (SELF-referencIng Embedded Strings), a string-based representation of molecules which is 100\textbackslash\% robust. Every SELFIES string corresponds to a valid molecule, and SELFIES can represent every molecule. SELFIES can be directly applied in arbitrary machine learning models without the adaptation of the models; each of the generated molecule candidates is valid. In our experiments, the model's internal memory stores two orders of magnitude more diverse molecules than a similar test with SMILES. Furthermore, as all molecules are valid, it allows for explanation and interpretation of the internal working of the generative models.},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Quantum Physics,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DBGRDSLF\\Krenn et al_2020_Self-Referencing Embedded Strings (SELFIES).pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\MINSY8Y3\\1905.html}
}

@unpublished{kuchaievFactorizationTricksLSTM2017,
  title = {Factorization Tricks for {{LSTM}} Networks},
  author = {Kuchaiev, Oleksii and Ginsburg, Boris},
  date = {2017-03-30},
  eprint = {1703.10722},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1703.10722},
  urldate = {2019-01-04},
  abstract = {We present two simple ways of reducing the number of parameters and accelerating the training of large Long Short-Term Memory (LSTM) networks: the first one is "matrix factorization by design" of LSTM matrix into the product of two smaller matrices, and the second one is partitioning of LSTM matrix, its inputs and states into the independent groups. Both approaches allow us to train large LSTM networks significantly faster to the near state-of the art perplexity while using significantly less RNN parameters.},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\RVNWZW97\\Kuchaiev_Ginsburg_2017_Factorization tricks for LSTM networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SM46WIFP\\1703.html}
}

@incollection{kulkarniHierarchicalDeepReinforcement2016,
  title = {Hierarchical {{Deep Reinforcement Learning}}: {{Integrating Temporal Abstraction}} and {{Intrinsic Motivation}}},
  shorttitle = {Hierarchical {{Deep Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  date = {2016},
  pages = {3675--3683},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/6233-hierarchical-deep-reinforcement-learning-integrating-temporal-abstraction-and-intrinsic-motivation.pdf},
  urldate = {2019-06-06},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BLMMX2SN\\Kulkarni et al_2016_Hierarchical Deep Reinforcement Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\UZ6FNZ2B\\6233-hierarchical-deep-reinforcement-learning-integrating-temporal-abstraction-and-intrinsic-mo.html}
}

@unpublished{kumarParallelArchitectureHyperparameter2018,
  title = {Parallel {{Architecture}} and {{Hyperparameter Search}} via {{Successive Halving}} and {{Classification}}},
  author = {Kumar, Manoj and Dahl, George E. and Vasudevan, Vijay and Norouzi, Mohammad},
  date = {2018-05-25},
  eprint = {1805.10255},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1805.10255},
  urldate = {2020-02-18},
  abstract = {We present a simple and powerful algorithm for parallel black box optimization called Successive Halving and Classification (SHAC). The algorithm operates in \$K\$ stages of parallel function evaluations and trains a cascade of binary classifiers to iteratively cull the undesirable regions of the search space. SHAC is easy to implement, requires no tuning of its own configuration parameters, is invariant to the scale of the objective function and can be built using any choice of binary classifier. We adopt tree-based classifiers within SHAC and achieve competitive performance against several strong baselines for optimizing synthetic functions, hyperparameters and architectures.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\E7YPP6YN\\Kumar et al_2018_Parallel Architecture and Hyperparameter Search via Successive Halving and.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TKVPN46V\\1805.html}
}

@incollection{kumarSelfPacedLearningLatent2010,
  title = {Self-{{Paced Learning}} for {{Latent Variable Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 23},
  author = {Kumar, M. P. and Packer, Benjamin and Koller, Daphne},
  editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
  date = {2010},
  pages = {1189--1197},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/3923-self-paced-learning-for-latent-variable-models.pdf},
  urldate = {2019-05-02},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\MUEJBQ2G\\Kumar et al_2010_Self-Paced Learning for Latent Variable Models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YJJUD9FN\\3923-self-paced-learning-for-latent-variable-models.html}
}

@unpublished{kusnerGrammarVariationalAutoencoder2017,
  title = {Grammar {{Variational Autoencoder}}},
  author = {Kusner, Matt J. and Paige, Brooks and Hernández-Lobato, José Miguel},
  date = {2017-03-06},
  eprint = {1703.01925},
  eprinttype = {arxiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/1703.01925},
  urldate = {2018-09-17},
  abstract = {Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as video and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which encodes and decodes directly to and from these parse trees, ensuring the generated outputs are always valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecular synthesis.},
  keywords = {Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\K6YYDB7H\\Kusner et al_2017_Grammar Variational Autoencoder.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YF2APUV6\\Kusner et al_2017_Grammar Variational Autoencoder2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FY6PSRJL\\1703.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\HCAJ5BJF\\1703.html}
}

@online{KUSSSWillkommen,
  title = {{{KUSSS}} | {{Willkommen}}},
  url = {https://kusss.jku.at/kusss/index.action},
  urldate = {2020-06-10},
  file = {C:\Users\USEBPERP\Zotero\storage\WJ4DPLQN\index.html}
}

@unpublished{lakshminarayananSimpleScalablePredictive2016,
  title = {Simple and {{Scalable Predictive Uncertainty Estimation}} Using {{Deep Ensembles}}},
  author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  date = {2016-12-05},
  eprint = {1612.01474},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1612.01474},
  urldate = {2019-04-24},
  abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\9APG4PIS\\Lakshminarayanan et al_2016_Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WE26CU6U\\Lakshminarayanan et al_2016_Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\5UQSLH2B\\1612.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\DV56R2BZ\\1612.html}
}

@article{lalElectrophilicNFFluorinating1996,
  title = {Electrophilic {{NF Fluorinating Agents}}},
  author = {Lal, G. Sankar and Pez, Guido P. and Syvret, Robert G.},
  date = {1996-01-01},
  journaltitle = {Chem. Rev.},
  volume = {96},
  number = {5},
  pages = {1737--1756},
  publisher = {American Chemical Society},
  issn = {0009-2665},
  doi = {10.1021/cr941145p},
  url = {https://doi.org/10.1021/cr941145p},
  urldate = {2020-09-03},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\H5BXREZW\\Lal et al_1996_Electrophilic NF Fluorinating Agents.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\E2VFYABM\\cr941145p.html}
}

@online{LambdaLabsNeurIPS,
  title = {Lambda {{Labs}} @ {{NeurIPS}} 2018},
  url = {https://lambdalabs.com/neurips-2018},
  urldate = {2020-11-02},
  abstract = {Visit us at NeurIPS 2018 and Get Discounts. Stop by our booth for discounts on our Deep Learning Servers, Workstations, and Laptops.},
  file = {C:\Users\USEBPERP\Zotero\storage\Y9UZSBY4\neurips-2018.html}
}

@article{lammFindingNearoptimalIndependent2017,
  title = {Finding Near-Optimal Independent Sets at Scale},
  author = {Lamm, Sebastian and Sanders, Peter and Schulz, Christian and Strash, Darren and Werneck, Renato F.},
  date = {2017-08-01},
  journaltitle = {J Heuristics},
  volume = {23},
  number = {4},
  pages = {207--229},
  issn = {1572-9397},
  doi = {10.1007/s10732-017-9337-x},
  url = {https://doi.org/10.1007/s10732-017-9337-x},
  urldate = {2023-09-02},
  abstract = {The maximum independent set problem is NP-hard and particularly difficult to solve in sparse graphs, which typically take exponential time to solve exactly using the best-known exact algorithms. In this paper, we present two new novel heuristic algorithms for computing large independent sets on huge sparse graphs, which are intractable in practice. First, we develop an advanced evolutionary algorithm that uses fast graph partitioning with local search algorithms to implement efficient combine operations that exchange whole blocks of given independent sets. Though the evolutionary algorithm itself is highly competitive with existing heuristic algorithms on large social networks, we further show that it can be effectively used as an oracle to guess vertices that are likely to be in large independent sets. We then show how to combine these guesses with kernelization techniques in a branch-and-reduce-like algorithm to compute high-quality independent sets quickly in huge complex networks. Our experiments against a recent (and fast) exact algorithm for large sparse graphs show that our technique always computes an optimal solution when the exact solution is known, and it further computes consistent results on even larger instances where the solution is unknown. Ultimately, we show that identifying and removing vertices likely to be in large independent sets opens up the reduction space—which not only speeds up the computation of large independent sets drastically, but also enables us to compute high-quality independent sets on much larger instances than previously reported in the literature.},
  langid = {english},
  keywords = {Evolutionary/genetic algorithms,Heuristic algorithms,Kernelization,Local search,Maximum independent set problem,Minimum vertex cover problem},
  file = {C:\Users\USEBPERP\Zotero\storage\M74XUGYU\Lamm et al. - 2017 - Finding near-optimal independent sets at scale.pdf}
}

@online{landrumNewLesselBriem,
  title = {A New "{{Lessel}} and {{Briem}} like" Dataset},
  author = {Landrum, Greg},
  url = {http://rdkit.blogspot.com/2019/10/a-new-lessel-and-briem-like-dataset.html},
  urldate = {2023-10-26},
  abstract = {This is another one that ends up being a bit beyond what blogger can handle. So the post is just a summary, full details are in the jupyter ...},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\8GHG9EZ7\a-new-lessel-and-briem-like-dataset.html}
}

@online{landrumRDKitFingerprintThresholds2013,
  title = {{{RDKit}}: {{Fingerprint Thresholds}}},
  shorttitle = {{{RDKit}}},
  author = {Landrum, Greg},
  date = {2013-10-09},
  url = {https://rdkit.blogspot.com/2013/10/fingerprint-thresholds.html},
  urldate = {2022-08-22},
  organization = {RDKit},
  keywords = {reference,similarity},
  file = {C:\Users\USEBPERP\Zotero\storage\6AP3RXL5\fingerprint-thresholds.html}
}

@online{landrumRDKitNewLessel2019,
  title = {{{RDKit}}: {{A}} New "{{Lessel}} and {{Briem}} like" Dataset},
  shorttitle = {{{RDKit}}},
  author = {Landrum, Greg},
  date = {2019-10-05},
  url = {https://rdkit.blogspot.com/2019/10/a-new-lessel-and-briem-like-dataset.html},
  urldate = {2023-10-29},
  organization = {RDKit},
  keywords = {ChEMBL,dataset,similarity},
  file = {C:\Users\USEBPERP\Zotero\storage\SLH5RSP8\a-new-lessel-and-briem-like-dataset.html}
}

@online{landrumRDKitOpensourceCheminformatics2006,
  title = {{{RDKit}}: {{Open-source}} Cheminformatics},
  shorttitle = {{{RDKit}}},
  author = {Landrum, Greg},
  date = {2006},
  url = {http://www.rdkit.org}
}

@online{landrumRDKitOptimizingDiversity2014,
  title = {{{RDKit}}: {{Optimizing Diversity Picking}} in the {{RDKit}}},
  shorttitle = {{{RDKit}}},
  author = {Landrum, Greg},
  date = {2014-08-10},
  url = {http://rdkit.blogspot.com/2014/08/optimizing-diversity-picking-in-rdkit.html},
  urldate = {2022-07-28},
  organization = {RDKit},
  keywords = {optimization,similarity,tutorial},
  file = {C:\Users\USEBPERP\Zotero\storage\JMLT73SJ\optimizing-diversity-picking-in-rdkit.html}
}

@online{landrumRDKitPickingDiverse2014,
  title = {{{RDKit}}: {{Picking}} Diverse Compounds from Large Sets},
  shorttitle = {{{RDKit}}},
  author = {Landrum, Greg},
  date = {2014-08-10},
  url = {http://rdkit.blogspot.com/2014/08/picking-diverse-compounds-from-large.html},
  urldate = {2022-07-28},
  organization = {RDKit},
  keywords = {similarity},
  file = {C:\Users\USEBPERP\Zotero\storage\R8MYTGPA\picking-diverse-compounds-from-large.html}
}

@online{landrumRDKitRevistingMaxMinPicker2017,
  title = {{{RDKit}}: {{Revisting}} the {{MaxMinPicker}}},
  shorttitle = {{{RDKit}}},
  author = {Landrum, Greg},
  date = {2017-11-15},
  url = {http://rdkit.blogspot.com/2017/11/revisting-maxminpicker.html},
  urldate = {2022-06-08},
  organization = {RDKit},
  keywords = {optimization,similarity,tutorial}
}

@online{LangeNachtUtopie,
  title = {Lange Nacht der Utopie | JKU Linz},
  url = {https://www.jku.at/lange-nacht-der-utopie/},
  urldate = {2020-07-31},
  langid = {ngerman},
  file = {C:\Users\USEBPERP\Zotero\storage\CBRW2B5P\lange-nacht-der-utopie.html}
}

@article{langevinExplainingAvoidingFailure2022,
  title = {Explaining and Avoiding Failure Modes in Goal-Directed Generation of Small Molecules},
  author = {Langevin, Maxime and Vuilleumier, Rodolphe and Bianciotto, Marc},
  date = {2022-04-01},
  journaltitle = {Journal of Cheminformatics},
  volume = {14},
  number = {1},
  pages = {20},
  issn = {1758-2946},
  doi = {10.1186/s13321-022-00601-y},
  url = {https://doi.org/10.1186/s13321-022-00601-y},
  urldate = {2022-04-29},
  abstract = {Despite growing interest and success in automated in-silico molecular design, questions remain regarding the ability of goal-directed generation algorithms to perform unbiased exploration of novel chemical spaces. A specific phenomenon has recently been highlighted: goal-directed generation guided with machine learning models produce molecules with high scores according to the optimization model, but low scores according to control models, even when trained on the same data distribution and the same target. In this work, we show that this worrisome behavior is actually due to issues with the predictive models and not the goal-directed generation algorithms. We show that with appropriate predictive models, this issue can be resolved, and molecules generated have high scores according to both the optimization and the control models.},
  keywords = {De novo design,Failure modes,Generative models,Goal-directed generation,QSAR,Quantitative structure-activity relationship,Recurrent neural network,Reinforcement learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7DJDH2D4\\Langevin et al. - 2022 - Explaining and avoiding failure modes in goal-dire.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\MAW2ELUZ\\s13321-022-00601-y.html}
}

@incollection{langleyEnhancingDrugDiscovery2002,
  title = {Enhancing Drug Discovery by Acquisition of Chemical Diversity},
  booktitle = {Pharmacochemistry {{Library}}},
  author = {Langley, David},
  editor = {family=Goot, given=Henk, prefix=van der, useprefix=true},
  date = {2002-01-01},
  series = {Trends in {{Drug Research III}}},
  volume = {32},
  pages = {135--146},
  publisher = {Elsevier},
  doi = {10.1016/S0165-7208(02)80015-0},
  url = {https://www.sciencedirect.com/science/article/pii/S0165720802800150},
  urldate = {2022-07-04},
  abstract = {One of the key parameters influencing the productivity of high throughput screens, in pharmaceutical drug discovery, is the chemical diversity of the compounds that are tested. Large pharmaceutical companies have significant collections of historical compounds at their disposal and continue to make more by conventional and combinatorial chemistries. The chapter examines the growth of the compound collection, used for screening in GlaxoWellcome, during the period 1996–2000; and discusses the internal drivers for expanding the compound collection and describes how to develop a rationale for compound acquisition as a means of achieving this. During the past ten or more years, the process of drug discovery in pharmaceutical companies has undergone many radical changes. There has been a move away from pharmacology-based testing of small numbers of compounds (often analogues of the natural ligand), generated by conventional medicinal chemistry, toward what is now understood as high throughput screening (HTS). The development of HTS has led to a demand for companies to have significant numbers of appropriate samples for screening at their disposal. One of the potential advantages of the random approach to screening is that one can identify activity with novel chemotypes that had not previously been associated with the particular biological target. There is thus a considerable onus on the screening organization to greatly enhance, on a continuing basis, the chemical diversity of its screening collection.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\MYJZYBP8\S0165720802800150.html}
}

@online{LargescaleComparisonMachine,
  title = {Large-Scale Comparison of Machine Learning Methods for Drug Target Prediction on {{ChEMBL}} - {{Chemical Science}} ({{RSC Publishing}}) {{DOI}}:10.1039/{{C8SC00148K}}},
  url = {https://pubs.rsc.org/ko/content/articlehtml/2018/sc/c8sc00148k},
  urldate = {2021-04-08},
  file = {C:\Users\USEBPERP\Zotero\storage\RXLSCPHD\c8sc00148k.html}
}

@unpublished{laugelIssuesPosthocCounterfactual2019,
  title = {Issues with Post-Hoc Counterfactual Explanations: A Discussion},
  shorttitle = {Issues with Post-Hoc Counterfactual Explanations},
  author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Detyniecki, Marcin},
  date = {2019-06-11},
  eprint = {1906.04774},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.04774},
  urldate = {2019-06-14},
  abstract = {Counterfactual post-hoc interpretability approaches have been proven to be useful tools to generate explanations for the predictions of a trained blackbox classifier. However, the assumptions they make about the data and the classifier make them unreliable in many contexts. In this paper, we discuss three desirable properties and approaches to quantify them: proximity, connectedness and stability. In addition, we illustrate that there is a risk for post-hoc counterfactual approaches to not satisfy these properties.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NYZ3ERC5\\Laugel et al_2019_Issues with post-hoc counterfactual explanations.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\R45HZ2EI\\1906.html}
}

@inproceedings{lavinEvaluatingRealtimeAnomaly2015,
  title = {Evaluating {{Real-time Anomaly Detection Algorithms}} - the {{Numenta Anomaly Benchmark}}},
  booktitle = {2015 {{IEEE}} 14th {{International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Lavin, Alexander and Ahmad, Subutai},
  date = {2015-12},
  eprint = {1510.03336},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {38--44},
  doi = {10.1109/ICMLA.2015.141},
  url = {http://arxiv.org/abs/1510.03336},
  urldate = {2023-01-25},
  abstract = {Much of the world's data is streaming, time-series data, where anomalies give significant information in critical situations; examples abound in domains such as finance, IT, security, medical, and energy. Yet detecting anomalies in streaming data is a difficult task, requiring detectors to process data in real-time, not batches, and learn while simultaneously making predictions. There are no benchmarks to adequately test and score the efficacy of real-time anomaly detectors. Here we propose the Numenta Anomaly Benchmark (NAB), which attempts to provide a controlled and repeatable environment of open-source tools to test and measure anomaly detection algorithms on streaming data. The perfect detector would detect all anomalies as soon as possible, trigger no false alarms, work with real-world time-series data across a variety of domains, and automatically adapt to changing statistics. Rewarding these characteristics is formalized in NAB, using a scoring algorithm designed for streaming data. NAB evaluates detectors on a benchmark dataset with labeled, real-world time-series data. We present these components, and give results and analyses for several open source, commercially-used algorithms. The goal for NAB is to provide a standard, open source framework with which the research community can compare and evaluate different algorithms for detecting anomalies in streaming data.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JS3SZPJV\\Lavin and Ahmad - 2015 - Evaluating Real-time Anomaly Detection Algorithms .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TE9DE3WL\\1510.html}
}

@article{lawSTUMPYPowerfulScalable2019,
  title = {{{STUMPY}}: {{A Powerful}} and {{Scalable Python Library}} for {{Time Series Data Mining}}},
  shorttitle = {{{STUMPY}}},
  author = {Law, Sean},
  date = {2019-07-18},
  journaltitle = {Journal of Open Source Software},
  volume = {4},
  pages = {1504},
  doi = {10.21105/joss.01504},
  file = {C:\Users\USEBPERP\Zotero\storage\HS7L7D43\Law - 2019 - STUMPY A Powerful and Scalable Python Library for.pdf}
}

@thesis{lazzeriArtificialIntelligenceDrug2018,
  title = {Artificial Intelligence in Drug Design: Generative Adversarial Network for Molecules Generation},
  shorttitle = {Artificial Intelligence in Drug Design},
  author = {Lazzeri, Isaac},
  date = {2018},
  institution = {Linz},
  url = {https://resolver.obvsg.at/urn:nbn:at:at-ubl:1-20991},
  urldate = {2018-09-17},
  abstract = {Generation of new chemical compounds plays a key role in drug discovery, but in-silico methods based on hand-crafted rules can only cover a tiny part of the synthetically available chemical space. Therefore computational methods able to automatically extract rules from data are desirable. The aim of this work is to adapt Generative Adversarial Networks (GANs) to generate novel chemical compounds.},
  langid = {english},
  keywords = {Algorithmisches Lernen,Artificial intelligence,Arzneimittel,Arzneimittelentwicklung,Automated learning,Computerunterstützte Intelligenz,Drug Development,Generative Adversarial Networks / GAN / Chemoinformatics / Deep Learning / Machine Learning / Drugs design / Neural networks,Hochschulschrift,Hochschulschrift ; Arzneimittelentwicklung ; Künstliche Intelligenz ; Maschinelles Lernen,KI,Künstliche Intelligenz,Lernen,Machine learning,Maschinelle Intelligenz,Maschinelles Lernen}
}

@unpublished{leclercTwoRegimesDeep2020,
  title = {The {{Two Regimes}} of {{Deep Network Training}}},
  author = {Leclerc, Guillaume and Madry, Aleksander},
  date = {2020-02-24},
  eprint = {2002.10376},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2002.10376},
  urldate = {2020-02-26},
  abstract = {Learning rate schedule has a major impact on the performance of deep learning models. Still, the choice of a schedule is often heuristical. We aim to develop a precise understanding of the effects of different learning rate schedules and the appropriate way to select them. To this end, we isolate two distinct phases of training, the first, which we refer to as the "large-step" regime, exhibits a rather poor performance from an optimization point of view but is the primary contributor to model generalization; the latter, "small-step" regime exhibits much more "convex-like" optimization behavior but used in isolation produces models that generalize poorly. We find that by treating these regimes separately-and em specializing our training algorithm to each one of them, we can significantly simplify learning rate schedules.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2LZ6TBIH\\Leclerc_Madry_2020_The Two Regimes of Deep Network Training.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\227X8VFK\\2002.html}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05},
  journaltitle = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  url = {https://www.nature.com/articles/nature14539},
  urldate = {2020-04-20},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  issue = {7553},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\525PZPHL\nature14539.html}
}

@article{lecunMNISTDATABASEHandwritten,
  title = {{{THE MNIST DATABASE}} of Handwritten Digits},
  author = {LECUN, Y.},
  journaltitle = {http://yann.lecun.com/exdb/mnist/},
  url = {https://ci.nii.ac.jp/naid/10027939599/},
  urldate = {2019-09-19},
  file = {C:\Users\USEBPERP\Zotero\storage\C6GSAUEH\10027939599.html}
}

@article{leeIndepthComparisonSubgraph2012,
  title = {An In-Depth Comparison of Subgraph Isomorphism Algorithms in Graph Databases},
  author = {Lee, Jinsoo and Han, Wook-Shin and Kasperovics, Romans and Lee, Jeong-Hoon},
  date = {2012-12},
  journaltitle = {Proc. VLDB Endow.},
  volume = {6},
  number = {2},
  pages = {133--144},
  issn = {2150-8097},
  doi = {10.14778/2535568.2448946},
  url = {https://dl.acm.org/doi/10.14778/2535568.2448946},
  urldate = {2021-06-02},
  abstract = {Finding subgraph isomorphisms is an important problem in many applications which deal with data modeled as graphs. While this problem is NP-hard, in recent years, many algorithms have been proposed to solve it in a reasonable time for real datasets using different join orders, pruning rules, and auxiliary neighborhood information. However, since they have not been empirically compared one another in most research work, it is not clear whether the later work outperforms the earlier work. Another problem is that reported comparisons were often done using the original authors’ binaries which were written in different programming environments. In this paper, we address these serious problems by re-implementing five state-of-the-art subgraph isomorphism algorithms in a common code base and by comparing them using many real-world datasets and their query loads. Through our in-depth analysis of experimental results, we report surprising empirical findings.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\9SN9F24R\Lee et al. - 2012 - An in-depth comparison of subgraph isomorphism alg.pdf}
}

@article{leeMolecularTransformerUnifies2019,
  ids = {a.leeMolecularTransformerUnifies2019a,lee2019moleculara},
  title = {Molecular {{Transformer}} Unifies Reaction Prediction and Retrosynthesis across Pharma Chemical Space},
  author = {Lee, Alpha A. and Yang, Qingyi and Sresht, Vishnu and Bolgar, Peter and Hou, Xinjun and L.~Klug-McLeod, Jacquelyn and R.~Butler, Christopher},
  date = {2019},
  journaltitle = {Chemical Communications},
  volume = {55},
  number = {81},
  pages = {12152--12155},
  publisher = {Royal Society of Chemistry},
  doi = {10.1039/C9CC05122H},
  url = {https://pubs.rsc.org/en/content/articlelanding/2019/cc/c9cc05122h},
  urldate = {2021-02-26},
  issue = {81},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\YJRWHJ4A\\A. Lee et al. - 2019 - Molecular Transformer unifies reaction prediction .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZK6ZQT6P\\A. Lee et al. - 2019 - Molecular Transformer unifies reaction prediction .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2UPMALMM\\c9cc05122h.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\5SWNVKWP\\c9cc05122h.html}
}

@inproceedings{leeReReLightweightRealtime2020,
  title = {{{ReRe}}: {{A Lightweight Real-time Ready-to-Go Anomaly Detection Approach}} for {{Time Series}}},
  shorttitle = {{{ReRe}}},
  booktitle = {2020 {{IEEE}} 44th {{Annual Computers}}, {{Software}}, and {{Applications Conference}} ({{COMPSAC}})},
  author = {Lee, Ming-Chang and Lin, Jia-Chun and Gran, Ernst Gunnar},
  date = {2020-07},
  eprint = {2004.02319},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {322--327},
  doi = {10.1109/COMPSAC48688.2020.0-226},
  url = {http://arxiv.org/abs/2004.02319},
  urldate = {2023-01-06},
  abstract = {Anomaly detection is an active research topic in many different fields such as intrusion detection, network monitoring, system health monitoring, IoT healthcare, etc. However, many existing anomaly detection approaches require either human intervention or domain knowledge, and may suffer from high computation complexity, consequently hindering their applicability in real-world scenarios. Therefore, a lightweight and ready-to-go approach that is able to detect anomalies in real-time is highly sought-after. Such an approach could be easily and immediately applied to perform time series anomaly detection on any commodity machine. The approach could provide timely anomaly alerts and by that enable appropriate countermeasures to be undertaken as early as possible. With these goals in mind, this paper introduces ReRe, which is a Real-time Ready-to-go proactive Anomaly Detection algorithm for streaming time series. ReRe employs two lightweight Long Short-Term Memory (LSTM) models to predict and jointly determine whether or not an upcoming data point is anomalous based on short-term historical data points and two long-term self-adaptive thresholds. Experiments based on real-world time-series datasets demonstrate the good performance of ReRe in real-time anomaly detection without requiring human intervention or domain knowledge.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\J7HFLI2G\\Lee et al. - 2020 - ReRe A Lightweight Real-time Ready-to-Go Anomaly .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZUYLXTR7\\2004.html}
}

@unpublished{leeRetCLSelectionbasedApproach2021,
  ids = {lee2020retcl},
  title = {{{RetCL}}: {{A Selection-based Approach}} for {{Retrosynthesis}} via {{Contrastive Learning}}},
  shorttitle = {{{RetCL}}},
  author = {Lee, Hankook and Ahn, Sungsoo and Seo, Seung-Woo and Song, You Young and Hwang, Sung-Ju and Yang, Eunho and Shin, Jinwoo},
  date = {2021-05-03},
  eprint = {2105.00795},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2105.00795},
  urldate = {2021-05-05},
  abstract = {Retrosynthesis, of which the goal is to find a set of reactants for synthesizing a target product, is an emerging research area of deep learning. While the existing approaches have shown promising results, they currently lack the ability to consider availability (e.g., stability or purchasability) of the reactants or generalize to unseen reaction templates (i.e., chemical reaction rules). In this paper, we propose a new approach that mitigates the issues by reformulating retrosynthesis into a selection problem of reactants from a candidate set of commercially available molecules. To this end, we design an efficient reactant selection framework, named RetCL (retrosynthesis via contrastive learning), for enumerating all of the candidate molecules based on selection scores computed by graph neural networks. For learning the score functions, we also propose a novel contrastive training scheme with hard negative mining. Extensive experiments demonstrate the benefits of the proposed selection-based approach. For example, when all 671k reactants in the USPTO \{database\} are given as candidates, our RetCL achieves top-1 exact match accuracy of \$71.3\textbackslash\%\$ for the USPTO-50k benchmark, while a recent transformer-based approach achieves \$59.6\textbackslash\%\$. We also demonstrate that RetCL generalizes well to unseen templates in various settings in contrast to template-based approaches.},
  keywords = {Computer Science - Machine Learning}
}

@unpublished{leeTrainingConfidencecalibratedClassifiers2017,
  title = {Training {{Confidence-calibrated Classifiers}} for {{Detecting Out-of-Distribution Samples}}},
  author = {Lee, Kimin and Lee, Honglak and Lee, Kibok and Shin, Jinwoo},
  date = {2017-11-25},
  eprint = {1711.09325},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1711.09325},
  urldate = {2019-04-30},
  abstract = {The problem of detecting whether a test sample is from in-distribution (i.e., training distribution by a classifier) or out-of-distribution sufficiently different from it arises in many real-world machine learning applications. However, the state-of-art deep neural networks are known to be highly overconfident in their predictions, i.e., do not distinguish in- and out-of-distributions. Recently, to handle this issue, several threshold-based detectors have been proposed given pre-trained neural classifiers. However, the performance of prior works highly depends on how to train the classifiers since they only focus on improving inference procedures. In this paper, we develop a novel training method for classifiers so that such inference algorithms can work better. In particular, we suggest two additional terms added to the original loss (e.g., cross entropy). The first one forces samples from out-of-distribution less confident by the classifier and the second one is for (implicitly) generating most effective training samples for the first one. In essence, our method jointly trains both classification and generative neural networks for out-of-distribution. We demonstrate its effectiveness using deep convolutional neural networks on various popular image datasets.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\FC4EV5BY\\Lee et al_2017_Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\445EF9MM\\1711.html}
}

@unpublished{leeWideNeuralNetworks2019,
  title = {Wide {{Neural Networks}} of {{Any Depth Evolve}} as {{Linear Models Under Gradient Descent}}},
  author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S. and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  date = {2019-02-18},
  eprint = {1902.06720},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.06720},
  urldate = {2019-02-20},
  abstract = {A longstanding goal in deep learning research has been to precisely characterize training and generalization. However, the often complex loss landscapes of neural networks have made a theory of learning dynamics elusive. In this work, we show that for wide neural networks the learning dynamics simplify considerably and that, in the infinite width limit, they are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters. Furthermore, mirroring the correspondence between wide Bayesian neural networks and Gaussian processes, gradient-based training of wide neural networks with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel. While these theoretical results are only exact in the infinite width limit, we nevertheless find excellent empirical agreement between the predictions of the original network and those of the linearized version even for finite practically-sized networks. This agreement is robust across different architectures, optimization methods, and loss functions.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\W52HN63M\\Lee et al_2019_Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\9NPNFAJZ\\1902.html}
}

@unpublished{lehmanSurprisingCreativityDigital2019,
  ids = {lehman2018surprising},
  title = {The {{Surprising Creativity}} of {{Digital Evolution}}: {{A Collection}} of {{Anecdotes}} from the {{Evolutionary Computation}} and {{Artificial Life Research Communities}}},
  shorttitle = {The {{Surprising Creativity}} of {{Digital Evolution}}},
  author = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J. and Bernard, Samuel and Beslon, Guillaume and Bryson, David M. and Chrabaszcz, Patryk and Cheney, Nick and Cully, Antoine and Doncieux, Stephane and Dyer, Fred C. and Ellefsen, Kai Olav and Feldt, Robert and Fischer, Stephan and Forrest, Stephanie and Frénoy, Antoine and Gagné, Christian and Goff, Leni Le and Grabowski, Laura M. and Hodjat, Babak and Hutter, Frank and Keller, Laurent and Knibbe, Carole and Krcah, Peter and Lenski, Richard E. and Lipson, Hod and MacCurdy, Robert and Maestre, Carlos and Miikkulainen, Risto and Mitri, Sara and Moriarty, David E. and Mouret, Jean-Baptiste and Nguyen, Anh and Ofria, Charles and Parizeau, Marc and Parsons, David and Pennock, Robert T. and Punch, William F. and Ray, Thomas S. and Schoenauer, Marc and Shulte, Eric and Sims, Karl and Stanley, Kenneth O. and Taddei, François and Tarapore, Danesh and Thibault, Simon and Weimer, Westley and Watson, Richard and Yosinski, Jason},
  date = {2019-11-21},
  eprint = {1803.03453},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1803.03453},
  urldate = {2020-04-20},
  abstract = {Biological evolution provides a creative fount of complex and subtle adaptations, often surprising the scientists who discover them. However, because evolution is an algorithmic process that transcends the substrate in which it occurs, evolution's creativity is not limited to nature. Indeed, many researchers in the field of digital evolution have observed their evolving algorithms and organisms subverting their intentions, exposing unrecognized bugs in their code, producing unexpected adaptations, or exhibiting outcomes uncannily convergent with ones in nature. Such stories routinely reveal creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2EFZHE3P\\Lehman et al_2018_The Surprising Creativity of Digital Evolution.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\33MBQ3FD\\Lehman et al_2019_The Surprising Creativity of Digital Evolution.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AJ4DXVJ9\\Lehman et al_2019_The Surprising Creativity of Digital Evolution.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7CYIP4N3\\1803.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\QRG4LQHB\\1803.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\WLEXI74L\\1803.html}
}

@unpublished{leiSimpleRecurrentUnits2017,
  title = {Simple {{Recurrent Units}} for {{Highly Parallelizable Recurrence}}},
  author = {Lei, Tao and Zhang, Yu and Wang, Sida I. and Dai, Hui and Artzi, Yoav},
  date = {2017-09-08},
  eprint = {1709.02755},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1709.02755},
  urldate = {2019-01-16},
  abstract = {Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model on translation by incorporating SRU into the architecture.},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\TJ4566YA\\Lei et al_2017_Simple Recurrent Units for Highly Parallelizable Recurrence.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AJHE32CP\\1709.html}
}

@online{leNavigatingDesignSpace2023,
  title = {Navigating the {{Design Space}} of {{Equivariant Diffusion-Based Generative Models}} for {{De Novo 3D Molecule Generation}}},
  author = {Le, Tuan and Cremer, Julian and Noé, Frank and Clevert, Djork-Arné and Schütt, Kristof},
  date = {2023-09-29},
  eprint = {2309.17296},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2309.17296},
  urldate = {2023-10-03},
  abstract = {Deep generative diffusion models are a promising avenue for de novo 3D molecular design in material science and drug discovery. However, their utility is still constrained by suboptimal performance with large molecular structures and limited training data. Addressing this gap, we explore the design space of E(3) equivariant diffusion models, focusing on previously blank spots. Our extensive comparative analysis evaluates the interplay between continuous and discrete state spaces. Out of this investigation, we introduce the EQGAT-diff model, which consistently surpasses the performance of established models on the QM9 and GEOM-Drugs datasets by a large margin. Distinctively, EQGAT-diff takes continuous atomic positions while chemical elements and bond types are categorical and employ a time-dependent loss weighting that significantly increases training convergence and the quality of generated samples. To further strengthen the applicability of diffusion models to limited training data, we examine the transferability of EQGAT-diff trained on the large PubChem3D dataset with implicit hydrogens to target distributions with explicit hydrogens. Fine-tuning EQGAT-diff for a couple of iterations further pushes state-of-the-art performance across datasets. We envision that our findings will find applications in structure-based drug design, where the accuracy of generative models for small datasets of complex molecules is critical.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning}
}

@article{levineGuidedPolicySearch,
  title = {Guided {{Policy Search}}},
  author = {Levine, Sergey and Koltun, Vladlen},
  pages = {10},
  abstract = {Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and often falling into poor local optima. We present a guided policy search algorithm that uses trajectory optimization to direct policy learning and avoid poor local optima. We show how differential dynamic programming can be used to generate suitable guiding samples, and describe a regularized importance sampled policy optimization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\5MUCF38N\Levine and Koltun - Guided Policy Search.pdf}
}

@article{levittWhyAreGambling2004,
  title = {Why Are {{Gambling Markets Organised}} so {{Differently}} from {{Financial Markets}}?},
  author = {Levitt, Steven D.},
  date = {2004-04-01},
  journaltitle = {The Economic Journal},
  volume = {114},
  number = {495},
  pages = {223--246},
  issn = {0013-0133, 1468-0297},
  doi = {10.1111/j.1468-0297.2004.00207.x},
  url = {https://academic.oup.com/ej/article/114/495/223-246/5086012},
  urldate = {2022-09-12},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\ZJ2EU423\Levitt - 2004 - Why are Gambling Markets Organised so Differently .pdf}
}

@book{lewarsModelingMarvelsComputational2008,
  title = {Modeling {{Marvels}}: {{Computational Anticipation}} of {{Novel Molecules}}},
  shorttitle = {Modeling {{Marvels}}},
  author = {Lewars, Errol G.},
  date = {2008},
  publisher = {Springer Netherlands},
  doi = {10.1007/978-1-4020-6973-4},
  url = {https://www.springer.com/gp/book/9781402069727},
  urldate = {2020-09-03},
  abstract = {The aim of this book is to survey a number of chemical compounds that chemists, both theoretical and experimental, find fascinating. Some of these compounds, like planar carbon species or oxirene, offer no obvious practical applications; nitrogen oligomers and polymers, in contrast, have been touted as possible high-energy-density materials. What unites this otherwise eclectic collection is that these substances are unknown and offer a challenge to theory and to synthesis. It is envisioned that this collection of idiosynchractic molecules will appeal to chemists who find the study of chemical oddities interesting and, on occasion, even rewarding. "A great romp through imagined molecules, a challenge to the talents of synthetic chemists! Errol Lewars leads us expertly through a wonderland of the chemical imagination, fascinating molecular structures that do not (yet) exist!"Prof. Roald Hoffmann - Nobel Laureate, Chem. 1981- Cornell University, New York, USA "This book is an educational and enjoyable read, devoted to species on the fringes of chemical, calculation and conceptual plausibility"Prof. Joel Liebman, University of Maryland, Baltimore County, USA},
  isbn = {978-1-4020-6972-7},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\PJ25MK9S\9781402069727.html}
}

@unpublished{liangEnhancingReliabilityOutofdistribution2017,
  title = {Enhancing {{The Reliability}} of {{Out-of-distribution Image Detection}} in {{Neural Networks}}},
  author = {Liang, Shiyu and Li, Yixuan and Srikant, R.},
  date = {2017-06-08},
  eprint = {1706.02690},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1706.02690},
  urldate = {2019-09-10},
  abstract = {We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7\% to 4.3\% on the DenseNet (applied to CIFAR-10) when the true positive rate is 95\%.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\X88GBYS5\\Liang et al_2017_Enhancing The Reliability of Out-of-distribution Image Detection in Neural.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\IG7WF6V6\\1706.html}
}

@incollection{liCombinatorialOptimizationGraph2018,
  title = {Combinatorial {{Optimization}} with {{Graph Convolutional Networks}} and {{Guided Tree Search}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Li, Zhuwen and Chen, Qifeng and Koltun, Vladlen},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {539--548},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/7335-combinatorial-optimization-with-graph-convolutional-networks-and-guided-tree-search.pdf},
  urldate = {2019-03-19},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\VLBI46SR\\Li et al_2018_Combinatorial Optimization with Graph Convolutional Networks and Guided Tree.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6Q4IZUCM\\7335-combinatorial-optimization-with-graph-convolutional-networks-and-guided-tree-search.html}
}

@article{liECODUnsupervisedOutlier2022,
  title = {{{ECOD}}: {{Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions}}},
  shorttitle = {{{ECOD}}},
  author = {Li, Zheng and Zhao, Yue and Hu, Xiyang and Botta, Nicola and Ionescu, Cezar and Chen, George H.},
  date = {2022},
  journaltitle = {IEEE Trans. Knowl. Data Eng.},
  eprint = {2201.00382},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {1--1},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2022.3159580},
  url = {http://arxiv.org/abs/2201.00382},
  urldate = {2023-03-17},
  abstract = {Outlier detection refers to the identification of data points that deviate from a general data distribution. Existing unsupervised approaches often suffer from high computational cost, complex hyperparameter tuning, and limited interpretability, especially when working with large, high-dimensional datasets. To address these issues, we present a simple yet effective algorithm called ECOD (Empirical-Cumulative-distribution-based Outlier Detection), which is inspired by the fact that outliers are often the "rare events" that appear in the tails of a distribution. In a nutshell, ECOD first estimates the underlying distribution of the input data in a nonparametric fashion by computing the empirical cumulative distribution per dimension of the data. ECOD then uses these empirical distributions to estimate tail probabilities per dimension for each data point. Finally, ECOD computes an outlier score of each data point by aggregating estimated tail probabilities across dimensions. Our contributions are as follows: (1) we propose a novel outlier detection method called ECOD, which is both parameter-free and easy to interpret; (2) we perform extensive experiments on 30 benchmark datasets, where we find that ECOD outperforms 11 state-of-the-art baselines in terms of accuracy, efficiency, and scalability; and (3) we release an easy-to-use and scalable (with distributed support) Python implementation for accessibility and reproducibility.},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\GTDPSNRP\\Li et al. - 2022 - ECOD Unsupervised Outlier Detection Using Empiric.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\MJR89Z39\\Li et al. - 2022 - ECOD Unsupervised Outlier Detection Using Empiric.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\S967IDFV\\Li et al. - 2022 - ECOD Unsupervised Outlier Detection Using Empiric.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\38W3X25I\\2201.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\44Y9RT44\\2201.html}
}

@incollection{liExplainingRegularizationEffect2019,
  title = {Towards {{Explaining}} the {{Regularization Effect}} of {{Initial Large Learning Rate}} in {{Training Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Alché-Buc, given=F., prefix=d\textbackslash textquotesingle, useprefix=false and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {11669--11680},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/9341-towards-explaining-the-regularization-effect-of-initial-large-learning-rate-in-training-neural-networks.pdf},
  urldate = {2020-02-03},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\85FV7FHK\\Li et al_2019_Towards Explaining the Regularization Effect of Initial Large Learning Rate in.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\U7QPJN3H\\9341-towards-explaining-the-regularization-effect-of-initial-large-learning-rate-in-training-ne.html}
}

@unpublished{liLearningDeepGenerative2018,
  title = {Learning {{Deep Generative Models}} of {{Graphs}}},
  author = {Li, Yujia and Vinyals, Oriol and Dyer, Chris and Pascanu, Razvan and Battaglia, Peter},
  date = {2018-03-08},
  eprint = {1803.03324},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.03324},
  urldate = {2019-05-06},
  abstract = {Graphs are fundamental data structures which concisely capture the relational structure in many important real-world domains, such as knowledge graphs, physical and social interactions, language, and chemistry. Here we introduce a powerful new approach for learning generative models over graphs, which can capture both their structure and attributes. Our approach uses graph neural networks to express probabilistic dependencies among a graph's nodes and edges, and can, in principle, learn distributions over any arbitrary graph. In a series of experiments our results show that once trained, our models can generate good quality samples of both synthetic graphs as well as real molecular graphs, both unconditionally and conditioned on data. Compared to baselines that do not use graph-structured representations, our models often perform far better. We also explore key challenges of learning generative models of graphs, such as how to handle symmetries and ordering of elements during the graph generation process, and offer possible solutions. Our work is the first and most general approach for learning generative models over arbitrary graphs, and opens new directions for moving away from restrictions of vector- and sequence-like knowledge representations, toward more expressive and flexible relational data structures.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\F92ZFQGY\\Li et al_2018_Learning Deep Generative Models of Graphs2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JKTDARQD\\Li et al_2018_Learning Deep Generative Models of Graphs.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8REYMX9W\\1803.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\KQKJ7XQN\\1803.html}
}

@unpublished{liLearningGraphLevelRepresentation2017,
  title = {Learning {{Graph-Level Representation}} for {{Drug Discovery}}},
  author = {Li, Junying and Cai, Deng and He, Xiaofei},
  date = {2017-09-12},
  eprint = {1709.03741},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1709.03741},
  urldate = {2018-09-17},
  abstract = {Predicating macroscopic influences of drugs on human body, like efficacy and toxicity, is a central problem of small-molecule based drug discovery. Molecules can be represented as an undirected graph, and we can utilize graph convolution networks to predication molecular properties. However, graph convolutional networks and other graph neural networks all focus on learning node-level representation rather than graph-level representation. Previous works simply sum all feature vectors for all nodes in the graph to obtain the graph feature vector for drug predication. In this paper, we introduce a dummy super node that is connected with all nodes in the graph by a directed edge as the representation of the graph and modify the graph operation to help the dummy super node learn graph-level feature. Thus, we can handle graph-level classification and regression in the same way as node-level classification and regression. In addition, we apply focal loss to address class imbalance in drug datasets. The experiments on MoleculeNet show that our method can effectively improve the performance of molecular properties predication.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NUF7SRUK\\Li et al_2017_Learning Graph-Level Representation for Drug Discovery.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DA556HTR\\1709.html}
}

@article{liLecture14Reinforcement,
  title = {Lecture 14: {{Reinforcement Learning}}},
  author = {Li, Fei-Fei and Johnson, Justin and Yeung, Serena},
  pages = {103},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\FUCCNZCU\Li et al_Lecture 14.pdf}
}

@online{liMADGANMultivariateAnomaly2019,
  title = {{{MAD-GAN}}: {{Multivariate Anomaly Detection}} for {{Time Series Data}} with {{Generative Adversarial Networks}}},
  shorttitle = {{{MAD-GAN}}},
  author = {Li, Dan and Chen, Dacheng and Shi, Lei and Jin, Baihong and Goh, Jonathan and Ng, See-Kiong},
  date = {2019-01-15},
  eprint = {1901.04997},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1901.04997},
  url = {http://arxiv.org/abs/1901.04997},
  urldate = {2023-01-06},
  abstract = {The prevalence of networked sensors and actuators in many real-world systems such as smart buildings, factories, power plants, and data centers generate substantial amounts of multivariate time series data for these systems. The rich sensor data can be continuously monitored for intrusion events through anomaly detection. However, conventional threshold-based anomaly detection methods are inadequate due to the dynamic complexities of these systems, while supervised machine learning methods are unable to exploit the large amounts of data due to the lack of labeled data. On the other hand, current unsupervised machine learning approaches have not fully exploited the spatial-temporal correlation and other dependencies amongst the multiple variables (sensors/actuators) in the system for detecting anomalies. In this work, we propose an unsupervised multivariate anomaly detection method based on Generative Adversarial Networks (GANs). Instead of treating each data stream independently, our proposed MAD-GAN framework considers the entire variable set concurrently to capture the latent interactions amongst the variables. We also fully exploit both the generator and discriminator produced by the GAN, using a novel anomaly score called DR-score to detect anomalies by discrimination and reconstruction. We have tested our proposed MAD-GAN using two recent datasets collected from real-world CPS: the Secure Water Treatment (SWaT) and the Water Distribution (WADI) datasets. Our experimental results showed that the proposed MAD-GAN is effective in reporting anomalies caused by various cyber-intrusions compared in these complex real-world systems.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\Y7HGVTW2\\Li et al. - 2019 - MAD-GAN Multivariate Anomaly Detection for Time S.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\5GGA6GBV\\1901.html}
}

@article{liMakingBetterDecisions2019,
  title = {Making Better Decisions during Synthetic Route Design: Leveraging Prediction to Achieve Greenness-by-Design},
  shorttitle = {Making Better Decisions during Synthetic Route Design},
  author = {Li, Jun and D.~Eastgate, Martin},
  date = {2019},
  journaltitle = {Reaction Chemistry \& Engineering},
  volume = {4},
  number = {9},
  pages = {1595--1607},
  doi = {10.1039/C9RE00019D},
  url = {https://pubs.rsc.org/en/content/articlelanding/2019/re/c9re00019d},
  urldate = {2019-10-02},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\IY3B9PR6\\Li_D. Eastgate_2019_Making better decisions during synthetic route design.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WDL5V93F\\c9re00019d.html}
}

@article{limMolecularGenerativeModel2018,
  title = {Molecular Generative Model Based on Conditional Variational Autoencoder for de Novo Molecular Design},
  author = {Lim, Jaechang and Ryu, Seongok and Kim, Jin Woo and Kim, Woo Youn},
  date = {2018-07-11},
  journaltitle = {Journal of Cheminformatics},
  volume = {10},
  number = {1},
  pages = {31},
  issn = {1758-2946},
  doi = {10.1186/s13321-018-0286-7},
  url = {https://doi.org/10.1186/s13321-018-0286-7},
  urldate = {2019-05-06},
  abstract = {We propose a molecular generative model based on the conditional variational autoencoder for de novo molecular design. It is specialized to control multiple molecular properties simultaneously by imposing them on a latent space. As a proof of concept, we demonstrate that it can be used to generate drug-like molecules with five target properties. We were also able to adjust a single property without changing the others and to manipulate it beyond the range of the dataset.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\8E85M6MT\\Lim et al_2018_Molecular generative model based on conditional variational autoencoder for de.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\NHNIBXB6\\Lim et al_2018_Molecular generative model based on conditional variational autoencoder for de2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SA6USGFK\\Lim et al. - 2018 - Molecular generative model based on conditional va.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZNUAY2GS\\s13321-018-0286-7.html}
}

@article{liMultiobjectiveNovoDrug2018,
  title = {Multi-Objective de Novo Drug Design with Conditional Graph Generative Model},
  author = {Li, Yibo and Zhang, Liangren and Liu, Zhenming},
  date = {2018-07-24},
  journaltitle = {Journal of Cheminformatics},
  volume = {10},
  number = {1},
  pages = {33},
  issn = {1758-2946},
  doi = {10.1186/s13321-018-0287-6},
  url = {https://doi.org/10.1186/s13321-018-0287-6},
  urldate = {2023-10-29},
  abstract = {Recently, deep generative models have revealed itself as a promising way of performing de novo molecule design. However, previous research has focused mainly on generating SMILES strings instead of molecular graphs. Although available, current graph generative models are are often too general and computationally expensive. In this work, a new de novo molecular design framework is proposed based on a type of sequential graph generators that do not use atom level recurrent units. Compared with previous graph generative models, the proposed method is much more tuned for molecule generation and has been scaled up to cover significantly larger molecules in the ChEMBL database. It is shown that the graph-based model outperforms SMILES based models in a variety of metrics, especially in the rate of valid outputs. For the application of drug design tasks, conditional graph generative model is employed. This method offers highe flexibility and is suitable for generation based on multiple objectives. The results have demonstrated that this approach can be effectively applied to solve several drug design problems, including the generation of compounds containing a given scaffold, compounds with specific drug-likeness and synthetic accessibility requirements, as well as dual inhibitors against JNK3 and GSK-3β.},
  keywords = {De novo drug design,Deep learning,Graph generative model},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\APFWEAC4\\Li et al. - 2018 - Multi-objective de novo drug design with condition.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\CJ6TAAM2\\s13321-018-0287-6.html}
}

@article{liMultivariateTimeSeries2017,
  title = {Multivariate Time Series Anomaly Detection: {{A}} Framework of {{Hidden Markov Models}}},
  shorttitle = {Multivariate Time Series Anomaly Detection},
  author = {Li, Jinbo and Pedrycz, Witold and Jamal, Iqbal},
  date = {2017-11-01},
  journaltitle = {Applied Soft Computing},
  volume = {60},
  pages = {229--240},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2017.06.035},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494617303782},
  urldate = {2023-01-25},
  abstract = {In this study, we develop an approach to multivariate time series anomaly detection focused on the transformation of multivariate time series to univariate time series. Several transformation techniques involving Fuzzy C-Means (FCM) clustering and fuzzy integral are studied. In the sequel, a Hidden Markov Model (HMM), one of the commonly encountered statistical methods, is engaged here to detect anomalies in multivariate time series. We construct HMM-based anomaly detectors and in this context compare several transformation methods. A suite of experimental studies along with some comparative analysis is reported.},
  langid = {english},
  keywords = {Anomaly detection,Fuzzy C-means,Fuzzy integral,Hidden Markov Model (HMM),Multivariate time series},
  file = {C:\Users\USEBPERP\Zotero\storage\UEKJGBV5\S1568494617303782.html}
}

@inproceedings{lingsongzhangMultiResolutionAnomalyDetection2008,
  title = {Multi-{{Resolution Anomaly Detection}} for the Internet},
  booktitle = {{{IEEE INFOCOM}} 2008 - {{IEEE Conference}} on {{Computer Communications Workshops}}},
  author = {{Lingsong Zhang} and {Zhengyuan Zhu} and Jeffay, Kevin and Marron, J. S. and Smith, F. Donelson},
  date = {2008-04},
  pages = {1--6},
  publisher = {IEEE},
  location = {Phoenix, Arizona, USA},
  doi = {10.1109/INFOCOM.2008.4544618},
  url = {http://ieeexplore.ieee.org/document/4544618/},
  urldate = {2023-01-06},
  abstract = {In the context of Internet traffic anomaly detection, we will show that some outliers in a time series can be difficult to detect at one scale while they are easy to find at another scale. In this paper, we develop an outlier detection method for a time series with long range dependence, and conclude that testing outliers at multiple time scales helps to reveal them. We present a Multi-Resolution Anomaly Detection (MRAD) procedure for detecting network anomalies. We show that the MRAD method is useful, especially when outliers appear as a slight local mean level shift with a rather long duration, e.g., as generated by a port scan. A novel MRAD outlier map is proposed to visualize the location of the outliers, and also to suggest the significance probabilities (p values) for them.},
  eventtitle = {{{IEEE INFOCOM}} 2008 - {{IEEE Conference}} on {{Computer Communications Workshops}}},
  isbn = {978-1-4244-2219-7},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\846Y6AWM\Lingsong Zhang et al. - 2008 - Multi-Resolution Anomaly Detection for the interne.pdf}
}

@article{liParticleFilteringBased2001,
  title = {Particle Filtering Based Likelihood Ratio Approach to Fault Diagnosis in Nonlinear Stochastic Systems},
  author = {Li, Ping and Kadirkamanathan, V.},
  date = {2001-08},
  journaltitle = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  volume = {31},
  number = {3},
  pages = {337--343},
  issn = {1558-2442},
  doi = {10.1109/5326.971661},
  abstract = {This paper presents the development of a particle filtering (PF) based method for fault detection and isolation (FDI) in stochastic nonlinear dynamic systems. The FDI problem is formulated in the multiple model (MM) environment, then by combining the likelihood ratio (LR) test with the PF, a new FDI scheme is developed. The simulation results on a highly nonlinear system are provided which demonstrate the effectiveness of the proposed method.},
  eventtitle = {{{IEEE Transactions}} on {{Systems}}, {{Man}}, and {{Cybernetics}}, {{Part C}} ({{Applications}} and {{Reviews}})},
  keywords = {Analytical models,Fault detection,Fault diagnosis,Filtering,Gaussian noise,Mathematical model,Monitoring,Nonlinear dynamical systems,Nonlinear systems,Stochastic systems},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6FLR8V8B\\Li and Kadirkamanathan - 2001 - Particle filtering based likelihood ratio approach.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\R8KMKDZD\\stamp.html}
}

@article{litsaMachineLearningGuided2019,
  title = {Machine {{Learning Guided Atom Mapping}} of {{Metabolic Reactions}}},
  author = {Litsa, Eleni E. and Peña, Matthew I. and Moll, Mark and Giannakopoulos, George and Bennett, George N. and Kavraki, Lydia E.},
  date = {2019-03-25},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {59},
  number = {3},
  pages = {1121--1135},
  issn = {1549-9596, 1549-960X},
  doi = {10.1021/acs.jcim.8b00434},
  url = {https://pubs.acs.org/doi/10.1021/acs.jcim.8b00434},
  urldate = {2019-09-27},
  abstract = {Atom mapping of a chemical reaction is a mapping between the atoms in the reactant molecules and the atoms in the product molecules. It encodes the underlying reaction mechanism and, as such, constitutes essential information in computational studies in drug design. Various techniques have been investigated for the automatic computation of the atom mapping of a chemical reaction, approaching the problem as a graph matching problem. The graph abstraction of the chemical problem, though, eliminates crucial chemical information. There have been efforts for enhancing the graph representation by introducing the bond stabilities as edge weights, as they are estimated based on experimental evidence. Here, we present a fully automated optimization-based approach, named AMLGAM (Automated Machine Learning Guided Atom Mapping), that uses machine learning techniques for the estimation of the bond stabilities based on the chemical environment of each bond. The optimization method finds the reaction mechanism which favors the breakage/formation of the less stable bonds. We evaluated our method on a manually curated data set of 382 chemical reactions and ran our method on a much larger and diverse data set of 7400 chemical reactions. We show that the proposed method improves the accuracy over existing techniques based on results published by earlier studies on a common data set and is capable of handling unbalanced reactions.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\FMUZXZPD\Litsa et al. - 2019 - Machine Learning Guided Atom Mapping of Metabolic .pdf}
}

@article{liuAnomalyDetectionTime2019,
  title = {Anomaly Detection for Time Series Using Temporal Convolutional Networks and {{Gaussian}} Mixture Model},
  author = {Liu, Jianwei and Zhu, Hongwei and Liu, Yongxia and Wu, Haobo and Lan, Yunsheng and Zhang, Xinyu},
  date = {2019-04},
  journaltitle = {J. Phys.: Conf. Ser.},
  volume = {1187},
  number = {4},
  pages = {042111},
  issn = {1742-6588, 1742-6596},
  doi = {10.1088/1742-6596/1187/4/042111},
  url = {https://iopscience.iop.org/article/10.1088/1742-6596/1187/4/042111},
  urldate = {2023-01-25},
  abstract = {Anomaly detection, as an important research field in the analysis of time series, has practical and significant applications in many occasions, such as network security, medical health, Internet of Things (IoT), fault diagnosis and so on. However, due to the inherent characteristics of time series, such as tremendous data volumes, the imbalance of normal data and abnormal data, additional constraints and challenges are added for anomaly detection for time series. We present a novel anomaly detection framework, which applies temporal convolutional networks to extract features of time series and combined Gaussian mixture model with Bayesian inference to detect anomalies of systems. In order to evaluate the effectiveness of our approach, experiments are carried out on two typical time series datasets including EEG dataset and current dataset of electrical equipment. The experiments indicate that temporal convolutional network can contribute to extracting salient features of time series and Gaussian mixture model with Bayesian inference has good generalization and reliability for anomaly detection. Meanwhile, the designed architecture and analysis approach of anomaly detection reveal the method’s effectiveness and generalization in the feature extraction and anomaly detection for other time series.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\TXIKQV88\Liu et al. - 2019 - Anomaly detection for time series using temporal c.pdf}
}

@incollection{liuConstrainedGraphVariational2018,
  title = {Constrained {{Graph Variational Autoencoders}} for {{Molecule Design}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Liu, Qi and Allamanis, Miltiadis and Brockschmidt, Marc and Gaunt, Alexander},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  date = {2018},
  pages = {7795--7804},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/8005-constrained-graph-variational-autoencoders-for-molecule-design.pdf},
  urldate = {2019-05-06},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\Q6V3PJG2\\Liu et al_2018_Constrained Graph Variational Autoencoders for Molecule Design2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8UQFKTAV\\8005-constrained-graph-variational-autoencoders-for-molecule-design.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\QY3J5GQ7\\8005-constrained-graph-variational-autoencoders-for-molecule-design.html}
}

@article{liuExplorationStrategyImproves2019,
  title = {An Exploration Strategy Improves the Diversity of de Novo Ligands Using Deep Reinforcement Learning: A Case for the Adenosine {{A2A}} Receptor},
  shorttitle = {An Exploration Strategy Improves the Diversity of de Novo Ligands Using Deep Reinforcement Learning},
  author = {Liu, Xuhan and Ye, Kai and family=Vlijmen, given=Herman W. T., prefix=van, useprefix=true and IJzerman, Adriaan P. and family=Westen, given=Gerard J. P., prefix=van, useprefix=true},
  date = {2019-05-24},
  journaltitle = {Journal of Cheminformatics},
  volume = {11},
  number = {1},
  pages = {35},
  issn = {1758-2946},
  doi = {10.1186/s13321-019-0355-6},
  url = {https://doi.org/10.1186/s13321-019-0355-6},
  urldate = {2020-07-23},
  abstract = {Over the last 5~years deep learning has progressed tremendously in both image recognition and natural language processing. Now it is increasingly applied to other data rich fields. In drug discovery, recurrent neural networks (RNNs) have been shown to be an effective method to generate novel chemical structures in the form of SMILES. However, ligands generated by current methods have so far provided relatively low diversity and do not fully cover the whole chemical space occupied by known ligands. Here, we propose a new method (DrugEx) to discover de novo drug-like molecules. DrugEx is an RNN model (generator) trained through reinforcement learning which was integrated with a special exploration strategy. As a case study we applied our method to design ligands against the adenosine A2A receptor. From ChEMBL data, a machine learning model (predictor) was created to predict whether generated molecules are active or not. Based on this predictor as the reward function, the generator was trained by reinforcement learning without any further data. We then compared the performance of our method with two previously published methods, REINVENT and ORGANIC. We found that candidate molecules our model designed, and predicted to be active, had a larger chemical diversity and better covered the chemical space of known ligands compared to the state-of-the-art.},
  keywords = {Adenosine receptors,Cheminformatics,Deep learning,Exploration strategy,Reinforcement learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\5AG5WQTK\\Liu et al. - 2019 - An exploration strategy improves the diversity of .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\9JPKMWAK\\Liu et al_2019_An exploration strategy improves the diversity of de novo ligands using deep.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\P3W39A8C\\Liu et al. - 2019 - An exploration strategy improves the diversity of .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QN4ZGJIJ\\Liu et al. - 2019 - An exploration strategy improves the diversity of .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YUE3FGQ7\\Liu et al. - 2019 - An exploration strategy improves the diversity of .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\F2SLM9EX\\s13321-019-0355-6.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\T3APPUZY\\s13321-019-0355-6.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\T5I8RASJ\\s13321-019-0355-6.html}
}

@inproceedings{liuIsolationForest2008,
  title = {Isolation {{Forest}}},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  date = {2008-12},
  pages = {413--422},
  publisher = {IEEE},
  location = {Pisa, Italy},
  doi = {10.1109/ICDM.2008.17},
  url = {http://ieeexplore.ieee.org/document/4781136/},
  urldate = {2023-04-11},
  abstract = {Most existing model-based approaches to anomaly detection construct a profile of normal instances, then identify instances that do not conform to the normal profile as anomalies. This paper proposes a fundamentally different model-based method that explicitly isolates anomalies instead of profiles normal points. To our best knowledge, the concept of isolation has not been explored in current literature. The use of isolation enables the proposed method, iForest, to exploit sub-sampling to an extent that is not feasible in existing methods, creating an algorithm which has a linear time complexity with a low constant and a low memory requirement. Our empirical evaluation shows that iForest performs favourably to ORCA, a near-linear time complexity distance-based method, LOF and Random Forests in terms of AUC and processing time, and especially in large data sets. iForest also works well in high dimensional problems which have a large number of irrelevant attributes, and in situations where training set does not contain any anomalies.},
  eventtitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}} ({{ICDM}})},
  isbn = {978-0-7695-3502-9},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\8KDIUMZJ\Liu et al. - 2008 - Isolation Forest.pdf}
}

@article{liuRetrosyntheticReactionPrediction2017,
  ids = {liu2017retrosynthetica,liu2017retrosyntheticb,liuretrosynthetic},
  title = {Retrosynthetic {{Reaction Prediction Using Neural Sequence-to-Sequence Models}}},
  author = {Liu, Bowen and Ramsundar, Bharath and Kawthekar, Prasad and Shi, Jade and Gomes, Joseph and Luu Nguyen, Quang and Ho, Stephen and Sloane, Jack and Wender, Paul and Pande, Vijay},
  date = {2017-10-25},
  journaltitle = {ACS Cent. Sci.},
  volume = {3},
  number = {10},
  eprint = {1706.01643},
  eprinttype = {arxiv},
  pages = {1103--1113},
  publisher = {American Chemical Society},
  issn = {2374-7943},
  doi = {10.1021/acscentsci.7b00303},
  url = {https://doi.org/10.1021/acscentsci.7b00303},
  urldate = {2021-04-08},
  abstract = {We describe a fully data driven model that learns to perform a retrosynthetic reaction prediction task, which is treated as a sequence-to-sequence mapping problem. The end-to-end trained model has an encoder–decoder architecture that consists of two recurrent neural networks, which has previously shown great success in solving other sequence-to-sequence prediction tasks such as machine translation. The model is trained on 50,000 experimental reaction examples from the United States patent literature, which span 10 broad reaction types that are commonly used by medicinal chemists. We find that our model performs comparably with a rule-based expert system baseline model, and also overcomes certain limitations associated with rule-based expert systems and with any machine learning approach that contains a rule-based expert system component. Our model provides an important first step toward solving the challenging problem of computational retrosynthetic analysis.},
  issue = {10},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JLWBD6S6\\Liu et al. - 2017 - Retrosynthetic Reaction Prediction Using Neural Se.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\Z96ZQUJ5\\acscentsci.html}
}

@unpublished{liuSequenceModelingTemporal2019,
  title = {Sequence {{Modeling}} of {{Temporal Credit Assignment}} for {{Episodic Reinforcement Learning}}},
  author = {Liu, Yang and Luo, Yunan and Zhong, Yuanyi and Chen, Xi and Liu, Qiang and Peng, Jian},
  date = {2019-05-31},
  eprint = {1905.13420},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.13420},
  urldate = {2019-06-12},
  abstract = {Recent advances in deep reinforcement learning algorithms have shown great potential and success for solving many challenging real-world problems, including Go game and robotic applications. Usually, these algorithms need a carefully designed reward function to guide training in each time step. However, in real world, it is non-trivial to design such a reward function, and the only signal available is usually obtained at the end of a trajectory, also known as the episodic reward or return. In this work, we introduce a new algorithm for temporal credit assignment, which learns to decompose the episodic return back to each time-step in the trajectory using deep neural networks. With this learned reward signal, the learning efficiency can be substantially improved for episodic reinforcement learning. In particular, we find that expressive language models such as the Transformer can be adopted for learning the importance and the dependency of states in the trajectory, therefore providing high-quality and interpretable learned reward signals. We have performed extensive experiments on a set of MuJoCo continuous locomotive control tasks with only episodic returns and demonstrated the effectiveness of our algorithm.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\SEG76HNP\\Liu et al_2019_Sequence Modeling of Temporal Credit Assignment for Episodic Reinforcement.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\45CZULPT\\1905.html}
}

@inproceedings{liuVarianceAdaptiveLearning2019,
  title = {On the {{Variance}} of the {{Adaptive Learning Rate}} and {{Beyond}}},
  author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  date = {2019-09-25},
  url = {https://openreview.net/forum?id=rkgz2aEKDr},
  urldate = {2020-09-25},
  abstract = {If warmup is the answer, what is the question?},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\P86Z3NS9\\Liu et al_2019_On the Variance of the Adaptive Learning Rate and Beyond.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\4EACK3YW\\forum.html}
}

@online{LOFIdentifyingDensitybased,
  title = {{{LOF}}: Identifying Density-Based Local Outliers: {{ACM SIGMOD Record}}: {{Vol}} 29, {{No}} 2},
  url = {https://dl.acm.org/doi/10.1145/335191.335388},
  urldate = {2023-04-11},
  file = {C:\Users\USEBPERP\Zotero\storage\GISY636Q\335191.html}
}

@article{lombardinoRoleMedicinalChemist2004,
  title = {The Role of the Medicinal Chemist in Drug Discovery — Then and Now},
  author = {Lombardino, Joseph G. and Lowe, John A.},
  date = {2004-10},
  journaltitle = {Nature Reviews Drug Discovery},
  volume = {3},
  number = {10},
  pages = {853--862},
  publisher = {Nature Publishing Group},
  issn = {1474-1784},
  doi = {10.1038/nrd1523},
  url = {https://www.nature.com/articles/nrd1523},
  urldate = {2021-04-08},
  abstract = {Medicinal chemists paly a crucial role in the drug discovery process through the selection and synthesis of compounds that establish structure–activity relationships and achieve efficacy and safety in preclinical animal testingMany aspects of the medicinal chemist's role have changed since the early era of drug discovery when animal testing and small, informal project teams dominated the process.Combinatorial chemistry, high-throughput screening and molecularly defined targets that allow structure-based drug design have changed the chemist's role in the modern era.In vitro screens for pharmacokinetic properties, the focus on synthesizing drug-like compounds, and in vitro toxicity screens are important new developments that aid the medicinal chemist's job today.Suggestions for improving the drug discovery process include more in vivo testing earlier in the drug discovery process, allowing medicinal chemists to champion their drug candidate during its development; and passing on the tacit knowledge of experienced medicinal chemists to their younger colleagues.},
  issue = {10},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\EFJS8NEV\\Lombardino and Lowe - 2004 - The role of the medicinal chemist in drug discover.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\X3HADJV6\\nrd1523.html}
}

@article{loshchilovFixingWeightDecay2018,
  title = {Fixing {{Weight Decay Regularization}} in {{Adam}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=rk6qdGgCZ},
  urldate = {2021-04-08},
  abstract = {Fixing weight decay regularization in adaptive gradient methods such as Adam},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\XN9UFWFI\\Loshchilov and Hutter - 2018 - Fixing Weight Decay Regularization in Adam.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DXJ3MRV9\\forum.html}
}

@thesis{loweExtractionChemicalStructures2012,
  type = {Thesis},
  ids = {lowe2012extractiona},
  title = {Extraction of Chemical Structures and Reactions from the Literature},
  author = {Lowe, Daniel Mark},
  date = {2012-10-09},
  institution = {University of Cambridge},
  doi = {10.17863/CAM.16293},
  url = {https://www.repository.cam.ac.uk/handle/1810/244727},
  urldate = {2021-04-08},
  abstract = {The ever increasing quantity of chemical literature necessitates  the creation of automated techniques for extracting relevant information.  This work focuses on two aspects: the conversion of chemical names to  computer readable structure representations and the extraction of chemical  reactions from text.  Chemical names are a common way of communicating chemical structure  information. OPSIN (Open Parser for Systematic IUPAC Nomenclature), an  open source, freely available algorithm for converting chemical names to  structures was developed. OPSIN employs a regular grammar to direct  tokenisation and parsing leading to the generation of an XML parse tree.  Nomenclature operations are applied successively to the tree with many  requiring the manipulation of an in-memory connection table representation  of the structure under construction. Areas of nomenclature supported are  described with attention being drawn to difficulties that may be  encountered in name to structure conversion. Results on sets of generated  names and names extracted from patents are presented. On generated names,  recall of between 96.2\% and 99.0\% was achieved with a lower bound of 97.9\%  on precision with all results either being comparable or superior to the  tested commercial solutions. On the patent names OPSIN s recall was 2-10\%  higher than the tested solutions when the patent names were processed as  found in the patents. The uses of OPSIN as a web service and as a tool for  identifying chemical names in text are shown to demonstrate the direct  utility of this algorithm.  A software system for extracting chemical reactions from the text of  chemical patents was developed. The system relies on the output of  ChemicalTagger, a tool for tagging words and identifying phrases of  importance in experimental chemistry text. Improvements to this tool  required to facilitate this task are documented. The structure of chemical  entities are where possible determined using OPSIN in conjunction with a  dictionary of name to structure relationships. Extracted reactions are  atom mapped to confirm that they are chemically consistent. 424,621 atom  mapped reactions were extracted from 65,034 organic chemistry USPTO  patents. On a sample of 100 of these extracted reactions chemical entities  were identified with 96.4\% recall and 88.9\% precision. Quantities could be  associated with reagents in 98.8\% of cases and 64.9\% of cases for products  whilst the correct role was assigned to chemical entities in 91.8\% of  cases. Qualitatively the system captured the essence of the reaction in  95\% of cases. This system is expected to be useful in the creation of  searchable databases of reactions from chemical patents and in  facilitating analysis of the properties of large populations of reactions.},
  langid = {english},
  annotation = {Accepted: 2013-07-23T08:23:10Z},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\IGKWKSZY\\Lowe - 2012 - Extraction of chemical structures and reactions fr.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\UFZ7IDE7\\244727.html}
}

@incollection{lucasDonTextquotesingleBlame2019,
  title = {Don\textbackslash textquotesingle t {{Blame}} the {{ELBO}}! {{A Linear VAE Perspective}} on {{Posterior Collapse}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Lucas, James and Tucker, George and Grosse, Roger B and Norouzi, Mohammad},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Alché-Buc, given=F., prefix=d\textbackslash textquotesingle, useprefix=false and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {9403--9413},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/9138-dont-blame-the-elbo-a-linear-vae-perspective-on-posterior-collapse.pdf},
  urldate = {2020-02-04},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ZRH7F8GF\\Lucas et al_2019_Don-textquotesingle t Blame the ELBO.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VC6N8NCX\\9138-dont-blame-the-elbo-a-linear-vae-perspective-on-posterior-collapse.html}
}

@article{lyuUltralargeLibraryDocking2019,
  title = {Ultra-Large Library Docking for Discovering New Chemotypes},
  author = {Lyu, Jiankun and Wang, Sheng and Balius, Trent E. and Singh, Isha and Levit, Anat and Moroz, Yurii S. and O’Meara, Matthew J. and Che, Tao and Algaa, Enkhjargal and Tolmachova, Kateryna and Tolmachev, Andrey A. and Shoichet, Brian K. and Roth, Bryan L. and Irwin, John J.},
  date = {2019-02},
  journaltitle = {Nature},
  volume = {566},
  number = {7743},
  pages = {224--229},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-0917-9},
  url = {http://www.nature.com/articles/s41586-019-0917-9},
  urldate = {2020-05-29},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\EXH5SDDQ\lyu2019.pdf}
}

@online{MachineLearningDiscussionWhy,
  title = {R/{{MachineLearning}} - [{{Discussion}}] {{Why}} Not Turn Momentum Update Equation into Exponentially Weighted Moving Average Update Equation?},
  url = {https://www.reddit.com/r/MachineLearning/comments/bfttns/discussion_why_not_turn_momentum_update_equation/},
  urldate = {2020-02-10},
  abstract = {8 votes and 3 comments so far on Reddit},
  langid = {american},
  organization = {reddit},
  file = {C:\Users\USEBPERP\Zotero\storage\NFLEILGD\discussion_why_not_turn_momentum_update_equation.html}
}

@online{MachineLearningMomentumMethods,
  title = {R/{{MachineLearning}} - [{{D}}] {{Momentum}} Methods Helps to Escape Local Minima, so What? {{It}} Was Never Our Objective.},
  shorttitle = {R/{{MachineLearning}} - [{{D}}] {{Momentum}} Methods Helps to Escape Local Minima, so What?},
  url = {https://www.reddit.com/r/MachineLearning/comments/dqbp9g/d_momentum_methods_helps_to_escape_local_minima/},
  urldate = {2020-02-10},
  abstract = {4 votes and 26 comments so far on Reddit},
  langid = {american},
  organization = {reddit},
  file = {C:\Users\USEBPERP\Zotero\storage\YUKJ955A\d_momentum_methods_helps_to_escape_local_minima.html}
}

@online{MachineLearningWhy,
  title = {Machine Learning - {{Why}} Mini Batch Size Is Better than One Single "Batch" with All Training Data?},
  url = {https://datascience.stackexchange.com/questions/16807/why-mini-batch-size-is-better-than-one-single-batch-with-all-training-data},
  urldate = {2019-08-28},
  organization = {Data Science Stack Exchange},
  file = {C:\Users\USEBPERP\Zotero\storage\BU9HW7UX\why-mini-batch-size-is-better-than-one-single-batch-with-all-training-data.html}
}

@unpublished{maddisonMoveEvaluationGo2015,
  title = {Move {{Evaluation}} in {{Go Using Deep Convolutional Neural Networks}}},
  author = {Maddison, Chris J. and Huang, Aja and Sutskever, Ilya and Silver, David},
  date = {2015-04-10},
  eprint = {1412.6564},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1412.6564},
  urldate = {2020-06-05},
  abstract = {The game of Go is more challenging than other board games, due to the difficulty of constructing a position or move evaluation function. In this paper we investigate whether deep convolutional networks can be used to directly represent and learn this knowledge. We train a large 12-layer convolutional neural network by supervised learning from a database of human professional games. The network correctly predicts the expert move in 55\% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GnuGo in 97\% of games, and matched the performance of a state-of-the-art Monte-Carlo tree search that simulates a million positions per move.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\W4AYYZS3\\Maddison et al_2015_Move Evaluation in Go Using Deep Convolutional Neural Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\69BI4IGB\\1412.html}
}

@unpublished{madhawaGraphNVPInvertibleFlow2019,
  title = {{{GraphNVP}}: {{An Invertible Flow Model}} for {{Generating Molecular Graphs}}},
  shorttitle = {{{GraphNVP}}},
  author = {Madhawa, Kaushalya and Ishiguro, Katushiko and Nakago, Kosuke and Abe, Motoki},
  date = {2019-05-28},
  eprint = {1905.11600},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.11600},
  urldate = {2019-12-19},
  abstract = {We propose GraphNVP, the first invertible, normalizing flow-based molecular graph generation model. We decompose the generation of a graph into two steps: generation of (i) an adjacency tensor and (ii) node attributes. This decomposition yields the exact likelihood maximization on graph-structured data, combined with two novel reversible flows. We empirically demonstrate that our model efficiently generates valid molecular graphs with almost no duplicated molecules. In addition, we observe that the learned latent space can be used to generate molecules with desired chemical properties.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\3UY5QDRU\\Madhawa et al_2019_GraphNVP.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6TYYCAHG\\1905.html}
}

@article{madzhidovAtomtoAtomMappingBenchmarking2020,
  title = {Atom-to-{{Atom Mapping}}: {{A Benchmarking Study}} of {{Popular Mapping Algorithms}} and {{Consensus Strategies}}},
  shorttitle = {Atom-to-{{Atom Mapping}}},
  author = {Madzhidov, Timur and Lin, Arkadii I. and Nugmanov, Ramil and Dyubankova, Natalia and Gimadiev, Timur and Wegner, Jörg Kurt and Rakhimbekova, Assima and Akhmetshin, Tagir and Ibragimova, Zarina and Varnek, Alexandre and Suleymanov, Rail and Ceulemans, Hugo and Verhoeven, Jonas},
  date = {2020-09-30},
  publisher = {ChemRxiv},
  doi = {10.26434/chemrxiv.13012679.v1},
  url = {/articles/preprint/Atom-to-Atom_Mapping_A_Benchmarking_Study_of_Popular_Mapping_Algorithms_and_Consensus_Strategies/13012679/1},
  urldate = {2021-01-20},
  abstract = {Here, we discuss a reaction standardization protocol followed by a comparison of popular Atom-to-atom mapping (AAM) tools (ChemAxon, Indigo, RDTool, NextMove and RXNMapper) as well as some consensus AAM strategies. For this purpose, a dataset of 1851 manually curated and mapped reactions was prepared (the Golden dataset) and used as a reference set. It has been found that RXNMapper possesses the highest accuracy, despite the fact that it has some clear disadvantages. Finally, RXNMapper was selected as the best tool, and it was applied to map the USPTO dataset. The standardization protocol used to prepare the data, as well as the data itself are available in the GitHub repository https://github.com/Laboratoire-de-Chemoinformatique.},
  langid = {english}
}

@article{makurvetBiologicsVsSmall2021,
  title = {Biologics vs. Small Molecules: {{Drug}} Costs and Patient Access},
  shorttitle = {Biologics vs. Small Molecules},
  author = {Makurvet, Favour Danladi},
  date = {2021-03-01},
  journaltitle = {Medicine in Drug Discovery},
  volume = {9},
  pages = {100075},
  issn = {2590-0986},
  doi = {10.1016/j.medidd.2020.100075},
  url = {https://www.sciencedirect.com/science/article/pii/S2590098620300622},
  urldate = {2024-04-28},
  abstract = {Significant advances in drug research and development are herein reviewed first to set the background for a critical consideration of the economic sustainability of biologics and small molecules, why biologic drugs are more expensive, and how drug cost often influences patient access to one drug class over the other. Also strongly emphasized is the need for the drug-making, especially the biopharmaceutical, industry to consider a reassignment of priorities so that more patients can enjoy the great benefits that come with blockbuster drugs, many of which are of biological origin but extremely expensive. A balance between the efficacy of wonder-performing drugs and the patient's financial ability to access them must be established to obliterate the crippling effect of the high costs of drugs on the poor majority of patients – those who cannot afford them. The overarching point broadly emphasized is that the actual success in drug discovery and development and in healthcare delivery should be measured not only by the magnitude of scientific breakthroughs but also by the level to which they are affordable to patients as determined by their costs. To enhance patients' access to drugs and new and/or improved healthcare technologies, more research attention must be paid to such cheaper alternatives as small molecules generics, biosimilars, and antibody-drug conjugates; government policies must be established to encourage the commercialization of biosimilars; and pharmaceutical companies must be charitable enough to run assistance programs for eligible financially handicap patients while seeking to make profits from the drug-making business.},
  keywords = {antibody-drug conjugates,biologics,biosimilars,drug cost,patient access to drugs,small molecule drugs},
  file = {C:\Users\USEBPERP\Zotero\storage\4K84AMEZ\S2590098620300622.html}
}

@inproceedings{malhotraLongShortTerm2015,
  title = {Long {{Short Term Memory Networks}} for {{Anomaly Detection}} in {{Time Series}}},
  author = {Malhotra, Pankaj and Vig, Lovekesh and Shroff, Gautam and Agarwal, Puneet},
  date = {2015-04-22},
  abstract = {Long Short Terma Memory (LSTM) Networks have been demonstrated to be particularly useful for learning sequences containing longer term patterns of unknown length, due to their ability to maintain long term memory. Stacking recurrent hidden layers in such networks also enables the learning of higher level temporal features, for faster learning with sparser representations. In this paper, we use stacked LSTM networks for anomaly/fault detection in time series. A network is trained on non-anomalous data and used as a predictor over a number of time steps. The resulting prediction errors are modeled as a multivariate Gaussian distribution, which is used to assess the likelihood of anomalous behavior. The efficacy of this approach is demonstrated on four datasets: ECG, space shuttle, power demand, and multi-sensor engine dataset.},
  file = {C:\Users\USEBPERP\Zotero\storage\NI3IKSWN\Malhotra et al. - 2015 - Long Short Term Memory Networks for Anomaly Detect.pdf}
}

@online{malhotraLSTMbasedEncoderDecoderMultisensor2016,
  title = {{{LSTM-based Encoder-Decoder}} for {{Multi-sensor Anomaly Detection}}},
  author = {Malhotra, Pankaj and Ramakrishnan, Anusha and Anand, Gaurangi and Vig, Lovekesh and Agarwal, Puneet and Shroff, Gautam},
  date = {2016-07-11},
  eprint = {1607.00148},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1607.00148},
  url = {http://arxiv.org/abs/1607.00148},
  urldate = {2023-01-06},
  abstract = {Mechanical devices such as engines, vehicles, aircrafts, etc., are typically instrumented with numerous sensors to capture the behavior and health of the machine. However, there are often external factors or variables which are not captured by sensors leading to time-series which are inherently unpredictable. For instance, manual controls and/or unmonitored environmental conditions or load may lead to inherently unpredictable time-series. Detecting anomalies in such scenarios becomes challenging using standard approaches based on mathematical models that rely on stationarity, or prediction models that utilize prediction errors to detect anomalies. We propose a Long Short Term Memory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD) that learns to reconstruct 'normal' time-series behavior, and thereafter uses reconstruction error to detect anomalies. We experiment with three publicly available quasi predictable time-series datasets: power demand, space shuttle, and ECG, and two real-world engine datasets with both predictive and unpredictable behavior. We show that EncDec-AD is robust and can detect anomalies from predictable, unpredictable, periodic, aperiodic, and quasi-periodic time-series. Further, we show that EncDec-AD is able to detect anomalies from short time-series (length as small as 30) as well as long time-series (length as large as 500).},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\65T5Z8J9\\Malhotra et al. - 2016 - LSTM-based Encoder-Decoder for Multi-sensor Anomal.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FG49UJKE\\1607.html}
}

@inproceedings{mannAtomMappingConstraint2013,
  title = {Atom {{Mapping}} with {{Constraint Programming}}},
  booktitle = {Principles and {{Practice}} of {{Constraint Programming}}},
  author = {Mann, Martin and Nahar, Feras and Ekker, Heinz and Backofen, Rolf and Stadler, Peter F. and Flamm, Christoph},
  editor = {Schulte, Christian},
  date = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {805--822},
  publisher = {Springer Berlin Heidelberg},
  abstract = {Chemical reactions consist of a rearrangement of bonds so that each atom in an educt molecule appears again in a specific position of a reaction product. In general this bijection between educt and product atoms is not reported by chemical reaction databases, leaving the Atom Mapping Problem as an important computational task for many practical applications in computational chemistry and systems biology. Elementary chemical reactions feature a cyclic imaginary transition state (ITS) that imposes additional restrictions on the bijection between educt and product atoms that are not taken into account by previous approaches. We demonstrate that Constraint Programming is well-suited to solving the Atom Mapping Problem in this setting. The performance of our approach is evaluated for a subset of chemical reactions from the KEGG database featuring various ITS cycle layouts and reaction mechanisms.},
  isbn = {978-3-642-40627-0},
  langid = {english},
  keywords = {Atom Mapping,Constraint Program,Constraint Satisfaction Problem,Product Graph,Reaction Mapping},
  file = {C:\Users\USEBPERP\Zotero\storage\MAXDLMBB\Mann et al_2013_Atom Mapping with Constraint Programming.pdf}
}

@article{mannholdCalculationMolecularLipophilicity2009,
  ids = {mannhold2009calculationa},
  title = {Calculation of {{Molecular Lipophilicity}}: {{State-of-the-Art}} and {{Comparison}} of {{LogP Methods}} on More than 96,000 {{Compounds}}},
  shorttitle = {Calculation of {{Molecular Lipophilicity}}},
  author = {Mannhold, Raimund and Poda, Gennadiy I. and Ostermann, Claude and Tetko, Igor V.},
  date = {2009-03-01},
  journaltitle = {JPharmSci},
  volume = {98},
  number = {3},
  pages = {861--893},
  publisher = {Elsevier},
  issn = {0022-3549},
  doi = {10.1002/jps.21494},
  url = {https://jpharmsci.org/article/S0022-3549(16)32908-2/abstract},
  urldate = {2020-05-04},
  abstract = {{$<$}h2{$>$}Abstract{$<$}/h2{$><$}p{$>$}We first review the state-of-the-art in development of log\emph{P} prediction approaches falling in two major categories: substructure-based and property-based methods. Then, we compare the predictive power of representative methods for one public (\emph{N}=266) and two \emph{in house} datasets from Nycomed (\emph{N}=882) and Pfizer (\emph{N}=95809). A total of 30 and 18 methods were tested for public and industrial datasets, respectively. Accuracy of models declined with the number of nonhydrogen atoms. The Arithmetic Average Model (AAM), which predicts the same value (the arithmetic mean) for all compounds, was used as a baseline model for comparison. Methods with \emph{Root Mean Squared Error} (\emph{RMSE}) greater than \emph{RMSE} produced by the AAM were considered as unacceptable. The majority of analyzed methods produced reasonable results for the public dataset but only seven methods were successful on the both \emph{in house} datasets. We proposed a simple equation based on the number of carbon atoms, NC, and the number of hetero atoms, NHET: log\emph{P}=1.46(±0.02)+0.11(±0.001) NC−0.11(±0.001) NHET. This equation outperformed a large number of programs benchmarked in this study. Factors influencing the accuracy of log\emph{P} predictions were elucidated and discussed.{$<$}/p{$>$}},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\V43CX2H8\\fulltext.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\WNIVQDR3\\fulltext.html}
}

@article{mannPredictingChemicalReaction2020,
  title = {Predicting {{Chemical Reaction Outcomes}}: {{A Grammar Ontology-based Transformer Framework}}},
  shorttitle = {Predicting {{Chemical Reaction Outcomes}}},
  author = {Mann, Vipul and Venkatasubramanian, Venkat},
  date = {2020-09-22},
  publisher = {ChemRxiv},
  doi = {10.26434/chemrxiv.12985892.v1},
  url = {/articles/preprint/Predicting_Chemical_Reaction_Outcomes_A_Grammar_Ontology-based_Transformer_Framework/12985892/1},
  urldate = {2021-01-20},
  abstract = {Discovering and designing novel materials is a challenging problem as it often requires searching a combinatorially large space of potential candidates. Evaluation of all candidates experimentally is typically infeasible as it requires great amounts of effort, time, expertise, and money. The ability to predict reaction outcomes without performing extensive experiments is, therefore, important. Towards that goal, we report an approach that uses context-free grammar (CFG) based representations of molecules in a neural machine translation framework. We formulate the reaction-prediction task as a machine translation problem that involves discovering the transformations from the source sequence (comprising the reactants and agents) to the target sequence (comprising the major product) in the reaction. The grammar ontology-based representation of molecules hierarchically incorporates rich molecular structure information that, in principle, should be valuable for modeling chemical reactions. We achieve an accuracy of 80.1\% on a standard reaction dataset using a model characterized by only a fraction of the number of training parameters in other sequence-to-sequence models based works in this area. Moreover, 99\% of the predictions made on the same reaction dataset were found to be syntactically valid. We conclude that CFGs-based ontological representations could be an efficient way of incorporating structural information, ensuring chemically valid predictions, and overcoming overfitting in complex machine learning architectures employed in reaction prediction tasks.},
  langid = {english}
}

@article{mannRetrosynthesisPredictionUsing2021,
  title = {Retrosynthesis {{Prediction}} Using {{Grammar-based Neural Machine Translation}}: {{An Information-Theoretic Approach}}},
  shorttitle = {Retrosynthesis {{Prediction}} Using {{Grammar-based Neural Machine Translation}}},
  author = {Mann, Vipul and Venkatasubramanian, Venkat},
  date = {2021-04-15},
  publisher = {ChemRxiv},
  doi = {10.26434/chemrxiv.14410442.v1},
  url = {/articles/preprint/Retrosynthesis_Prediction_using_Grammar-based_Neural_Machine_Translation_An_Information-Theoretic_Approach/14410442/1},
  urldate = {2021-05-18},
  abstract = {Retrosynthetic prediction is one of the main challenges in chemical synthesis that requires identifying reaction pathways and precursor molecules for synthesizing a target molecule. This requires a search over the space of plausible chemical reactions that often results in complex, multi-step, branched synthesis trees for even moderately complex organic reactions. Here, we propose an approach that performs single-step retrosynthesis prediction using SMILES grammar-based representations in a neural machine translation framework. Information-theoretic analyses of such grammar-representations reveal that they are both superior and well-suited for machine learning tasks due to their underlying redundancy and high information capacity compared to purely character-based representations. We report the top-1 prediction accuracy of 43.8\% (top-5 measure of 61.4\%) and syntactic validity of 95.6\% (top-5 measure of 91.6\%) on a standard reaction dataset. Comparing our model's performance with previous work that used purely character-based SMILES representations demonstrate improved accuracy and reduced grammatically invalid predictions.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\Y9VNLFI7\\Mann and Venkatasubramanian - 2021 - Retrosynthesis Prediction using Grammar-based Neur.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\U6VPTPBI\\1.html}
}

@article{maoMolecularGraphEnhanced2020,
  title = {Molecular {{Graph Enhanced Transformer}} for {{Retrosynthesis Prediction}}},
  author = {Mao, Kelong and Zhao, Peilin and Xu, Tingyang and Rong, Yu and Xiao, Xi and Huang, Junzhou},
  date = {2020-03-09},
  journaltitle = {bioRxiv},
  pages = {2020.03.05.979773},
  publisher = {Cold Spring Harbor Laboratory},
  doi = {10.1101/2020.03.05.979773},
  url = {https://www.biorxiv.org/content/10.1101/2020.03.05.979773v2},
  urldate = {2021-05-05},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}With massive possible synthetic routes in chemistry, retrosynthesis prediction is still a challenge for researchers. Recently, retrosynthesis prediction is formulated as a Machine Translation (MT) task. Namely, since each molecule can be represented as a Simplified Molecular-Input Line-Entry System (SMILES) string, the process of retrosynthesis is analogized to a process of language translation from the product to reactants. However, the MT models that applied on SMILES data usually ignore the information of natural atomic connections and the topology of molecules. To make more chemically plausible constrains on the atom representation learning for better performance, in this paper, we propose a Graph Enhanced Transformer (GET) framework, which adopts both the sequential and graphical information of molecules. Four different GET designs are proposed, which fuse the SMILES representations with atom embeddings learned from our improved Graph Neural Network (GNN). Empirical results show that our model significantly outperforms the vanilla Transformer model in test accuracy.{$<$}/p{$>$}},
  langid = {english}
}

@unpublished{maQuasihyperbolicMomentumAdam2019,
  title = {Quasi-Hyperbolic Momentum and {{Adam}} for Deep Learning},
  author = {Ma, Jerry and Yarats, Denis},
  date = {2019-05-02},
  eprint = {1810.06801},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.06801},
  urldate = {2020-02-10},
  abstract = {Momentum-based acceleration of stochastic gradient descent (SGD) is widely used in deep learning. We propose the quasi-hyperbolic momentum algorithm (QHM) as an extremely simple alteration of momentum SGD, averaging a plain SGD step with a momentum step. We describe numerous connections to and identities with other algorithms, and we characterize the set of two-state optimization algorithms that QHM can recover. Finally, we propose a QH variant of Adam called QHAdam, and we empirically demonstrate that our algorithms lead to significantly improved training in a variety of settings, including a new state-of-the-art result on WMT16 EN-DE. We hope that these empirical results, combined with the conceptual and practical simplicity of QHM and QHAdam, will spur interest from both practitioners and researchers. Code is immediately available.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\J43RTGV7\\Ma_Yarats_2019_Quasi-hyperbolic momentum and Adam for deep learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\I9NDDIMJ\\1810.html}
}

@article{marcouExpertSystemPredicting2015,
  title = {Expert {{System}} for {{Predicting Reaction Conditions}}: {{The Michael Reaction Case}}},
  shorttitle = {Expert {{System}} for {{Predicting Reaction Conditions}}},
  author = {Marcou, G. and Aires de Sousa, J. and Latino, D. A. R. S. and family=Luca, given=A., prefix=de, useprefix=true and Horvath, D. and Rietsch, V. and Varnek, A.},
  date = {2015-02-23},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {55},
  number = {2},
  pages = {239--250},
  issn = {1549-9596},
  doi = {10.1021/ci500698a},
  url = {https://doi.org/10.1021/ci500698a},
  urldate = {2019-10-14},
  abstract = {A generic chemical transformation may often be achieved under various synthetic conditions. However, for any specific reagents, only one or a few among the reported synthetic protocols may be successful. For example, Michael β-addition reactions may proceed under different choices of solvent (e.g., hydrophobic, aprotic polar, protic) and catalyst (e.g., Brønsted acid, Lewis acid, Lewis base, etc.). Chemoinformatics methods could be efficiently used to establish a relationship between the reagent structures and the required reaction conditions, which would allow synthetic chemists to waste less time and resources in trying out various protocols in search for the appropriate one. In order to address this problem, a number of 2-classes classification models have been built on a set of 198 Michael reactions retrieved from literature. Trained models discriminate between processes that are compatible and respectively processes not feasible under a specific reaction condition option (feasible or not with a Lewis acid catalyst, feasible or not in hydrophobic solvent, etc.). Eight distinct models were built to decide the compatibility of a Michael addition process with each considered reaction condition option, while a ninth model was aimed to predict whether the assumed Michael addition is feasible at all. Different machine-learning methods (Support Vector Machine, Naive Bayes, and Random Forest) in combination with different types of descriptors (ISIDA fragments issued from Condensed Graphs of Reactions, MOLMAP, Electronic Effect Descriptors, and Chemistry Development Kit computed descriptors) have been used. Models have good predictive performance in 3-fold cross-validation done three times: balanced accuracy varies from 0.7 to 1. Developed models are available for the users at http://infochim.u-strasbg.fr/webserv/VSEngine.html. Eventually, these were challenged to predict feasibility conditions for ∼50 novel Michael reactions from the eNovalys database (originally from patent literature).},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\8662J8WA\\Marcou et al_2015_Expert System for Predicting Reaction Conditions.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\D9D66NY6\\ci500698a.html}
}

@article{markouNoveltyDetectionReview2003,
  title = {Novelty Detection: A Review—Part 2:: Neural Network Based Approaches},
  shorttitle = {Novelty Detection},
  author = {Markou, Markos and Singh, Sameer},
  date = {2003-12-01},
  journaltitle = {Signal Processing},
  volume = {83},
  number = {12},
  pages = {2499--2521},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2003.07.019},
  url = {http://www.sciencedirect.com/science/article/pii/S0165168403002032},
  urldate = {2019-07-08},
  abstract = {Novelty detection is the identification of new or unknown data or signal that a machine learning system is not aware of during training. In this paper we focus on neural network-based approaches for novelty detection. Statistical approaches are covered in Part 1 paper.},
  keywords = {ART,MLP,Network-based approaches,Neural networks,Novelty detection,RBF},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\AVBPAEWG\\Markou_Singh_2003_Novelty detection.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\CZBSQ5IZ\\S0165168403002032.html}
}

@article{martinDiverseViewpointsComputational2001,
  title = {Diverse {{Viewpoints}} on {{Computational Aspects}} of {{Molecular Diversity}}},
  author = {Martin, Yvonne C.},
  date = {2001-05-01},
  journaltitle = {J. Comb. Chem.},
  volume = {3},
  number = {3},
  pages = {231--250},
  publisher = {American Chemical Society},
  issn = {1520-4766},
  doi = {10.1021/cc000073e},
  url = {https://doi.org/10.1021/cc000073e},
  urldate = {2022-07-04},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\HBHEGR9F\\Martin - 2001 - Diverse Viewpoints on Computational Aspects of Mol.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FWRFJI9G\\cc000073e.html}
}

@article{martinMeasuringDiversityExperimental1995,
  title = {Measuring {{Diversity}}: {{Experimental Design}} of {{Combinatorial Libraries}} for {{Drug Discovery}}},
  shorttitle = {Measuring {{Diversity}}},
  author = {Martin, Eric J. and Blaney, Jeffrey M. and Siani, Michael A. and Spellmeyer, David C. and Wong, Alex K. and Moos, Walter H.},
  date = {1995-04-01},
  journaltitle = {J. Med. Chem.},
  volume = {38},
  number = {9},
  pages = {1431--1436},
  publisher = {American Chemical Society},
  issn = {0022-2623},
  doi = {10.1021/jm00009a003},
  url = {https://doi.org/10.1021/jm00009a003},
  urldate = {2022-06-10},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ITQRP9RI\\Martin et al. - 1995 - Measuring Diversity Experimental Design of Combin.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\MPZI78RC\\jm00009a003.html}
}

@article{martinProfileQSARKinaseVirtual2017,
  title = {Profile-{{QSAR}} 2.0: {{Kinase Virtual Screening Accuracy Comparable}} to {{Four-Concentration IC50s}} for {{Realistically Novel Compounds}}},
  shorttitle = {Profile-{{QSAR}} 2.0},
  author = {Martin, Eric J. and Polyakov, Valery R. and Tian, Li and Perez, Rolando C.},
  date = {2017-08-28},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {57},
  number = {8},
  pages = {2077--2088},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.7b00166},
  url = {https://doi.org/10.1021/acs.jcim.7b00166},
  urldate = {2019-05-20},
  abstract = {While conventional random forest regression (RFR) virtual screening models appear to have excellent accuracy on random held-out test sets, they prove lacking in actual practice. Analysis of 18 historical virtual screens showed that random test sets are far more similar to their training sets than are the compounds project teams actually order. A new, cluster-based “realistic” training/test set split, which mirrors the chemical novelty of real-life virtual screens, recapitulates the poor predictive power of RFR models in real projects. The original Profile-QSAR (pQSAR) method greatly broadened the domain of applicability over conventional models by using as independent variables a profile of activity predictions from all historical assays in a large protein family. However, the accuracy still fell short of experiment on realistic test sets. The improved “pQSAR 2.0” method replaces probabilities of activity from naïve Bayes categorical models at several thresholds with predicted IC50s from RFR models. Unexpectedly, the high accuracy also requires removing the RFR model for the actual assay of interest from the independent variable profile. With these improvements, pQSAR 2.0 activity predictions are now statistically comparable to medium-throughput four-concentration IC50 measurements even on the realistic test set. Beyond the yes/no activity predictions from a typical high-throughput screen (HTS) or conventional virtual screen, these semiquantitative IC50 predictions allow for predicted potency, ligand efficiency, lipophilic efficiency, and selectivity against antitargets, greatly facilitating hitlist triaging and enabling virtual screening panels such as toxicity panels and overall promiscuity predictions.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2C5CHW84\\Martin et al_2017_Profile-QSAR 2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6FZUKP6G\\acs.jcim.html}
}

@unpublished{mastersRevisitingSmallBatch2018,
  title = {Revisiting {{Small Batch Training}} for {{Deep Neural Networks}}},
  author = {Masters, Dominic and Luschi, Carlo},
  date = {2018-04-20},
  eprint = {1804.07612},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.07612},
  urldate = {2019-08-26},
  abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput. In this paper, we review common assumptions on learning rate scaling and training duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size \$m\$. The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient calculations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between \$m = 2\$ and \$m = 32\$, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\66KX86SM\\Masters_Luschi_2018_Revisiting Small Batch Training for Deep Neural Networks2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ICLJWED8\\Masters_Luschi_2018_Revisiting Small Batch Training for Deep Neural Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WDYY85F7\\Masters and Luschi - 2018 - Revisiting Small Batch Training for Deep Neural Ne.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\3H6KZ6K2\\1804.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\NLSBPQLZ\\1804.html}
}

@article{matsubaraDigitizationOrganicSynthesis2020,
  title = {Digitization of {{Organic Synthesis}} — {{How Synthetic Organic Chemists Use AI Technology}} —},
  author = {Matsubara, Seijiro},
  date = {2020-12-11},
  journaltitle = {Chem. Lett.},
  volume = {50},
  number = {3},
  pages = {475--481},
  publisher = {The Chemical Society of Japan},
  issn = {0366-7022},
  doi = {10.1246/cl.200802},
  url = {https://www.journal.csj.jp/doi/full/10.1246/cl.200802},
  urldate = {2021-05-06},
  abstract = {The core technologies of fully automated organic synthesis include the management of large amounts of accumulated reaction examples, machine learning based on these experimental results, and subsequent development of synthesis devices. Digitization of organic synthesis is key in progressing towards full automation. Various attempts at digitization are already enabling the development of a framework, in which the input of a molecular structure is converted into actual molecular synthesis.},
  issue = {3}
}

@article{matterSelectingOptimallyDiverse1997,
  title = {Selecting {{Optimally Diverse Compounds}} from {{Structure Databases}}:\, {{A Validation Study}} of {{Two-Dimensional}} and {{Three-Dimensional Molecular Descriptors}}},
  shorttitle = {Selecting {{Optimally Diverse Compounds}} from {{Structure Databases}}},
  author = {Matter, Hans},
  date = {1997-04-01},
  journaltitle = {J. Med. Chem.},
  volume = {40},
  number = {8},
  pages = {1219--1229},
  publisher = {American Chemical Society},
  issn = {0022-2623},
  doi = {10.1021/jm960352+},
  url = {https://doi.org/10.1021/jm960352+},
  urldate = {2022-06-09},
  abstract = {The efficiency of the drug discovery process can be significantly improved using design techniques to maximize the diversity of structure databases or combinatorial libraries. Here, several physicochemical descriptors were investigated to quantify molecular diversity. Based on the 2D or 3D topological similarity of molecules, the relationship between physicochemical metrics and biological activity was studied to find valid descriptors. Several compounds were selected using those descriptors from a database containing diverse templates and 55 biological classes. It was evaluated whether the obtained subsets represent all biological properties and structural variations of the original database. In addition, hierarchical cluster analyses were used to group molecules from the parent database, which should have similar biological properties. Using various sets of structurally similar molecules, it was possible to derive quantitative measures for compound similarities in relation to biological properties. A similarity radius for 2D fingerprints and molecular steric fields was estimated; compounds within this radius of another molecule were shown to have comparable biological properties. This study demonstrates that 2D fingerprints alone or in combination with other metrics as the primary descriptor allow to handle global diversity. In addition, standard atom-pair descriptors or molecular steric fields can be used to correlate structural diversity with biological activity. Hence, the latter two descriptors can be classified as secondary descriptors useful for analog library design, while 2D fingerprints are applicable to design a general library for lead discovery. Based on these findings, an optimally diverse subset containing only 38\% of the entire IC93 database was generated using 2D fingerprints. Here no structure is more similar than 0.85 to any other (Tanimoto coefficient), but all biological classes were selected. This reduction of redundancy led to a child database with the same physicochemical diversity space, which contains the same information as the original database.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\UGKPLUPX\\Matter - 1997 - Selecting Optimally Diverse Compounds from Structu.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\RJ4ADHK8\\jm960352+.html}
}

@article{mayrDeepToxToxicityPrediction2016,
  title = {{{DeepTox}}: {{Toxicity Prediction}} Using {{Deep Learning}}},
  shorttitle = {{{DeepTox}}},
  author = {Mayr, Andreas and Klambauer, Günter and Unterthiner, Thomas and Hochreiter, Sepp},
  date = {2016},
  journaltitle = {Frontiers in Environmental Science},
  volume = {3},
  issn = {2296-665X},
  url = {https://www.frontiersin.org/articles/10.3389/fenvs.2015.00080},
  urldate = {2023-10-02},
  abstract = {The Tox21 Data Challenge has been the largest effort of the scientific community to compare computational methods for toxicity prediction. This challenge comprised 12,000 environmental chemicals and drugs which were measured for 12 different toxic effects by specifically designed assays. We participated in this challenge to assess the performance of Deep Learning in computational toxicity prediction. Deep Learning has already revolutionized image processing, speech recognition, and language understanding but has not yet been applied to computational toxicity. Deep Learning is founded on novel algorithms and architectures for artificial neural networks together with the recent availability of very fast computers and massive datasets. It discovers multiple levels of distributed representations of the input, with higher levels representing more abstract concepts. We hypothesized that the construction of a hierarchy of chemical features gives Deep Learning the edge over other toxicity prediction methods. Furthermore, Deep Learning naturally enables multi-task learning, that is, learning of all toxic effects in one neural network and thereby learning of highly informative chemical features. In order to utilize Deep Learning for toxicity prediction, we have developed the DeepTox pipeline. First, DeepTox normalizes the chemical representations of the compounds. Then it computes a large number of chemical descriptors that are used as input to machine learning methods. In its next step, DeepTox trains models, evaluates them, and combines the best of them to ensembles. Finally, DeepTox predicts the toxicity of new compounds. In the Tox21 Data Challenge, DeepTox had the highest performance of all computational methods winning the grand challenge, the nuclear receptor panel, the stress response panel, and six single assays (teams “Bioinf@JKU”). We found that Deep Learning excelled in toxicity prediction and outperformed many other computational approaches like naive Bayes, support vector machines, and random forests.},
  file = {C:\Users\USEBPERP\Zotero\storage\F8EKA5FI\Mayr et al. - 2016 - DeepTox Toxicity Prediction using Deep Learning.pdf}
}

@article{mayrLargescaleComparisonMachine2018,
  title = {Large-Scale Comparison of Machine Learning Methods for Drug Target Prediction on {{ChEMBL}}},
  author = {Mayr, Andreas and Klambauer, Günter and Unterthiner, Thomas and Steijaert, Marvin and Wegner, Jörg K. and Ceulemans, Hugo and Clevert, Djork-Arné and Hochreiter, Sepp},
  date = {2018-06-06},
  journaltitle = {Chem. Sci.},
  issn = {2041-6539},
  doi = {10.1039/C8SC00148K},
  url = {http://pubs.rsc.org/en/content/articlelanding/2018/sc/c8sc00148k},
  urldate = {2018-06-08},
  abstract = {Deep learning is currently the most successful machine learning technique in a wide range of application areas and has recently been applied successfully in drug discovery research to predict potential drug targets and to screen for active molecules. However, due to (1) the lack of large-scale studies, (2) the compound series bias that is characteristic of drug discovery datasets and (3) the hyperparameter selection bias that comes with the high number of potential deep learning architectures, it remains unclear whether deep learning can indeed outperform existing computational methods in drug discovery tasks. We therefore assessed the performance of several deep learning methods on a large-scale drug discovery dataset and compared the results with those of other machine learning and target prediction methods. To avoid potential biases from hyperparameter selection or compound series, we used a nested cluster-cross-validation strategy. We found (1) that deep learning methods significantly outperform all competing methods and (2) that the predictive performance of deep learning is in many cases comparable to that of tests performed in wet labs (i.e., in vitro assays).},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JV29FH2I\\Mayr et al_2018_Large-scale comparison of machine learning methods for drug target prediction2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TB5AAZI9\\Mayr et al_2018_Large-scale comparison of machine learning methods for drug target prediction.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\73BRK2Y4\\C8SC00148K.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\9IPS2YJ7\\c8sc00148k.html}
}

@unpublished{maziarkaMolCycleGANGenerativeModel2019,
  title = {Mol-{{CycleGAN}} - a Generative Model for Molecular Optimization},
  author = {Maziarka, Łukasz and Pocha, Agnieszka and Kaczmarczyk, Jan and Rataj, Krzysztof and Warchoł, Michał},
  date = {2019-02-06},
  eprint = {1902.02119},
  eprinttype = {arxiv},
  eprintclass = {physics, stat},
  url = {http://arxiv.org/abs/1902.02119},
  urldate = {2019-02-07},
  abstract = {Designing a molecule with desired properties is one of the biggest challenges in drug development, as it requires optimization of chemical compound structures with respect to many complex properties. To augment the compound design process we introduce Mol-CycleGAN - a CycleGAN-based model that generates optimized compounds with high structural similarity to the original ones. Namely, given a molecule our model generates a structurally similar one with an optimized value of the considered property. We evaluate the performance of the model on selected optimization objectives related to structural properties (presence of halogen groups, number of aromatic rings) and to a physicochemical property (penalized logP). In the task of optimization of penalized logP of drug-like molecules our model significantly outperforms previous results.},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7CH9YDK5\\Maziarka et al_2019_Mol-CycleGAN - a generative model for molecular optimization2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SPM9YUE5\\Maziarka et al_2019_Mol-CycleGAN - a generative model for molecular optimization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XRLACAMJ\\1902.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZTFR75UR\\1902.html}
}

@article{mccammonComputeraidedMolecularDesign1987,
  title = {Computer-Aided Molecular Design},
  author = {McCammon, J. A.},
  date = {1987-10-23},
  journaltitle = {Science},
  volume = {238},
  number = {4826},
  eprint = {3310236},
  eprinttype = {pmid},
  pages = {486--491},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.3310236},
  url = {https://science.sciencemag.org/content/238/4826/486},
  urldate = {2021-04-08},
  abstract = {Theoretical chemistry, as implemented on fast computers, is beginning to yield accurate predictions of the thermodynamic and kinetic properties of large molecular assemblies. In addition to providing detailed insights into the origins of molecular activity, theoretical calculations can be used to design new molecules with specific properties. This article describes two types of calculations that show special promise as design tools, the thermodynamic cycle-perturbation method and the Brownian reactive dynamics method. These methods can be applied to calculate equilibrium and rate constants that describe many aspects of molecular recognition, stability, and reactivity.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\4VB5PHZC\486.html}
}

@unpublished{mccandlishEmpiricalModelLargeBatch2018,
  title = {An {{Empirical Model}} of {{Large-Batch Training}}},
  author = {McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Team, OpenAI Dota},
  date = {2018-12-14},
  eprint = {1812.06162},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1812.06162},
  urldate = {2020-05-18},
  abstract = {In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4RPQ5PZK\\McCandlish et al_2018_An Empirical Model of Large-Batch Training.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\I7YIIC7V\\1812.html}
}

@article{mccloskeyUsingAttributionDecode2019,
  title = {Using Attribution to Decode Binding Mechanism in Neural Network Models for Chemistry},
  author = {McCloskey, Kevin and Taly, Ankur and Monti, Federico and Brenner, Michael P. and Colwell, Lucy J.},
  date = {2019-05-24},
  journaltitle = {Proc Natl Acad Sci USA},
  pages = {201820657},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1820657116},
  url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1820657116},
  urldate = {2019-06-03},
  abstract = {Deep neural networks have achieved state-of-the-art accuracy at classifying molecules with respect to whether they bind to specific protein targets. A key breakthrough would occur if these models could reveal the fragment pharmacophores that are causally involved in binding. Extracting chemical details of binding from the networks could enable scientific discoveries about the mechanisms of drug actions. However, doing so requires shining light into the black box that is the trained neural network model, a task that has proved difficult across many domains. Here we show how the binding mechanism learned by deep neural network models can be interrogated, using a recently described attribution method. We first work with carefully constructed synthetic datasets, in which the molecular features responsible for “binding” are fully known. We find that networks that achieve perfect accuracy on held-out test datasets still learn spurious correlations, and we are able to exploit this nonrobustness to construct adversarial examples that fool the model. This makes these models unreliable for accurately revealing information about the mechanisms of protein–ligand binding. In light of our findings, we prescribe a test that checks whether a hypothesized mechanism can be learned. If the test fails, it indicates that the model must be simplified or regularized and/or that the training dataset requires augmentation.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\JMYIUSMB\McCloskey et al. - 2019 - Using attribution to decode binding mechanism in n.pdf}
}

@article{mcgaugheyUnderstandingCovariateShift2016,
  title = {Understanding Covariate Shift in Model Performance},
  author = {McGaughey, Georgia and Walters, W. Patrick and Goldman, Brian},
  date = {2016-04-07},
  journaltitle = {F1000Res},
  volume = {5},
  pages = {597},
  issn = {2046-1402},
  doi = {10.12688/f1000research.8317.1},
  url = {https://f1000research.com/articles/5-597/v1},
  urldate = {2019-06-21},
  abstract = {Three (3) different methods (logistic regression, covariate shift and k-NN) were applied to five (5) internal datasets and one (1) external, publically available dataset where covariate shift existed. In all cases, k-NN’s performance was inferior to either logistic regression or covariate shift. Surprisingly, there was no obvious advantage for using covariate shift to reweight the training data in the examined datasets.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7XYSEZU6\\McGaughey et al_2016_Understanding covariate shift in model performance.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\H8P2VRAH\\v1.html}
}

@unpublished{mehriSampleRNNUnconditionalEndtoEnd2016,
  title = {{{SampleRNN}}: {{An Unconditional End-to-End Neural Audio Generation Model}}},
  shorttitle = {{{SampleRNN}}},
  author = {Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
  date = {2016-12-22},
  eprint = {1612.07837},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1612.07837},
  urldate = {2019-10-21},
  abstract = {In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Sound},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\PL72BXQR\\Mehri et al_2016_SampleRNN.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DEDXM3PK\\1612.html}
}

@article{mendez-lucioNovoGenerationHitlike2018,
  title = {De {{Novo Generation}} of {{Hit-like Molecules}} from {{Gene Expression Signatures Using Artificial Intelligence}}},
  author = {Méndez-Lucio, Oscar and Baillif, Benoit and Clevert, Djork-Arné and Rouquié, David and Wichard, Joerg},
  date = {2018-11-05},
  doi = {10.26434/chemrxiv.7294388.v1},
  url = {https://chemrxiv.org/articles/De_Novo_Generation_of_Hit-like_Molecules_from_Gene_Expression_Signatures_Using_Artificial_Intelligence/7294388},
  urldate = {2019-05-06},
  abstract = {Finding new molecules with a desired biological activity is an extremely difficult task. In this context, artificial intelligence and generative models have been used for molecular de novo design and compound optimization. Herein, we report the first generative model that bridges systems biology and molecular design conditioning a generative adversarial network with transcriptomic data. By doing this we could generate molecules that have high probability to produce a desired biological effect at cellular level. We show that this model is able to design active-like molecules for desired targets without any previous target annotation of the training compounds as long as the gene expression signature of the desired state is provided. The molecules generated by this model are more similar to active compounds than the ones identified by similarity of gene expression signatures, which is the state-of-the-art method for navigating compound-induced gene expression data. Overall, this method represents a novel way to bridge chemistry and biology to advance in the long and difficult road of drug discovery.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\UGUJEHYF\Méndez-Lucio et al_2018_De Novo Generation of Hit-like Molecules from Gene Expression Signatures Using.pdf}
}

@unpublished{merityRegularizingOptimizingLSTM2017,
  title = {Regularizing and {{Optimizing LSTM Language Models}}},
  author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  date = {2017-08-07},
  eprint = {1708.02182},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1708.02182},
  urldate = {2019-10-22},
  abstract = {Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\99M95XX8\\Merity et al_2017_Regularizing and Optimizing LSTM Language Models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YL2LTE6S\\1708.html}
}

@article{merkNovoDesignBioactive2018,
  title = {De {{Novo Design}} of {{Bioactive Small Molecules}} by {{Artificial Intelligence}}},
  author = {Merk, Daniel and Friedrich, Lukas and Grisoni, Francesca and Schneider, Gisbert},
  date = {2018-01},
  journaltitle = {Mol Inform},
  volume = {37},
  number = {1-2},
  eprint = {29319225},
  eprinttype = {pmid},
  issn = {1868-1743},
  doi = {10.1002/minf.201700153},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5838524/},
  urldate = {2019-05-06},
  abstract = {Generative artificial intelligence offers a fresh view on molecular design. We present the first‐time prospective application of a deep learning model for designing new druglike compounds with desired activities. For this purpose, we trained a recurrent neural network to capture the constitution of a large set of known bioactive compounds represented as SMILES strings. By transfer learning, this general model was fine‐tuned on recognizing retinoid X and peroxisome proliferator‐activated receptor agonists. We synthesized five top‐ranking compounds designed by the generative model. Four of the compounds revealed nanomolar to low‐micromolar receptor modulatory activity in cell‐based assays. Apparently, the computational model intrinsically captured relevant chemical and biological knowledge without the need for explicit rules. The results of this study advocate generative artificial intelligence for prospective de novo molecular design, and demonstrate the potential of these methods for future medicinal chemistry.},
  pmcid = {PMC5838524},
  keywords = {Automation,drug discovery,machine learning,medicinal chemistry,nuclear receptor},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\5TI7LZX5\\Merk et al_2018_De Novo Design of Bioactive Small Molecules by Artificial Intelligence.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\CUKAVCBB\\Merk et al_2018_De Novo Design of Bioactive Small Molecules by Artificial Intelligence3.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FGM2SXEP\\Merk et al_2018_De Novo Design of Bioactive Small Molecules by Artificial Intelligence2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TEN2CP32\\Merk et al_2018_De Novo Design of Bioactive Small Molecules by Artificial Intelligence4.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\4RH7TJ5G\\minf.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\8DQUQT7C\\minf.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\YE5PYWJA\\minf.html}
}

@article{merkTuningArtificialIntelligence2018,
  title = {Tuning Artificial Intelligence on the de Novo Design of Natural-Product-Inspired Retinoid {{X}} Receptor Modulators},
  author = {Merk, Daniel and Grisoni, Francesca and Friedrich, Lukas and Schneider, Gisbert},
  date = {2018-10-22},
  journaltitle = {Nature Communications Chemistry},
  volume = {1},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2399-3669},
  doi = {10.1038/s42004-018-0068-1},
  url = {https://www.nature.com/articles/s42004-018-0068-1},
  urldate = {2020-04-21},
  abstract = {Artificial intelligence approaches to medicinal chemistry are increasingly powerful but struggle to predict bioactive molecules. Here a machine learning model generates synthetically accessible mimetics of natural products, which are shown to be bioactive against the retinoid X receptor.},
  issue = {1},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\5A7PJ3P4\\Merk et al_2018_Tuning artificial intelligence on the de novo design of.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HFF3HNLB\\s42004-018-0068-1.html}
}

@unpublished{metzDiscreteSequentialPrediction2019,
  title = {Discrete {{Sequential Prediction}} of {{Continuous Actions}} for {{Deep RL}}},
  author = {Metz, Luke and Ibarz, Julian and Jaitly, Navdeep and Davidson, James},
  date = {2019-06-07},
  eprint = {1705.05035},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1705.05035},
  urldate = {2020-10-22},
  abstract = {It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that predict one dimension at a time. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.},
  version = {3},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\I7XYRZEJ\\Metz et al_2019_Discrete Sequential Prediction of Continuous Actions for Deep RL.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TC97DKAR\\1705.html}
}

@article{meyersNovoMolecularDesign2021,
  title = {De Novo Molecular Design and Generative Models},
  author = {Meyers, Joshua and Fabian, Benedek and Brown, Nathan},
  date = {2021-11-01},
  journaltitle = {Drug Discovery Today},
  volume = {26},
  number = {11},
  pages = {2707--2715},
  issn = {1359-6446},
  doi = {10.1016/j.drudis.2021.05.019},
  url = {https://www.sciencedirect.com/science/article/pii/S1359644621002531},
  urldate = {2024-01-05},
  abstract = {Molecular design strategies are integral to therapeutic progress in drug discovery. Computational approaches for de novo molecular design have been developed over the past three decades and, recently, thanks in part to advances in machine learning (ML) and artificial intelligence (AI), the drug discovery field has gained practical experience. Here, we review these learnings and present de novo approaches according to the coarseness of their molecular representation: that is, whether molecular design is modeled on an atom-based, fragment-based, or reaction-based paradigm. Furthermore, we emphasize the value of strong benchmarks, describe the main challenges to using these methods in practice, and provide a viewpoint on further opportunities for exploration and challenges to be tackled in the upcoming years.},
  keywords = {Artificial intelligence,Atom-based,Automated design,design,Fragment-based,Generative chemistry,Generative models,Molecular design,Molecular representation,Reaction-based},
  file = {C:\Users\USEBPERP\Zotero\storage\SL8MJ3AN\S1359644621002531.html}
}

@article{meyersNovoMolecularDesign2021a,
  title = {De Novo Molecular Design and Generative Models},
  author = {Meyers, Joshua and Fabian, Benedek and Brown, Nathan},
  date = {2021-11-01},
  journaltitle = {Drug Discovery Today},
  volume = {26},
  number = {11},
  pages = {2707--2715},
  issn = {1359-6446},
  doi = {10.1016/j.drudis.2021.05.019},
  url = {https://www.sciencedirect.com/science/article/pii/S1359644621002531},
  urldate = {2022-09-15},
  abstract = {Molecular design strategies are integral to therapeutic progress in drug discovery. Computational approaches for de novo molecular design have been developed over the past three decades and, recently, thanks in part to advances in machine learning (ML) and artificial intelligence (AI), the drug discovery field has gained practical experience. Here, we review these learnings and present de novo approaches according to the coarseness of their molecular representation: that is, whether molecular design is modeled on an atom-based, fragment-based, or reaction-based paradigm. Furthermore, we emphasize the value of strong benchmarks, describe the main challenges to using these methods in practice, and provide a viewpoint on further opportunities for exploration and challenges to be tackled in the upcoming years.},
  langid = {english},
  keywords = {Artificial intelligence,Atom-based,Automated design,design,Fragment-based,Generative chemistry,Generative models,Molecular design,Molecular representation,Reaction-based},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4GS7A9XR\\Meyers et al. - 2021 - De novo molecular design and generative models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\KNLIRQ66\\Meyers et al. - 2021 - De novo molecular design and generative models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LA7I9NJV\\Meyers et al. - 2021 - De novo molecular design and generative models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6QYCUVYA\\S1359644621002531.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\PWFFZPZT\\S1359644621002531.html}
}

@incollection{michelAreSixteenHeads2019,
  title = {Are {{Sixteen Heads Really Better}} than {{One}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Michel, Paul and Levy, Omer and Neubig, Graham},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and family=Alché-Buc, given=F., prefix=d\textbackslash textquotesingle, useprefix=false and Fox, E. and Garnett, R.},
  date = {2019},
  pages = {14014--14024},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/9551-are-sixteen-heads-really-better-than-one.pdf},
  urldate = {2019-12-19},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\83B6466G\\Michel et al_2019_Are Sixteen Heads Really Better than One.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\C8AKP53A\\9551-are-sixteen-heads-really-better-than-one.html}
}

@online{MightHaveGone,
  title = {I {{Might Have Gone}} a {{Little Bit Overboard}}},
  url = {https://dirtydatagirl.moogle.cc/rf7put/i-might-have-gone-a-little-bit-overboard},
  urldate = {2021-07-07},
  abstract = {When you're me you take it for granted that you're just going to go overboard. Hubster refuses to al},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\6VUHT7ZC\i-might-have-gone-a-little-bit-overboard.html}
}

@article{minskyStepsArtificialIntelligence1961,
  title = {Steps toward {{Artificial Intelligence}}},
  author = {Minsky, Marvin},
  date = {1961-01},
  journaltitle = {Proceedings of the IRE},
  volume = {49},
  number = {1},
  pages = {8--30},
  issn = {0096-8390},
  doi = {10.1109/JRPROC.1961.287775},
  url = {http://ieeexplore.ieee.org/document/4066245/},
  urldate = {2019-03-01},
  abstract = {The problems of heuristic programming-of making computers solve really difficult problems-are divided into five main areas: Search, Pattern-Recognition, Learning, Planning, and Induction.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\YTEV2U6W\Minsky_1961_Steps toward Artificial Intelligence.pdf}
}

@article{mishkinSystematicEvaluationCNN2017,
  title = {Systematic Evaluation of {{CNN}} Advances on the {{ImageNet}}},
  author = {Mishkin, Dmytro and Sergievskiy, Nikolay and Matas, Jiri},
  date = {2017-08},
  journaltitle = {Computer Vision and Image Understanding},
  volume = {161},
  eprint = {1606.02228},
  eprinttype = {arxiv},
  pages = {11--19},
  issn = {10773142},
  doi = {10.1016/j.cviu.2017.05.007},
  url = {http://arxiv.org/abs/1606.02228},
  urldate = {2020-02-03},
  abstract = {The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatibility with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the "deficit" is small suggesting independence of their benefits. We show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ZNWMSJNU\\Mishkin et al_2017_Systematic evaluation of CNN advances on the ImageNet.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LTF6SV6U\\1606.html}
}

@inproceedings{misraSelfSupervisedLearningPretextInvariant2020,
  title = {Self-{{Supervised Learning}} of {{Pretext-Invariant Representations}}},
  author = {Misra, Ishan and family=Maaten, given=Laurens, prefix=van der, useprefix=false},
  date = {2020},
  pages = {6707--6717},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Misra_Self-Supervised_Learning_of_Pretext-Invariant_Representations_CVPR_2020_paper.html},
  urldate = {2021-04-08},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\F2PJZHMZ\\Misra and Maaten - 2020 - Self-Supervised Learning of Pretext-Invariant Repr.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BMHRE2EJ\\Misra_Self-Supervised_Learning_of_Pretext-Invariant_Representations_CVPR_2020_paper.html}
}

@unpublished{mnihAsynchronousMethodsDeep2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adrià Puigdomènech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  date = {2016-02-04},
  eprint = {1602.01783},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1602.01783},
  urldate = {2018-07-02},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\EDJHV4Z2\\Mnih et al_2016_Asynchronous Methods for Deep Reinforcement Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SVUG7D95\\1602.html}
}

@article{mnihFastSimpleAlgorithm,
  title = {A Fast and Simple Algorithm for Training Neural Probabilistic Language Models},
  author = {Mnih, Andriy and Teh, Yee Whye},
  pages = {8},
  abstract = {In spite of their superior performance, neural probabilistic language models (NPLMs) remain far less widely used than n-gram models due to their notoriously long training times, which are measured in weeks even for moderately-sized datasets. Training NPLMs is computationally expensive because they are explicitly normalized, which leads to having to consider all words in the vocabulary when computing the log-likelihood gradients.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\Q3L3ZHUU\Mnih and Teh - A fast and simple algorithm for training neural pr.pdf}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  date = {2015-02},
  journaltitle = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  url = {https://www.nature.com/articles/nature14236},
  urldate = {2018-07-02},
  abstract = {The theory of reinforcement learning provides a normative account1, deeply rooted in psychological2 and neuroscientific3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems4,5, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms3. While reinforcement learning agents have achieved some successes in a variety of domains6,7,8, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks9,10,11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games12. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\DTJU7252\nature14236.html}
}

@online{ModellingChemicalReasoning,
  title = {Modelling {{Chemical Reasoning}} to {{Predict}} and {{Invent Reactions}} - {{Segler}} - 2017 - {{Chemistry}} \&\#8211; {{A European Journal}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1002/chem.201604556},
  urldate = {2019-01-09},
  file = {C:\Users\USEBPERP\Zotero\storage\75GNVB69\Modelling Chemical Reasoning to Predict and Invent Reactions - Segler - 2017 -.pdf}
}

@online{Mol_optMainMimosa,
  title = {Mol\_opt/Main/Mimosa at Main · Wenhao-Gao/Mol\_opt},
  url = {https://github.com/wenhao-gao/mol_opt/tree/main/main/mimosa},
  urldate = {2023-10-29},
  abstract = {Contribute to wenhao-gao/mol\_opt development by creating an account on GitHub.},
  langid = {english},
  organization = {GitHub},
  file = {C:\Users\USEBPERP\Zotero\storage\YKTADAJ4\mimosa.html}
}

@online{MolecularTransformerModel2021,
  title = {Molecular {{Transformer}}: {{A Model}} for {{Uncertainty-Calibrated Chemical Reaction Prediction}} | {{ACS Central Science}}},
  date = {2021-01-22},
  url = {https://pubs.acs.org/doi/10.1021/acscentsci.9b00576},
  urldate = {2021-01-22}
}

@article{molgaLogicTranslatingChemical2019,
  title = {The Logic of Translating Chemical Knowledge into Machine-Processable Forms: A Modern Playground for Physical-Organic Chemistry},
  shorttitle = {The Logic of Translating Chemical Knowledge into Machine-Processable Forms},
  author = {Molga, Karol and P.~Gajewska, Ewa and Szymkuć, Sara and A.~Grzybowski, Bartosz},
  date = {2019},
  journaltitle = {Reaction Chemistry \& Engineering},
  volume = {4},
  number = {9},
  pages = {1506--1521},
  doi = {10.1039/C9RE00076C},
  url = {https://pubs.rsc.org/en/content/articlelanding/2019/re/c9re00076c},
  urldate = {2019-10-02},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2RVGJSEJ\\Molga et al_2019_The logic of translating chemical knowledge into machine-processable forms.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2NF67E4X\\c9re00076c.html}
}

@article{moretBeamSearchAutomated,
  title = {Beam Search for Automated Design and Scoring of Novel {{ROR}} Ligands with Machine Intelligence},
  author = {Moret, Michael and Helmstädter, Moritz and Grisoni, Francesca and Schneider, Gisbert and Merk, Daniel},
  journaltitle = {Angewandte Chemie International Edition},
  volume = {n/a},
  number = {n/a},
  issn = {1521-3773},
  doi = {10.1002/anie.202104405},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.202104405},
  urldate = {2021-07-07},
  abstract = {Chemical language models enable de novo drug design without the requirement for explicit molecular construction rules. While such models have been applied to generate novel compounds with desired bioactivity, the actual prioritization and selection of the most promising computational designs remains challenging. In this work, we leveraged the probabilities learnt by chemical language models with the beam search algorithm as a model-intrinsic technique for automated molecule design and scoring. Prospective application of this method yielded three novel inverse agonists of retinoic acid receptor-related orphan receptors (RORs). Each design was synthesizable in three reaction steps and presented low-micromolar to nanomolar potency towards RORγ. This model-intrinsic sampling technique eliminates the strict need for external compound scoring functions, thereby further extending the applicability of generative artificial intelligence to data-driven drug discovery.},
  langid = {english},
  keywords = {de novo design,deep learning,drug discovery,neural network,nuclear receptor},
  file = {C:\Users\USEBPERP\Zotero\storage\WM345DF5\Moret et al. - Beam search for automated design and scoring of no.pdf}
}

@article{morganGenerationUniqueMachine1965,
  title = {The {{Generation}} of a {{Unique Machine Description}} for {{Chemical Structures-A Technique Developed}} at {{Chemical Abstracts Service}}.},
  author = {Morgan, H. L.},
  date = {1965-05-01},
  journaltitle = {J. Chem. Doc.},
  volume = {5},
  number = {2},
  pages = {107--113},
  publisher = {American Chemical Society},
  issn = {0021-9576},
  doi = {10.1021/c160017a018},
  url = {https://doi.org/10.1021/c160017a018},
  urldate = {2021-04-08},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BN4QRSZW\\Morgan - 1965 - The Generation of a Unique Machine Description for.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\NGSKLQAF\\c160017a018.html}
}

@online{MostDownloadedArticles,
  title = {Most {{Downloaded Articles}} - {{Drug Discovery Today}}: {{Technologies}} - {{Journal}} - {{Elsevier}}},
  shorttitle = {Most {{Downloaded Articles}} - {{Drug Discovery Today}}},
  url = {https://www.journals.elsevier.com/drug-discovery-today-technologies/journals.elsevier.com/drug-discovery-today-technologies/most-downloaded-articles},
  urldate = {2021-08-24},
  abstract = {The Drug Discovery Today reviews collection provides a resource of review content aligning the key output of human molecular medicine with the specific requir…},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\UZPSDN7Z\most-downloaded-articles.html}
}

@article{mountIcePickFlexibleSurfaceBased1999,
  title = {{{{\emph{IcePick}}}} : {{A Flexible Surface-Based System}} for {{Molecular Diversity}}},
  shorttitle = {{{{\emph{IcePick}}}}},
  author = {Mount, John and Ruppert, Jim and Welch, Will and Jain, Ajay N.},
  date = {1999-01-01},
  journaltitle = {J. Med. Chem.},
  volume = {42},
  number = {1},
  pages = {60--66},
  issn = {0022-2623, 1520-4804},
  doi = {10.1021/jm970775r},
  url = {https://pubs.acs.org/doi/10.1021/jm970775r},
  urldate = {2022-06-09},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\8Y5RPSG4\\Mount et al. - 1999 - IcePick  A Flexible Surface-Based System for Mole.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\U8VRRHTA\\Mount et al. - 1999 - IcePick  A Flexible Surface-Based System f.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\P28YXNQV\\jm970775r.html}
}

@article{mullardParsingClinicalSuccess2016,
  title = {Parsing Clinical Success Rates},
  author = {Mullard, Asher},
  date = {2016-07-01},
  journaltitle = {Nature Reviews Drug Discovery},
  volume = {15},
  number = {7},
  pages = {447--447},
  publisher = {Nature Publishing Group},
  issn = {1474-1784},
  doi = {10.1038/nrd.2016.136},
  url = {https://www.nature.com/articles/nrd.2016.136},
  urldate = {2024-04-28},
  langid = {english},
  keywords = {Business strategy in drug development,Clinical trials,Drug development},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\KI6DYZCB\\Mullard - 2016 - Parsing clinical success rates.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XAU5I4PM\\nrd.2016.html}
}

@article{mullerModelsIdentificationErroneous2012,
  title = {Models for {{Identification}} of {{Erroneous Atom-to-Atom Mapping}} of {{Reactions Performed}} by {{Automated Algorithms}}},
  author = {Muller, Christophe and Marcou, Gilles and Horvath, Dragos and Aires-de-Sousa, João and Varnek, Alexandre},
  date = {2012-12-21},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {52},
  number = {12},
  pages = {3116--3122},
  issn = {1549-9596},
  doi = {10.1021/ci300418q},
  url = {https://doi.org/10.1021/ci300418q},
  urldate = {2019-10-23},
  abstract = {Machine learning (SVM and JRip rule learner) methods have been used in conjunction with the Condensed Graph of Reaction (CGR) approach to identify errors in the atom-to-atom mapping of chemical reactions produced by an automated mapping tool by ChemAxon. The modeling has been performed on the three first enzymatic classes of metabolic reactions from the KEGG database. Each reaction has been converted into a CGR representing a pseudomolecule with conventional (single, double, aromatic, etc.) bonds and dynamic bonds characterizing chemical transformations. The ChemAxon tool was used to automatically detect the matching atom pairs in reagents and products. These automated mappings were analyzed by the human expert and classified as “correct” or “wrong”. ISIDA fragment descriptors generated for CGRs for both correct and wrong mappings were used as attributes in machine learning. The learned models have been validated in n-fold cross-validation on the training set followed by a challenge to detect correct and wrong mappings within an external test set of reactions, never used for learning. Results show that both SVM and JRip models detect most of the wrongly mapped reactions. We believe that this approach could be used to identify erroneous atom-to-atom mapping performed by any automated algorithm.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6LBEP6W7\\Muller et al_2012_Models for Identification of Erroneous Atom-to-Atom Mapping of Reactions.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2AR8GPL9\\ci300418q.html}
}

@unpublished{mullerWhenDoesLabel2019,
  title = {When {{Does Label Smoothing Help}}?},
  author = {Müller, Rafael and Kornblith, Simon and Hinton, Geoffrey},
  date = {2019-06-06},
  eprint = {1906.02629},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.02629},
  urldate = {2019-06-12},
  abstract = {The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\IGMIAHFT\\Müller et al_2019_When Does Label Smoothing Help.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2LABCDAA\\1906.html}
}

@online{MultiobjectiveNovoDrug,
  title = {Multi-Objective de Novo Drug Design with Conditional Graph Generative Model | {{Journal}} of {{Cheminformatics}} | {{Full Text}}},
  url = {https://jcheminf.biomedcentral.com/articles/10.1186/s13321-018-0287-6},
  urldate = {2019-05-06},
  file = {C:\Users\USEBPERP\Zotero\storage\DDMKZPNL\s13321-018-0287-6.html}
}

@article{munirDeepAnTDeepLearning2019,
  title = {{{DeepAnT}}: {{A Deep Learning Approach}} for {{Unsupervised Anomaly Detection}} in {{Time Series}}},
  shorttitle = {{{DeepAnT}}},
  author = {Munir, Mohsin and Siddiqui, Shoaib Ahmed and Dengel, Andreas and Ahmed, Sheraz},
  date = {2019},
  journaltitle = {IEEE Access},
  volume = {7},
  pages = {1991--2005},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2886457},
  abstract = {Traditional distance and density-based anomaly detection techniques are unable to detect periodic and seasonality related point anomalies which occur commonly in streaming data, leaving a big gap in time series anomaly detection in the current era of the IoT. To address this problem, we present a novel deep learning-based anomaly detection approach (DeepAnT) for time series data, which is equally applicable to the non-streaming cases. DeepAnT is capable of detecting a wide range of anomalies, i.e., point anomalies, contextual anomalies, and discords in time series data. In contrast to the anomaly detection methods where anomalies are learned, DeepAnT uses unlabeled data to capture and learn the data distribution that is used to forecast the normal behavior of a time series. DeepAnT consists of two modules: time series predictor and anomaly detector. The time series predictor module uses deep convolutional neural network (CNN) to predict the next time stamp on the defined horizon. This module takes a window of time series (used as a context) and attempts to predict the next time stamp. The predicted value is then passed to the anomaly detector module, which is responsible for tagging the corresponding time stamp as normal or abnormal. DeepAnT can be trained even without removing the anomalies from the given data set. Generally, in deep learning-based approaches, a lot of data are required to train a model. Whereas in DeepAnT, a model can be trained on relatively small data set while achieving good generalization capabilities due to the effective parameter sharing of the CNN. As the anomaly detection in DeepAnT is unsupervised, it does not rely on anomaly labels at the time of model generation. Therefore, this approach can be directly applied to real-life scenarios where it is practically impossible to label a big stream of data coming from heterogeneous sensors comprising of both normal as well as anomalous points. We have performed a detailed evaluation of 15 algorithms on 10 anomaly detection benchmarks, which contain a total of 433 real and synthetic time series. Experiments show that DeepAnT outperforms the state-of-the-art anomaly detection methods in most of the cases, while performing on par with others.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Anomaly detection,artificial intelligence,Benchmark testing,Clustering algorithms,convolutional neural network,Data models,deep neural networks,Heuristic algorithms,recurrent neural networks,time series analysis,Time series analysis},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\IF4FVD89\\Munir et al. - 2019 - DeepAnT A Deep Learning Approach for Unsupervised.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XZX4B88L\\stamp.html}
}

@article{munirFuseADUnsupervisedAnomaly2019,
  title = {{{FuseAD}}: {{Unsupervised Anomaly Detection}} in {{Streaming Sensors Data}} by {{Fusing Statistical}} and {{Deep Learning Models}}},
  shorttitle = {{{FuseAD}}},
  author = {Munir, Mohsin and Siddiqui, Shoaib Ahmed and Chattha, Muhammad Ali and Dengel, Andreas and Ahmed, Sheraz},
  date = {2019-01},
  journaltitle = {Sensors},
  volume = {19},
  number = {11},
  pages = {2451},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1424-8220},
  doi = {10.3390/s19112451},
  url = {https://www.mdpi.com/1424-8220/19/11/2451},
  urldate = {2023-01-06},
  abstract = {The need for robust unsupervised anomaly detection in streaming data is increasing rapidly in the current era of smart devices, where enormous data are gathered from numerous sensors. These sensors record the internal state of a machine, the external environment, and the interaction of machines with other machines and humans. It is of prime importance to leverage this information in order to minimize downtime of machines, or even avoid downtime completely by constant monitoring. Since each device generates a different type of streaming data, it is normally the case that a specific kind of anomaly detection technique performs better than the others depending on the data type. For some types of data and use-cases, statistical anomaly detection techniques work better, whereas for others, deep learning-based techniques are preferred. In this paper, we present a novel anomaly detection technique, FuseAD, which takes advantage of both statistical and deep-learning-based approaches by fusing them together in a residual fashion. The obtained results show an increase in area under the curve (AUC) as compared to state-of-the-art anomaly detection methods when FuseAD is tested on a publicly available dataset (Yahoo Webscope benchmark). The obtained results advocate that this fusion-based technique can obtain the best of both worlds by combining their strengths and complementing their weaknesses. We also perform an ablation study to quantify the contribution of the individual components in FuseAD, i.e., the statistical ARIMA model as well as the deep-learning-based convolutional neural network (CNN) model.},
  issue = {11},
  langid = {english},
  keywords = {anomaly detection,deep neural networks,model fusion,sensor data,statistical models,time-series analysis},
  file = {C:\Users\USEBPERP\Zotero\storage\TQHISJJT\Munir et al. - 2019 - FuseAD Unsupervised Anomaly Detection in Streamin.pdf}
}

@unpublished{nabiOptimalTrainingFair2019,
  title = {Optimal {{Training}} of {{Fair Predictive Models}}},
  author = {Nabi, Razieh and Malinsky, Daniel and Shpitser, Ilya},
  date = {2019-10-09},
  eprint = {1910.04109},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1910.04109},
  urldate = {2019-10-16},
  abstract = {Recently there has been sustained interest in modifying prediction algorithms to satisfy fairness constraints. These constraints are typically complex nonlinear functionals of the observed data distribution. Focusing on the causal constraints proposed by Nabi and Shpitser (2018), we introduce new theoretical results and optimization techniques to make model training easier and more accurate. Specifically, we show how to reparameterize the observed data likelihood such that fairness constraints correspond directly to parameters that appear in the likelihood, transforming a complex constrained optimization objective into a simple optimization problem with box constraints. We also exploit methods from empirical likelihood theory in statistics to improve predictive performance, without requiring parametric models for high-dimensional feature vectors.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4HVJJ5V4\\Nabi et al_2019_Optimal Training of Fair Predictive Models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\KL3U9QSF\\1910.html}
}

@online{NachTerroranschlagWien,
  title = {Nach Terroranschlag in Wien: EU plant offenbar Verschlüsselungsverbot},
  shorttitle = {Nach Terroranschlag in Wien},
  url = {https://www.derstandard.at/story/2000121553240/nach-terroranschlag-in-wien-eu-plant-offenbar-verschluesselungsverbot},
  urldate = {2020-11-09},
  abstract = {Whatsapp und Co sollen nach Ansicht der Staats- und Regierungschefs künftig einen Generalschlüssel für verschlüsselte Chats bereitstellen},
  langid = {austrian},
  organization = {DER STANDARD}
}

@article{nakamuraSelectingMoleculesDiverse2022,
  title = {Selecting Molecules with Diverse Structures and Properties by Maximizing Submodular Functions of Descriptors Learned with Graph Neural Networks},
  author = {Nakamura, Tomohiro and Sakaue, Shinsaku and Fujii, Kaito and Harabuchi, Yu and Maeda, Satoshi and Iwata, Satoru},
  date = {2022-01-21},
  journaltitle = {Sci Rep},
  volume = {12},
  number = {1},
  pages = {1124},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-04967-9},
  url = {https://www.nature.com/articles/s41598-022-04967-9},
  urldate = {2023-09-01},
  abstract = {Selecting diverse molecules from unexplored areas of chemical space is one of the most important tasks for discovering novel molecules and reactions. This paper proposes a new approach for selecting a subset of diverse molecules from a given molecular list by using two existing techniques studied in machine learning and mathematical optimization: graph neural networks (GNNs) for learning vector representation of molecules and a diverse-selection framework called submodular function maximization. Our method, called SubMo-GNN, first trains a GNN with property prediction tasks, and then the trained GNN transforms molecular graphs into molecular vectors, which capture both properties and structures of molecules. Finally, to obtain a subset of diverse molecules, we define a submodular function, which quantifies the diversity of molecular vectors, and find a subset of molecular vectors with a large submodular function value. This can be done efficiently by using the greedy algorithm, and the diversity of selected molecules measured by the submodular function value is mathematically guaranteed to be at least 63\% of that of an optimal selection. We also introduce a new evaluation criterion to measure the diversity of selected molecules based on molecular properties. Computational experiments confirm that our SubMo-GNN successfully selects diverse molecules from the QM9 dataset regarding the property-based criterion, while performing comparably to existing methods regarding standard structure-based criteria. We also demonstrate that SubMo-GNN with a GNN trained on the QM9 dataset can select diverse molecules even from other MoleculeNet datasets whose domains are different from the QM9 dataset. The proposed method enables researchers to obtain diverse sets of molecules for discovering new molecules and novel chemical reactions, and the proposed diversity criterion is useful for discussing the diversity of molecular libraries from a new property-based perspective.},
  issue = {1},
  langid = {english},
  keywords = {Applied mathematics,Cheminformatics,Computational science},
  file = {C:\Users\USEBPERP\Zotero\storage\GYZ42SZC\Nakamura et al. - 2022 - Selecting molecules with diverse structures and pr.pdf}
}

@article{nalisnickDeepGenerativeModels2018,
  ids = {nalisnickDeepGenerativeModels2018a},
  title = {Do {{Deep Generative Models Know What They Don}}'t {{Know}}?},
  author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=H1xwNhCcYm},
  urldate = {2019-04-24},
  abstract = {A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data.  A plethora of work has demonstrated...},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\9UZ8MT36\\Nalisnick et al_2018_Do Deep Generative Models Know What They Don't Know3.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AVLJPNXY\\Nalisnick et al_2018_Do Deep Generative Models Know What They Don't Know.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZU7SMSIF\\Nalisnick et al_2018_Do Deep Generative Models Know What They Don't Know2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\93TDPJCP\\forum.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\FMNN4ZJP\\1810.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\XWKH4PMB\\1810.html}
}

@unpublished{nalisnickDetectingOutofDistributionInputs2019,
  title = {Detecting {{Out-of-Distribution Inputs}} to {{Deep Generative Models Using}} a {{Test}} for {{Typicality}}},
  author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Lakshminarayanan, Balaji},
  date = {2019-06-07},
  eprint = {1906.02994},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.02994},
  urldate = {2019-08-14},
  abstract = {Recent work has shown that deep generative models can assign higher likelihood to out-of-distribution data sets than to their training data. We posit that this phenomenon is caused by a mismatch between the model's typical set and its areas of high probability density. In-distribution inputs should reside in the former but not necessarily in the latter, as previous work has presumed. To determine whether or not inputs reside in the typical set, we propose a statistically principled, easy-to-implement test using the empirical distribution of model likelihoods. The test is model agnostic and widely applicable, only requiring that the likelihood can be computed or closely approximated. We report experiments showing that our procedure can successfully detect the out-of-distribution sets in several of the challenging cases reported by Nalisnick et al. (2019).},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ZBPFQFZU\\Nalisnick et al_2019_Detecting Out-of-Distribution Inputs to Deep Generative Models Using a Test for.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\B66WWIW4\\1906.html}
}

@incollection{nalmpantisSignal2VecTimeSeries2019,
  title = {{{Signal2Vec}}: {{Time Series Embedding Representation}}},
  shorttitle = {{{Signal2Vec}}},
  booktitle = {Engineering {{Applications}} of {{Neural Networks}}},
  author = {Nalmpantis, Christoforos and Vrakas, Dimitris},
  editor = {Macintyre, John and Iliadis, Lazaros and Maglogiannis, Ilias and Jayne, Chrisina},
  date = {2019},
  volume = {1000},
  pages = {80--90},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-20257-6_7},
  url = {http://link.springer.com/10.1007/978-3-030-20257-6_7},
  urldate = {2022-08-16},
  abstract = {The rise of Internet-of-Things (IoT) and the exponential increase of devices using sensors, has lead to an increasing interest in data mining of time series. In this context, several representation methods have been proposed. Signal2vec is a novel framework, which can represent any time-series in a vector space. It is unsupervised, computationally efficient, scalable and generic. The framework is evaluated via a theoretical analysis and real world applications, with a focus on energy data. The experimental results are compared against a baseline using raw data and two other popular representations, SAX and PAA. Signal2vec is superior not only in terms of performance, but also in efficiency, due to dimensionality reduction.},
  isbn = {978-3-030-20256-9 978-3-030-20257-6},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\FLSFIKEZ\Nalmpantis and Vrakas - 2019 - Signal2Vec Time Series Embedding Representation.pdf}
}

@unpublished{namLinkingNeuralMachine2016,
  title = {Linking the {{Neural Machine Translation}} and the {{Prediction}} of {{Organic Chemistry Reactions}}},
  author = {Nam, Juno and Kim, Jurae},
  date = {2016-12-29},
  eprint = {1612.09529},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1612.09529},
  urldate = {2021-04-08},
  abstract = {Finding the main product of a chemical reaction is one of the important problems of organic chemistry. This paper describes a method of applying a neural machine translation model to the prediction of organic chemical reactions. In order to translate 'reactants and reagents' to 'products', a gated recurrent unit based sequence-to-sequence model and a parser to generate input tokens for model from reaction SMILES strings were built. Training sets are composed of reactions from the patent databases, and reactions manually generated applying the elementary reactions in an organic chemistry textbook of Wade. The trained models were tested by examples and problems in the textbook. The prediction process does not need manual encoding of rules (e.g., SMARTS transformations) to predict products, hence it only needs sufficient training reaction sets to learn new types of reactions.},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\M24K3M5G\\Nam and Kim - 2016 - Linking the Neural Machine Translation and the Pre.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\B8T7KZ98\\1612.html}
}

@online{nandinijammiBiggestStoryTech2021,
  type = {Tweet},
  title = {The Biggest Story in Tech No One’s Talking about Is {{Uber}} Discovering They’d Been Defrauded out of \${{100M}} - or 2/3 of Their Ad Spend. {{And}} All Bc {{Sleeping Giants}} Kept Bugging Them to Block Their Ads on {{Breitbart}}. {{https://t.co/SiS3MndewS}}},
  author = {{Nandini Jammi}},
  date = {2021-01-03T16:51Z},
  url = {https://twitter.com/nandoodles/status/1345774768746852353},
  urldate = {2021-01-04},
  langid = {english},
  organization = {@nandoodles},
  file = {C:\Users\USEBPERP\Zotero\storage\MUK3D75I\1345774768746852353.html}
}

@article{neilEXPLORINGDEEPRECURRENT2018,
  title = {{{EXPLORING DEEP RECURRENT MODELS WITH REIN- FORCEMENT LEARNING FOR MOLECULE DESIGN}}},
  author = {Neil, Daniel and Segler, Marwin and Guasch, Laura and Ahmed, Mohamed and Plumbley, Dean and Sellwood, Matthew and Brown, Nathan},
  date = {2018},
  pages = {15},
  abstract = {The design of small molecules with bespoke properties is of central importance to drug discovery. However significant challenges yet remain for computational methods, despite recent advances such as deep recurrent networks and reinforcement learning strategies for sequence generation, and it can be difficult to compare results across different works. This work proposes 19 benchmarks selected by subject experts, expands smaller datasets previously used to approximately 1.1 million training molecules, and explores how to apply new reinforcement learning techniques effectively for molecular design. The benchmarks here, built as OpenAI Gym environments, will be open-sourced to encourage innovation in molecular design algorithms and to enable usage by those without a background in chemistry. Finally, this work explores recent development in reinforcement-learning methods with excellent sample complexity (the A2C and PPO algorithms) and investigates their behavior in molecular generation, demonstrating significant performance gains compared to standard reinforcement learning techniques.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\T4R6MLRD\Neil et al. - 2018 - EXPLORING DEEP RECURRENT MODELS WITH REIN- FORCEME.pdf}
}

@online{neilExploringDeepRecurrent2018a,
  title = {Exploring {{Deep Recurrent Models}} with {{Reinforcement Learning}} for {{Molecule Design}}},
  author = {Neil, Daniel and Segler, Marwin and Guasch, Laura and Ahmed, Mohamed and Plumbley, Dean and Sellwood, Matthew and Brown, Nathan},
  date = {2018-02-15},
  url = {https://openreview.net/forum?id=HkcTe-bR-},
  urldate = {2019-05-06},
  abstract = {The design of small molecules with bespoke properties is of central importance to drug discovery.  However significant challenges yet remain for computational methods, despite recent advances such...},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\3Y5P9HVK\\Neil et al_2018_Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LGK2YBKJ\\Neil et al_2018_Exploring Deep Recurrent Models with Reinforcement Learning for Molecule Design.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GKMFNYT4\\forum.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\U7R5BEAT\\forum.html}
}

@online{NeuralNetworksPrediction,
  title = {Neural {{Networks}} for the {{Prediction}} of {{Organic Chemistry Reactions}} | {{ACS Central Science}}},
  url = {https://pubs.acs.org/doi/abs/10.1021/acscentsci.6b00219},
  urldate = {2020-11-19},
  file = {C:\Users\USEBPERP\Zotero\storage\Y6N96KQL\acscentsci.html}
}

@inproceedings{newellChessMachineExample1955,
  title = {The {{Chess Machine}}: {{An Example}} of {{Dealing}} with a {{Complex Task}} by {{Adaptation}}},
  shorttitle = {The {{Chess Machine}}},
  booktitle = {Proceedings of the {{March}} 1-3, 1955, {{Western Joint Computer Conference}}},
  author = {Newell, Allen},
  date = {1955},
  series = {{{AFIPS}} '55 ({{Western}})},
  pages = {101--108},
  publisher = {ACM},
  location = {New York, NY, USA},
  doi = {10.1145/1455292.1455312},
  url = {http://doi.acm.org/10.1145/1455292.1455312},
  urldate = {2019-03-01},
  abstract = {The modern general-purpose computer can be characterized as the embodiment of a three-point philosophy: (1) There shall exist a way of computing anything computable; (2) The computer shall be so fast that it does not matter how complicated the way is; and (3) Man shall be so intelligent that he will be able to discern the way and instruct the computer.},
  venue = {Los Angeles, California},
  file = {C:\Users\USEBPERP\Zotero\storage\AE3LTSMR\Newell_1955_The Chess Machine.pdf}
}

@online{NewsORF,
  title = {news.ORF.at},
  url = {https://orf.at/},
  urldate = {2020-11-04},
  abstract = {news.ORF.at: Die aktuellsten Nachrichten auf einen Blick - aus Österreich und der ganzen Welt. In Text, Bild und Video.},
  langid = {ngerman},
  organization = {news.ORF.at},
  file = {C:\Users\USEBPERP\Zotero\storage\HHYH4JI6\orf.at.html}
}

@incollection{newtonMolecularDiversityDrug2002,
  title = {Molecular {{Diversity}} in {{Drug Design}}. {{Application}} to {{High-speed Synthesis}} and {{High-Throughput Screening}}},
  booktitle = {Molecular {{Diversity}} in {{Drug Design}}},
  author = {Newton, Christopher G.},
  editor = {Dean, Philip M. and Lewis, Richard A.},
  date = {2002},
  pages = {23--42},
  publisher = {Springer Netherlands},
  location = {Dordrecht},
  doi = {10.1007/0-306-46873-5_2},
  url = {https://doi.org/10.1007/0-306-46873-5_2},
  urldate = {2022-06-07},
  abstract = {The goal of High Speed Synthesis, High Throughput Screening and Molecular Diversity technologies in the pharmaceutical industry is to reduce the cost of finding good quality leads against a pharmaceutical target. Good quality leads should allow faster optimisation to candidate drugs. It is vital to maintain this perspective when discussing the advantages of these enabling technologies and the large costs associated with their implementation and running. The focus of this paper will be on reviewing the factors that seems to explain why some compounds are better leads and candidate drugs than others. This will help to set out the strategic decisions that have to be made, to try to optimise the benefits and synergies of HSS, HTS and diversity. The conclusion is that considerations of pharmacological conformity — that the molecules designed have the best chance of being fit-for-purpose — should be placed before considerations of how diverse molecules are from one another.},
  isbn = {978-0-306-46873-5},
  langid = {english},
  keywords = {Bioavailability,Pharmacodynamics,Pharmacokinetics,Solubility}
}

@online{NextMoveSoftwareTalks,
  title = {{{NextMove Software}} | {{Talks}}},
  url = {https://www.nextmovesoftware.com/talks.html},
  urldate = {2023-11-17},
  file = {C:\Users\USEBPERP\Zotero\storage\EVKP5DWZ\talks.html}
}

@unpublished{neyshaburWhatBeingTransferred2020,
  title = {What Is Being Transferred in Transfer Learning?},
  author = {Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  date = {2020-08-26},
  eprint = {2008.11687},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2008.11687},
  urldate = {2020-08-31},
  abstract = {One desired capability for machines is the ability to transfer their knowledge of one domain to another where data is (usually) scarce. Despite ample adaptation of transfer learning in various deep learning applications, we yet do not understand what enables a successful transfer and which part of the network is responsible for that. In this paper, we provide new tools and analyses to address these fundamental questions. Through a series of analyses on transferring to block-shuffled images, we separate the effect of feature reuse from learning low-level statistics of data and show that some benefit of transfer learning comes from the latter. We present that when training from pre-trained weights, the model stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.},
  version = {1},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\I2ATTYQE\\Neyshabur et al_2020_What is being transferred in transfer learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XCLR8DRS\\2008.html}
}

@article{ngChallengesOpportunitiesComputeraided2015,
  title = {Challenges and Opportunities in Computer-Aided Molecular Design},
  author = {Ng, Lik Yin and Chong, Fah Keen and Chemmangattuvalappil, Nishanth G.},
  date = {2015-10-04},
  journaltitle = {Computers \& Chemical Engineering},
  series = {Special {{Issue}}: {{Selected}} Papers from the 8th {{International Symposium}} on the {{Foundations}} of {{Computer-Aided Process Design}} ({{FOCAPD}} 2014), {{July}} 13-17, 2014, {{Cle Elum}}, {{Washington}}, {{USA}}},
  volume = {81},
  pages = {115--129},
  issn = {0098-1354},
  doi = {10.1016/j.compchemeng.2015.03.009},
  url = {https://www.sciencedirect.com/science/article/pii/S0098135415000800},
  urldate = {2021-04-08},
  abstract = {In this paper, the significant development, current challenges and future opportunities in the field of chemical product design using computer-aided molecular design (CAMD) tools are highlighted. With the gaining of focus on the design of novel and improved chemical products, the traditional heuristic based approaches may not be effective in designing optimal products. This leads to the vast development and application of CAMD tools, which are methods that combine property prediction models with computer-assisted search in the design of various chemical products. The introduction and development of different classes of property prediction methods in the overall product design process is discussed. The exploration and application of CAMD tools in numerous single component product designs, mixture design, and later in the integrated process-product design are reviewed in this paper. Difficulties and possible future extension of CAMD are then discussed in detail. The highlighted challenges and opportunities are mainly about the needs for exploration and development of property models, suitable design scale and computational effort as well as sustainable chemical product design framework. In order to produce a chemical product in a sustainable way, the role of each level in a chemical product design enterprise hierarchy is discussed. In addition to process parameters and product quality, environment, health and safety performance are required to be considered in shaping a sustainable chemical product design framework. On top of these, recent developments and opportunities in the design of ionic liquids using molecular design techniques have been discussed.},
  langid = {english},
  keywords = {Chemical product design,Computer-aided molecular design,Ionic liquids,Property estimation methods,Sustainable chemical product design},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\F6NSM65S\\Ng et al. - 2015 - Challenges and opportunities in computer-aided mol.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GUHB9Z8Y\\S0098135415000800.html}
}

@unpublished{nguyenDeepNeuralNetworks2015,
  title = {Deep {{Neural Networks}} Are {{Easily Fooled}}: {{High Confidence Predictions}} for {{Unrecognizable Images}}},
  shorttitle = {Deep {{Neural Networks}} Are {{Easily Fooled}}},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  date = {2015-04-02},
  eprint = {1412.1897},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1412.1897},
  urldate = {2020-03-10},
  abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-the-art DNNs believe to be recognizable objects with 99.99\% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call "fooling images" (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\J6PCZ88V\\Nguyen et al_2015_Deep Neural Networks are Easily Fooled.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\9SJ43PM3\\1412.html}
}

@unpublished{nguyenGenerativeModelMolecule2021,
  title = {A Generative Model for Molecule Generation Based on Chemical Reaction Trees},
  author = {Nguyen, Dai Hai and Tsuda, Koji},
  date = {2021-06-07},
  eprint = {2106.03394},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.03394},
  urldate = {2021-06-21},
  abstract = {Deep generative models have been shown powerful in generating novel molecules with desired chemical properties via their representations such as strings, trees or graphs. However, these models are limited in recommending synthetic routes for the generated molecules in practice. We propose a generative model to generate molecules via multi-step chemical reaction trees. Specifically, our model first propose a chemical reaction tree with predicted reaction templates and commercially available molecules (starting molecules), and then perform forward synthetic steps to obtain product molecules. Experiments show that our model can generate chemical reactions whose product molecules are with desired chemical properties. Also, the complete synthetic routes for these product molecules are provided.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\GDU6XL4I\\Nguyen and Tsuda - 2021 - A generative model for molecule generation based o.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8X5ZSAUL\\2106.html}
}

@unpublished{nguyenLossLandscapeClass2018,
  title = {On the Loss Landscape of a Class of Deep Neural Networks with No Bad Local Valleys},
  author = {Nguyen, Quynh and Mukkamala, Mahesh Chandra and Hein, Matthias},
  date = {2018-09-27},
  eprint = {1809.10749},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1809.10749},
  urldate = {2018-10-01},
  abstract = {We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley, in the sense that from any point in parameter space there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This implies that these networks have no sub-optimal strict local minima.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BRWRADSB\\Nguyen et al_2018_On the loss landscape of a class of deep neural networks with no bad local.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\9CNZA95B\\1809.html}
}

@unpublished{nguyenScalableInterpretableOneclass2018,
  title = {Scalable and {{Interpretable One-class SVMs}} with {{Deep Learning}} and {{Random Fourier}} Features},
  author = {Nguyen, Minh-Nghia and Vien, Ngo Anh},
  date = {2018-04-13},
  eprint = {1804.04888},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.04888},
  urldate = {2019-07-08},
  abstract = {One-class support vector machine (OC-SVM) for a long time has been one of the most effective anomaly detection methods and extensively adopted in both research as well as industrial applications. The biggest issue for OC-SVM is yet the capability to operate with large and high-dimensional datasets due to optimization complexity. Those problems might be mitigated via dimensionality reduction techniques such as manifold learning or autoencoder. However, previous work often treats representation learning and anomaly prediction separately. In this paper, we propose autoencoder based one-class support vector machine (AE-1SVM) that brings OC-SVM, with the aid of random Fourier features to approximate the radial basis kernel, into deep learning context by combining it with a representation learning architecture and jointly exploit stochastic gradient descent to obtain end-to-end training. Interestingly, this also opens up the possible use of gradient-based attribution methods to explain the decision making for anomaly detection, which has ever been challenging as a result of the implicit mappings between the input space and the kernel space. To the best of our knowledge, this is the first work to study the interpretability of deep learning in anomaly detection. We evaluate our method on a wide range of unsupervised anomaly detection tasks in which our end-to-end training architecture achieves a performance significantly better than the previous work using separate training.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\TUCFG2JX\\Nguyen_Vien_2018_Scalable and Interpretable One-class SVMs with Deep Learning and Random Fourier.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\Z4VDGRTN\\1804.html}
}

@article{nicaEVALUATINGGENERALIZATIONGFLOWNETS2022,
  title = {{{EVALUATING GENERALIZATION IN GFLOWNETS FOR MOLECULE DESIGN}}},
  author = {Nica, Andrei C and Jain, Moksh and Bengio, Emmanuel and Liu, Cheng-Hao and Korablyov, Maksym and Bronstein, Michael M and Bengio, Yoshua},
  date = {2022},
  pages = {16},
  abstract = {Deep learning bears promise for drug discovery problems such as de novo molecular design. Generating data to train such models is a costly and time-consuming process, given the need for wet-lab experiments or expensive simulations. This problem is compounded by the notorious data-hungriness of machine learning algorithms. In small molecule generation the recently proposed GFlowNet method has shown good performance in generating diverse high-scoring candidates, and has the interesting advantage of being an off-policy offline method. Finding an appropriate generalization evaluation metric for such models, one predictive of the desired search performance (i.e. finding high-scoring diverse candidates), will help guide online data collection for such an algorithm. In this work, we develop techniques for evaluating GFlowNet performance on a test set, and identify the most promising metric for predicting generalization. We present empirical results on several small-molecule design tasks in drug discovery, for several GFlowNet training setups, and we find a metric strongly correlated with diverse high-scoring batch generation. This metric should be used to identify the best generative model from which to sample batches of molecules to be evaluated.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\SWATWXPG\Nica et al. - 2022 - EVALUATING GENERALIZATION IN GFLOWNETS FOR MOLECUL.pdf}
}

@unpublished{nicholGottaLearnFast2018,
  title = {Gotta {{Learn Fast}}: {{A New Benchmark}} for {{Generalization}} in {{RL}}},
  shorttitle = {Gotta {{Learn Fast}}},
  author = {Nichol, Alex and Pfau, Vicki and Hesse, Christopher and Klimov, Oleg and Schulman, John},
  date = {2018-04-10},
  eprint = {1804.03720},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.03720},
  urldate = {2019-06-06},
  abstract = {In this report, we present a new reinforcement learning (RL) benchmark based on the Sonic the Hedgehog (TM) video game franchise. This benchmark is intended to measure the performance of transfer learning and few-shot learning algorithms in the RL domain. We also present and evaluate some baseline algorithms on the new benchmark.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2LM389QV\\Nichol et al_2018_Gotta Learn Fast.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TT962CVF\\1804.html}
}

@online{nichollsCuringPharmaAvoiding,
  title = {Curing {{Pharma}}: (1) {{Avoiding Hype-based Science}}},
  shorttitle = {Curing {{Pharma}}},
  author = {Nicholls, Anthony},
  url = {https://www.eyesopen.com/ants-rants/curing-pharma-avoiding-hype-based-science},
  urldate = {2021-01-04},
  abstract = {I’ve been thinking a lot lately about Jacques Tati’s masterful 1967 work Play Time, which describes an antiseptic, colorless and angular world of the future, a bleak world that only relents when the characters get together for a drink.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\S7JGQH28\curing-pharma-avoiding-hype-based-science.html}
}

@article{nicolaouProximalLillyCollection2016,
  title = {The {{Proximal Lilly Collection}}: {{Mapping}}, {{Exploring}} and {{Exploiting Feasible Chemical Space}}},
  author = {Nicolaou, Christos A. and Watson, Ian A. and Hu, Hong and Wang, Jibo},
  date = {2016-07-25},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {56},
  number = {7},
  pages = {1253--1266},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.6b00173},
  url = {https://doi.org/10.1021/acs.jcim.6b00173},
  issue = {7}
}

@inproceedings{niculescu-mizilPredictingGoodProbabilities2005,
  title = {Predicting Good Probabilities with Supervised Learning},
  booktitle = {Proceedings of the 22nd International Conference on {{Machine}} Learning  - {{ICML}} '05},
  author = {Niculescu-Mizil, Alexandru and Caruana, Rich},
  date = {2005},
  pages = {625--632},
  publisher = {ACM Press},
  location = {Bonn, Germany},
  doi = {10.1145/1102351.1102430},
  url = {http://portal.acm.org/citation.cfm?doid=1102351.1102430},
  urldate = {2019-09-09},
  abstract = {We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.},
  eventtitle = {The 22nd International Conference},
  isbn = {978-1-59593-180-1},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\P3G8SSU4\Niculescu-Mizil and Caruana - 2005 - Predicting good probabilities with supervised lear.pdf}
}

@article{nigamGenerativeModelsSuperfast2021,
  title = {Beyond Generative Models: Superfast Traversal, Optimization, Novelty, Exploration and Discovery ({{STONED}}) Algorithm for Molecules Using {{SELFIES}}},
  shorttitle = {Beyond Generative Models},
  author = {Nigam, AkshatKumar and Pollice, Robert and Krenn, Mario and Gomes, Gabriel dos Passos and Aspuru-Guzik, Alán},
  date = {2021-05-26},
  journaltitle = {Chem. Sci.},
  volume = {12},
  number = {20},
  pages = {7079--7090},
  publisher = {The Royal Society of Chemistry},
  issn = {2041-6539},
  doi = {10.1039/D1SC00231G},
  url = {https://pubs.rsc.org/en/content/articlelanding/2021/sc/d1sc00231g},
  urldate = {2023-10-29},
  abstract = {Inverse design allows the generation of molecules with desirable physical quantities using property optimization. Deep generative models have recently been applied to tackle inverse design, as they possess the ability to optimize molecular properties directly through structure modification using gradients. While the ability to carry out direct property optimizations is promising, the use of generative deep learning models to solve practical problems requires large amounts of data and is very time-consuming. In this work, we propose STONED – a simple and efficient algorithm to perform interpolation and exploration in the chemical space, comparable to deep generative models. STONED bypasses the need for large amounts of data and training times by using string modifications in the SELFIES molecular representation. First, we achieve non-trivial performance on typical benchmarks for generative models without any training. Additionally, we demonstrate applications in high-throughput virtual screening for the design of drugs, photovoltaics, and the construction of chemical paths, allowing for both property and structure-based interpolation in the chemical space. Overall, we anticipate our results to be a stepping stone for developing more sophisticated inverse design models and benchmarking tools, ultimately helping generative models achieve wider adoption.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LMX8H35P\\Nigam et al. - 2021 - Beyond generative models superfast traversal, opt.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\U6HQ2NCA\\Nigam et al. - 2021 - Beyond generative models superfast traversal, opt.pdf}
}

@article{nilakantanDatabaseDiversityAssessment1997,
  title = {Database Diversity Assessment: {{New}} Ideas, Concepts, and Tools},
  shorttitle = {Database Diversity Assessment},
  author = {Nilakantan, Ramaswamy and Bauman, Norman and Haraki, Kevin S.},
  date = {1997-09-01},
  journaltitle = {J Comput Aided Mol Des},
  volume = {11},
  number = {5},
  pages = {447--452},
  issn = {1573-4951},
  doi = {10.1023/A:1007937308615},
  url = {https://doi.org/10.1023/A:1007937308615},
  urldate = {2022-07-05},
  abstract = {We present some new ideas for characterizing and comparing largechemical databases. The comparison of the contents of large databases is nottrivial since it implies pairwise comparison of hundreds of thousands ofcompounds. We have developed methods for categorizing compounds into groupsor series based on their ring-system content, using precalculatedstructure-based hashcodes. Two large databases can then be compared bysimply comparing their hashcode tables. Furthermore, the number of distinctring-system combinations can be used as an indicator of database diversity.We also present an indepen- dent technique for diversity assessment calledthe ’saturation diversity‘ approach. This method is based on picking as manymutually dissimilar compounds as possible from a database or a subsetthereof. We show that both methods yield similar results. Since the twomethods measure very different properties, this probably says more about theproperties of the databases studied than about the methods.},
  langid = {english},
  keywords = {Combinatorial,Comparison,Database,Ring,Ring-cluster,Similarity},
  file = {C:\Users\USEBPERP\Zotero\storage\VH8AMV3R\Nilakantan et al. - 1997 - Database diversity assessment New ideas, concepts.pdf}
}

@online{NobelPrizeChemistry2021,
  title = {The {{Nobel Prize}} in {{Chemistry}} 1950},
  date = {2021-01-27},
  url = {https://www.nobelprize.org/prizes/chemistry/1950/speedread/},
  urldate = {2021-01-27},
  abstract = {The Nobel Prize in Chemistry 1950 was awarded jointly to Otto Paul Hermann Diels and Kurt Alder "for their discovery and development of the diene synthesis".},
  langid = {american},
  organization = {NobelPrize.org}
}

@online{NobelPrizeChemistry2021a,
  title = {The {{Nobel Prize}} in {{Chemistry}} 1912},
  date = {2021-01-27},
  url = {https://www.nobelprize.org/prizes/chemistry/1912/speedread/},
  urldate = {2021-01-27},
  abstract = {The Nobel Prize in Chemistry 1912 was divided equally between Victor Grignard "for the discovery of the so-called Grignard reagent, which in recent years has greatly advanced the progress of organic chemistry" and Paul Sabatier "for his method of hydrogenating organic compounds in the presence of finely disintegrated metals whereby the progress of organic chemistry has been greatly advanced in recent years".},
  langid = {american},
  organization = {NobelPrize.org}
}

@online{norvigArtificialIntelligenceModern,
  title = {Artificial {{Intelligence}}: {{A Modern Approach}}, 4th {{US}} Ed.},
  author = {Norvig, Peter and Russel, Stuart},
  url = {https://aima.cs.berkeley.edu/},
  urldate = {2023-01-05},
  file = {C:\Users\USEBPERP\Zotero\storage\32I8TTUZ\aima.cs.berkeley.edu.html}
}

@online{noutahiGottaBeSAFE2023,
  title = {Gotta Be {{SAFE}}: {{A New Framework}} for {{Molecular Design}}},
  shorttitle = {Gotta Be {{SAFE}}},
  author = {Noutahi, Emmanuel and Gabellini, Cristian and Craig, Michael and Lim, Jonathan S. C. and Tossou, Prudencio},
  date = {2023-12-10},
  eprint = {2310.10773},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  doi = {10.48550/arXiv.2310.10773},
  url = {http://arxiv.org/abs/2310.10773},
  urldate = {2024-06-09},
  abstract = {Traditional molecular string representations, such as SMILES, often pose challenges for AI-driven molecular design due to their non-sequential depiction of molecular substructures. To address this issue, we introduce Sequential Attachment-based Fragment Embedding (SAFE), a novel line notation for chemical structures. SAFE reimagines SMILES strings as an unordered sequence of interconnected fragment blocks while maintaining compatibility with existing SMILES parsers. It streamlines complex generative tasks, including scaffold decoration, fragment linking, polymer generation, and scaffold hopping, while facilitating autoregressive generation for fragment-constrained design, thereby eliminating the need for intricate decoding or graph-based models. We demonstrate the effectiveness of SAFE by training an 87-million-parameter GPT2-like model on a dataset containing 1.1 billion SAFE representations. Through targeted experimentation, we show that our SAFE-GPT model exhibits versatile and robust optimization performance. SAFE opens up new avenues for the rapid exploration of chemical space under various constraints, promising breakthroughs in AI-driven molecular design.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ZCK8RSGP\\Noutahi et al. - 2023 - Gotta be SAFE A New Framework for Molecular Desig.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\MTILXBJF\\2310.html}
}

@online{NovoMolecularDesign,
  ids = {novo},
  title = {De Novo {{Molecular Design}} | {{Wiley}}},
  url = {https://www.wiley.com/en-us/De+novo+Molecular+Design-p-9783527677030},
  urldate = {2020-03-17},
  abstract = {Systematically examining current methods and strategies, this ready reference covers a wide range of molecular structures, from organic-chemical drugs to peptides, Proteins and nucleic acids, in line with emerging new drug classes derived from biomacromolecules. A leader in the field and one of the pioneers of this young discipline has assembled here the most prominent experts from across the world to provide first-hand knowledge. While most of their methods and examples come from the area of pharmaceutical discovery and development, the approaches are equally applicable for chemical probes and diagnostics, pesticides, and any other molecule designed to interact with a biological system. Numerous images and screenshots illustrate the many examples and method descriptions. With its broad and balanced coverage, this will be the firststop resource not only for medicinal chemists, biochemists and biotechnologists, but equally for bioinformaticians and molecular designers for many years to come. From the content: * Reaction-driven de novo design * Adaptive methods in molecular design * Design of ligands against multitarget profiles * Free energy methods in ligand design * Fragment-based de novo design * Automated design of focused and target family-oriented compound libraries * Molecular de novo design by nature-inspired computing * 3D QSAR approaches to de novo drug design * Bioisosteres in de novo design * De novo design of peptides, proteins and nucleic acid structures, including RNA aptamers and many more.},
  langid = {american},
  organization = {Wiley.com},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4T9T3E9K\\De+novo+Molecular+Design-p-9783527677030.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\PKVEUPFH\\De+novo+Molecular+Design-p-9783527677030.html}
}

@online{NovoMoleculeDesign,
  title = {De {{Novo Molecule Design}} by {{Translating}} from {{Reduced Graphs}} to {{SMILES}} - {{Journal}} of {{Chemical Information}} and {{Modeling}} ({{ACS Publications}})},
  url = {https://pubs.acs.org/doi/10.1021/acs.jcim.8b00626},
  urldate = {2019-05-06},
  file = {C:\Users\USEBPERP\Zotero\storage\KWF8NPWL\acs.jcim.html}
}

@article{nueschOrderRestrictedStatistical1991,
  title = {Order Restricted Statistical Inference, {{T}}. {{Robertson}}, {{F}}. {{T}}. {{Wright}}, {{R}}. {{L}}. {{Dykstra Wiley Series}} in {{Probability}} and {{Mathematical Statistics}}, {{John Wiley}} and {{Sons}}, {{Chichester}}, 1988. {{ISBN}} 0-471-91787-7 Cloth, £37.50, Pp. 488},
  author = {Nüesch, P. E.},
  date = {1991},
  journaltitle = {Journal of Applied Econometrics},
  volume = {6},
  number = {1},
  pages = {105--107},
  issn = {1099-1255},
  doi = {10.1002/jae.3950060111},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.3950060111},
  urldate = {2019-09-09},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\AR6F4RX9\jae.html}
}

@article{oboyleDeepSMILESAdaptationSMILES2018,
  title = {{{DeepSMILES}}: {{An Adaptation}} of {{SMILES}} for {{Use}} in {{Machine-Learning}} of {{Chemical Structures}}},
  shorttitle = {{{DeepSMILES}}},
  author = {O'Boyle, Noel and Dalke, Andrew},
  date = {2018-09-19},
  publisher = {ChemRxiv},
  doi = {10.26434/chemrxiv.7097960.v1},
  url = {https://chemrxiv.org/articles/DeepSMILES_An_Adaptation_of_SMILES_for_Use_in_Machine-Learning_of_Chemical_Structures/7097960},
  urldate = {2020-04-15},
  abstract = {BackgroundThere has been increasing interest in the use of deep neural networks for de novo design of molecules with desired properties. A common approach is to train a generative model on SMILES strings and then use this to generate SMILES strings for molecules with a desired property. Unfortunately, these SMILES strings are often not syntactically valid due to elements of SMILES syntax that must occur in pairs.ResultsWe describe a SMILES-like syntax called DeepSMILES that addresses two of the main reasons for invalid syntax when using a probabilistic model to generate SMILES strings. The DeepSMILES syntax avoids the problem of unbalanced parentheses by only using close parentheses, where the number of parentheses indicates the branch length. In addition, DeepSMILES avoids the problem of pairing ring closure symbols by using only a single symbol at the ring closing location, where the symbol indicates the ring size. We show that this syntax can be interconverted to/from SMILES with string processing without any loss of information, including stereo configuration.ConclusionWe believe that DeepSMILES will be useful, not just for those using SMILES in deep neural networks, but also for other computational methods that use SMILES as the basis for generating molecular structures such as genetic algorithms.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\4UGEZMUQ\O'Boyle_Dalke_2018_DeepSMILES.pdf}
}

@article{ohModelingUncertaintyHedged2018,
  title = {Modeling {{Uncertainty}} with {{Hedged Instance Embeddings}}},
  author = {Oh, Seong Joon and Murphy, Kevin P. and Pan, Jiyan and Roth, Joseph and Schroff, Florian and Gallagher, Andrew C.},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=r1xQQhAqKX},
  urldate = {2019-04-24},
  abstract = {Instance embeddings are an efficient and versatile image representation that facilitates applications like recognition, verification, retrieval, and clustering. Many metric learning methods...},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\S7TQAG4T\\Oh et al_2018_Modeling Uncertainty with Hedged Instance Embeddings.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\X4WIMD8F\\forum.html}
}

@article{olahFeatureVisualization2017,
  title = {Feature {{Visualization}}},
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  date = {2017-11-07},
  journaltitle = {Distill},
  volume = {2},
  number = {11},
  pages = {e7},
  issn = {2476-0757},
  doi = {10.23915/distill.00007},
  url = {https://distill.pub/2017/feature-visualization},
  urldate = {2018-06-05},
  abstract = {How neural networks build up their understanding of images},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\DDI9PH8U\feature-visualization.html}
}

@article{olivecronaMolecularDenovoDesign2017,
  title = {Molecular De-Novo Design through Deep Reinforcement Learning},
  author = {Olivecrona, Marcus and Blaschke, Thomas and Engkvist, Ola and Chen, Hongming},
  date = {2017-09-04},
  journaltitle = {Journal of Cheminformatics},
  volume = {9},
  number = {1},
  pages = {48},
  issn = {1758-2946},
  doi = {10.1186/s13321-017-0235-x},
  url = {https://doi.org/10.1186/s13321-017-0235-x},
  urldate = {2019-05-06},
  abstract = {This work introduces a method to tune a sequence-based generative model for molecular de novo design that through augmented episodic likelihood can learn to generate structures with certain specified desirable properties. We demonstrate how this model can execute a range of tasks such as generating analogues to a query structure and generating compounds predicted to be active against a biological target. As a proof of principle, the model is first trained to generate molecules that do not contain sulphur. As a second example, the model is trained to generate analogues to the drug Celecoxib, a technique that could be used for scaffold hopping or library expansion starting from a single molecule. Finally, when tuning the model towards generating compounds predicted to be active against the dopamine receptor type 2, the model generates structures of which more than 95\% are predicted to be active, including experimentally confirmed actives that have not been included in either the generative model nor the activity prediction model.Graphical abstract.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\CXPQ8R5E\\Olivecrona et al_2017_Molecular de-novo design through deep reinforcement learning3.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GHRMIXM6\\Olivecrona et al_2017_Molecular de-novo design through deep reinforcement learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZHYT3UED\\Olivecrona et al_2017_Molecular de-novo design through deep reinforcement learning2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6BBHTL8M\\1704.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\KCF7SJ9B\\s13321-017-0235-x.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\XC94NABM\\1704.html}
}

@unpublished{oliverRealisticEvaluationDeep2018,
  title = {Realistic {{Evaluation}} of {{Deep Semi-Supervised Learning Algorithms}}},
  author = {Oliver, Avital and Odena, Augustus and Raffel, Colin and Cubuk, Ekin D. and Goodfellow, Ian J.},
  date = {2018-04-24},
  eprint = {1804.09170},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.09170},
  urldate = {2019-09-30},
  abstract = {Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that these algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, that SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\8JM488RC\\Oliver et al_2018_Realistic Evaluation of Deep Semi-Supervised Learning Algorithms.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2YZ4C7S7\\1804.html}
}

@unpublished{oordRepresentationLearningContrastive2018,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {family=Oord, given=Aaron, prefix=van den, useprefix=false and Li, Yazhe and Vinyals, Oriol},
  date = {2018},
  eprint = {1807.03748},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1807.03748},
  urldate = {2021-04-08},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\38CQMSIM\\Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\NAUS99QR\\1807.html}
}

@unpublished{oordWaveNetGenerativeModel2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {family=Oord, given=Aaron, prefix=van den, useprefix=false and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  date = {2016-09-12},
  eprint = {1609.03499},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1609.03499},
  urldate = {2019-10-21},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ZMG3G62J\\Oord et al_2016_WaveNet.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\A3IRWVR6\\1609.html}
}

@online{OpinionatedGuideML,
  title = {An {{Opinionated Guide}} to {{ML Research}}},
  url = {http://joschu.net/blog/opinionated-guide-ml-research.html},
  urldate = {2020-02-03},
  file = {C:\Users\USEBPERP\Zotero\storage\LAMWRE65\opinionated-guide-ml-research.html}
}

@unpublished{ovadiaCanYouTrust2019,
  title = {Can {{You Trust Your Model}}'s {{Uncertainty}}? {{Evaluating Predictive Uncertainty Under Dataset Shift}}},
  shorttitle = {Can {{You Trust Your Model}}'s {{Uncertainty}}?},
  author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua V. and Lakshminarayanan, Balaji and Snoek, Jasper},
  date = {2019-06-06},
  eprint = {1906.02530},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.02530},
  urldate = {2019-06-13},
  abstract = {Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive \{\textbackslash em uncertainty\}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7XWSNCU2\\Ovadia et al_2019_Can You Trust Your Model's Uncertainty.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\Q67Z4778\\1906.html}
}

@book{OxfordHandbookThinking2012,
  title = {The {{Oxford Handbook}} of {{Thinking}} and {{Reasoning}}},
  date = {2012-03-21},
  publisher = {Oxford University Press},
  doi = {10.1093/oxfordhb/9780199734689.001.0001},
  url = {https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199734689.001.0001/oxfordhb-9780199734689},
  urldate = {2019-10-01},
  abstract = {Thinking and reasoning, long the academic province of philosophy, have emerged over the past century as core topics of empirical investigation and theoretical analysis in the modern fields of cognitive psychology, cognitive science, and cognitive neuroscience. Formerly seen as too complicated and amorphous to be included in early textbooks on the science of cognition, the study of thinking and reasoning has since taken off, branching off in a distinct direction from the field from which it originated. This comprehensive publication covers all the core topics of the field of thinking and reasoning. Written by the foremost experts from cognitive psychology, cognitive science, and cognitive neuroscience, individual articles summarize basic concepts and findings for a major topic, sketch its history, and give a sense of the directions in which research is currently heading. The authors provide introductions to foundational issues and methods of study in the field, as well as treatment of specific types of thinking and reasoning and their application in a broad range of fields including business, education, law, medicine, music, and science.},
  isbn = {978-0-19-996871-8},
  langid = {american},
  file = {C:\Users\USEBPERP\Zotero\storage\9I8CMS8U\oxfordhb-9780199734689.html}
}

@article{pangDeepGenerativeModels2024,
  title = {Deep {{Generative Models}} in {{De Novo Drug Molecule Generation}}},
  author = {Pang, Chao and Qiao, Jianbo and Zeng, Xiangxiang and Zou, Quan and Wei, Leyi},
  date = {2024-04-08},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {64},
  number = {7},
  pages = {2174--2194},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.3c01496},
  url = {https://doi.org/10.1021/acs.jcim.3c01496},
  urldate = {2024-04-27},
  abstract = {The discovery of new drugs has important implications for human health. Traditional methods for drug discovery rely on experiments to optimize the structure of lead molecules, which are time-consuming and high-cost. Recently, artificial intelligence has exhibited promising and efficient performance for drug-like molecule generation. In particular, deep generative models achieve great success in de novo generation of drug-like molecules with desired properties, showing massive potential for novel drug discovery. In this study, we review the recent progress of molecule generation using deep generative models, mainly focusing on molecule representations, public databases, data processing tools, and advanced artificial intelligence based molecule generation frameworks. In particular, we present a comprehensive comparison of state-of-the-art deep generative models for molecule generation and a summary of commonly used molecular design strategies. We identify research gaps and challenges of molecule generation such as the need for better databases, missing 3D information in molecular representation, and the lack of high-precision evaluation metrics. We suggest future directions for molecular generation and drug discovery.},
  file = {C:\Users\USEBPERP\Zotero\storage\SLFCKBRK\Pang et al. - 2024 - Deep Generative Models in De Novo Drug Molecule Ge.pdf}
}

@unpublished{papamakariosNormalizingFlowsProbabilistic2019,
  title = {Normalizing {{Flows}} for {{Probabilistic Modeling}} and {{Inference}}},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  date = {2019-12-05},
  eprint = {1912.02762},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.02762},
  urldate = {2019-12-19},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\QTXFXUAK\\Papamakarios et al_2019_Normalizing Flows for Probabilistic Modeling and Inference.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SKEF5PGI\\1912.html}
}

@software{paparrizosTSBAUDEndtoEndAnomaly2022,
  title = {{{TSB-AUD}}: {{An End-to-End Anomaly Detection Benchmark Suite}} for {{Univariate Time-Series Data}}},
  shorttitle = {{{TSB-AUD}}},
  author = {Paparrizos, John},
  date = {2022-08-15T14:24:45Z},
  origdate = {2021-10-03T19:08:53Z},
  url = {https://github.com/johnpaparrizos/TSB-UAD},
  urldate = {2022-08-16},
  abstract = {An End-to-End Benchmark Suite for Univariate Time-Series Anomaly Detection}
}

@article{paparrizosTSBUADEndtoendBenchmark2022,
  title = {{{TSB-UAD}}: An End-to-End Benchmark Suite for Univariate Time-Series Anomaly Detection},
  shorttitle = {{{TSB-UAD}}},
  author = {Paparrizos, John and Kang, Yuhao and Boniol, Paul and Tsay, Ruey S. and Palpanas, Themis and Franklin, Michael J.},
  date = {2022-06-22},
  journaltitle = {Proc. VLDB Endow.},
  volume = {15},
  number = {8},
  pages = {1697--1711},
  issn = {2150-8097},
  doi = {10.14778/3529337.3529354},
  url = {https://doi.org/10.14778/3529337.3529354},
  urldate = {2023-01-05},
  abstract = {The detection of anomalies in time series has gained ample academic and industrial attention. However, no comprehensive benchmark exists to evaluate time-series anomaly detection methods. It is common to use (i) proprietary or synthetic data, often biased to support particular claims; or (ii) a limited collection of publicly available datasets. Consequently, we often observe methods performing exceptionally well in one dataset but surprisingly poorly in another, creating an illusion of progress. To address the issues above, we thoroughly studied over one hundred papers to identify, collect, process, and systematically format datasets proposed in the past decades. We summarize our effort in TSB-UAD, a new benchmark to ease the evaluation of univariate time-series anomaly detection methods. Overall, TSB-UAD contains 13766 time series with labeled anomalies spanning different domains with high variability of anomaly types, ratios, and sizes. TSB-UAD includes 18 previously proposed datasets containing 1980 time series and we contribute two collections of datasets. Specifically, we generate 958 time series using a principled methodology for transforming 126 time-series classification datasets into time series with labeled anomalies. In addition, we present data transformations with which we introduce new anomalies, resulting in 10828 time series with varying complexity for anomaly detection. Finally, we evaluate 12 representative methods demonstrating that TSB-UAD is a robust resource for assessing anomaly detection methods. We make our data and code available at www.timeseries.org/TSB-UAD. TSB-UAD provides a valuable, reproducible, and frequently updated resource to establish a leaderboard of univariate time-series anomaly detection methods.}
}

@unpublished{parkEffectNetworkWidth2019,
  title = {The {{Effect}} of {{Network Width}} on {{Stochastic Gradient Descent}} and {{Generalization}}: An {{Empirical Study}}},
  shorttitle = {The {{Effect}} of {{Network Width}} on {{Stochastic Gradient Descent}} and {{Generalization}}},
  author = {Park, Daniel S. and Sohl-Dickstein, Jascha and Le, Quoc V. and Smith, Samuel L.},
  date = {2019-05-09},
  eprint = {1905.03776},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.03776},
  urldate = {2019-06-04},
  abstract = {We investigate how the final parameters found by stochastic gradient descent are influenced by over-parameterization. We generate families of models by increasing the number of channels in a base network, and then perform a large hyper-parameter search to study how the test error depends on learning rate, batch size, and network width. We find that the optimal SGD hyper-parameters are determined by a "normalized noise scale," which is a function of the batch size, learning rate, and initialization conditions. In the absence of batch normalization, the optimal normalized noise scale is directly proportional to width. Wider networks, with their higher optimal noise scale, also achieve higher test accuracy. These observations hold for MLPs, ConvNets, and ResNets, and for two different parameterization schemes ("Standard" and "NTK"). We observe a similar trend with batch normalization for ResNets. Surprisingly, since the largest stable learning rate is bounded, the largest batch size consistent with the optimal normalized noise scale decreases as the width increases.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\EDRHHZQL\\Park et al_2019_The Effect of Network Width on Stochastic Gradient Descent and Generalization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AIGUWHWC\\1905.html}
}

@article{parker-holderEffectiveDiversityPopulationBased,
  title = {Effective {{Diversity}} in {{Population-Based Reinforcement Learning}}},
  author = {Parker-Holder, Jack and Pacchiano, Aldo and Choromanski, Krzysztof and Roberts, Stephen},
  pages = {19},
  abstract = {Exploration is a key problem in reinforcement learning, since agents can only learn from data they acquire in the environment. With that in mind, maintaining a population of agents is an attractive method, as it allows data be collected with a diverse set of behaviors. This behavioral diversity is often boosted via multi-objective loss functions. However, those approaches typically leverage mean field updates based on pairwise distances, which makes them susceptible to cycling behaviors and increased redundancy. In addition, explicitly boosting diversity often has a detrimental impact on optimizing already fruitful behaviors for rewards. As such, the reward-diversity trade off typically relies on heuristics. Finally, such methods require behavioral representations, often handcrafted and domain specific. In this paper, we introduce an approach to optimize all members of a population simultaneously. Rather than using pairwise distance, we measure the volume of the entire population in a behavioral manifold, defined by task-agnostic behavioral embeddings. In addition, our algorithm Diversity via Determinants (DvD), adapts the degree of diversity during training using online learning techniques. We introduce both evolutionary and gradient-based instantiations of DvD and show they effectively improve exploration without reducing performance when better exploration is not required.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\N8YWYYIJ\Parker-Holder et al. - Effective Diversity in Population-Based Reinforcem.pdf}
}

@inproceedings{parkMultimodalExecutionMonitoring2016,
  title = {Multimodal Execution Monitoring for Anomaly Detection during Robot Manipulation},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Park, Daehyung and Erickson, Zackory and Bhattacharjee, Tapomayukh and Kemp, Charles C.},
  date = {2016-05},
  pages = {407--414},
  doi = {10.1109/ICRA.2016.7487160},
  abstract = {Online detection of anomalous execution can be valuable for robot manipulation, enabling robots to operate more safely, determine when a behavior is inappropriate, and otherwise exhibit more common sense. By using multiple complementary sensory modalities, robots could potentially detect a wider variety of anomalies, such as anomalous contact or a loud utterance by a human. However, task variability and the potential for false positives make online anomaly detection challenging, especially for long-duration manipulation behaviors. In this paper, we provide evidence for the value of multimodal execution monitoring and the use of a detection threshold that varies based on the progress of execution. Using a data-driven approach, we train an execution monitor that runs in parallel to a manipulation behavior. Like previous methods for anomaly detection, our method trains a hidden Markov model (HMM) using multimodal observations from non-anomalous executions. In contrast to prior work, our system also uses a detection threshold that changes based on the execution progress. We evaluated our approach with haptic, visual, auditory, and kinematic sensing during a variety of manipulation tasks performed by a PR2 robot. The tasks included pushing doors closed, operating switches, and assisting able-bodied participants with eating yogurt. In our evaluations, our anomaly detection method performed substantially better with multimodal monitoring than single modality monitoring. It also resulted in more desirable ROC curves when compared with other detection threshold methods from the literature, obtaining higher true positive rates for comparable false positive rates.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  keywords = {Force,Hidden Markov models,Kinematics,Microwave theory and techniques,Monitoring,Robot sensing systems},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\922UBUAZ\\Park et al. - 2016 - Multimodal execution monitoring for anomaly detect.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\A7VEH9RQ\\Park et al. - 2016 - Multimodal execution monitoring for anomaly detect.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\E5W75GZY\\stamp.html}
}

@article{paszkeAutomaticDifferentiationPyTorch2017,
  title = {Automatic Differentiation in {{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  date = {2017-10-28},
  journaltitle = {NIPS-w},
  url = {https://openreview.net/forum?id=BJJsrmfCZ},
  urldate = {2019-09-19},
  abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua...},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\RSIPDBJ3\\Paszke et al_2017_Automatic differentiation in PyTorch.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FE5Z4R93\\forum.html}
}

@article{patelKnowledgeBasedApproachNovo2009,
  title = {Knowledge-{{Based Approach}} to de {{Novo Design Using Reaction Vectors}}},
  author = {Patel, Hina and Bodkin, Michael J. and Chen, Beining and Gillet, Valerie J.},
  date = {2009-05-22},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {49},
  number = {5},
  pages = {1163--1184},
  issn = {1549-9596},
  doi = {10.1021/ci800413m},
  url = {https://doi.org/10.1021/ci800413m},
  urldate = {2019-10-01},
  abstract = {A knowledge-based approach to the de novo design of synthetically feasible molecules is described. The method is based on reaction vectors which represent the structural changes that take place at the reaction center along with the environment in which the reaction occurs. The reaction vectors are derived automatically from a database of reactions which is not restricted by size or reaction complexity. A structure generation algorithm has been developed whereby reaction vectors can be applied to previously unseen starting materials in order to suggest novel syntheses. The approach has been implemented in KNIME and is validated by reproducing known synthetic routes. We then present applications of the method in different drug design scenarios including lead optimization and library enumeration. The method offers great potential for capturing and using the growing body of data on reactions that is becoming available through electronic laboratory notebooks.},
  file = {C:\Users\USEBPERP\Zotero\storage\ZTQNNUSS\ci800413m.html}
}

@article{patelSAVISilicoGeneration2020,
  title = {{{SAVI}}, in Silico Generation of Billions of Easily Synthesizable Compounds through Expert-System Type Rules},
  author = {Patel, Hitesh and Ihlenfeldt, Wolf-Dietrich and Judson, Philip N. and Moroz, Yurii S. and Pevzner, Yuri and Peach, Megan L. and Delannée, Victorien and Tarasova, Nadya I. and Nicklaus, Marc C.},
  date = {2020-11-11},
  journaltitle = {Sci Data},
  volume = {7},
  number = {1},
  eprint = {33177514},
  eprinttype = {pmid},
  pages = {384},
  issn = {2052-4463},
  doi = {10.1038/s41597-020-00727-4},
  abstract = {We have made available a database of over 1 billion compounds predicted to be easily synthesizable, called Synthetically Accessible Virtual Inventory (SAVI). They have been created by a set of transforms based on an adaptation and extension of the CHMTRN/PATRAN programming languages describing chemical synthesis expert knowledge, which originally stem from the LHASA project. The chemoinformatics toolkit CACTVS was used to apply a total of 53 transforms to about 150,000 readily available building blocks (enamine.net). Only single-step, two-reactant syntheses were calculated for this database even though the technology can execute multi-step reactions. The possibility to incorporate scoring systems in CHMTRN allowed us to subdivide the database of 1.75 billion compounds in sets according to their predicted synthesizability, with the most-synthesizable class comprising 1.09 billion synthetic products. Properties calculated for all SAVI products show that the database should be well-suited for drug discovery. It is being made publicly available for free download from https://doi.org/10.35115/37n9-5738.},
  issue = {1},
  langid = {english},
  pmcid = {PMC7658252}
}

@article{pattersonNeighborhoodBehaviorUseful1996,
  title = {Neighborhood {{Behavior}}:\, {{A Useful Concept}} for {{Validation}} of “{{Molecular Diversity}}” {{Descriptors}}},
  shorttitle = {Neighborhood {{Behavior}}},
  author = {Patterson, David E. and Cramer, Richard D. and Ferguson, Allan M. and Clark, Robert D. and Weinberger, Laurence E.},
  date = {1996-01-01},
  journaltitle = {J. Med. Chem.},
  volume = {39},
  number = {16},
  pages = {3049--3059},
  publisher = {American Chemical Society},
  issn = {0022-2623},
  doi = {10.1021/jm960290n},
  url = {https://doi.org/10.1021/jm960290n},
  urldate = {2022-07-26},
  abstract = {When searching for new leads, testing molecules that are too “similar” is wasteful, but when investigating a lead, testing molecules that are “similar” to the lead is efficient. Two questions then arise. Which are the molecular descriptors that should be “similar”? How much “similarity” is enough? These questions are answered by demonstrating that, if a molecular descriptor is to be a valid and useful measure of “similarity” in drug discovery, a plot of differences in its values vs differences in biological activities for a set of related molecules will exhibit a characteristic trapezoidal distribution enhancement, revealing a “neighborhood behavior” for the descriptor. Applying this finding to 20 datasets allows 11 molecular diversity descriptors to be ranked by their validity for compound library design. In order of increasing frequency of usefulness, these are random numbers = log P = MR = strain energy {$<$} connectivity indices {$<$} 2D fingerprints (whole molecule) = atom pairs = autocorrelation indices {$<$} steric CoMFA fields = 2D fingerprints (side chain only) = H-bonding CoMFA fields.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\8RTBTCIL\\Patterson et al. - 1996 - Neighborhood Behavior  A Useful Concept for Valid.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\3WRJHA9I\\jm960290n.html}
}

@article{paulHowImproveProductivity2010,
  title = {How to Improve {{R}}\&{{D}} Productivity: The Pharmaceutical Industry's Grand Challenge},
  shorttitle = {How to Improve {{R}}\&{{D}} Productivity},
  author = {Paul, Steven M. and Mytelka, Daniel S. and Dunwiddie, Christopher T. and Persinger, Charles C. and Munos, Bernard H. and Lindborg, Stacy R. and Schacht, Aaron L.},
  date = {2010-03},
  journaltitle = {Nat Rev Drug Discov},
  volume = {9},
  number = {3},
  pages = {203--214},
  issn = {1474-1776, 1474-1784},
  doi = {10.1038/nrd3078},
  url = {http://www.nature.com/articles/nrd3078},
  urldate = {2019-10-23},
  langid = {english}
}

@online{PdfPdf,
  title = {Pdf.Pdf},
  url = {https://openreview.net/pdf?id=HkNDsiC9KQ},
  urldate = {2018-12-27},
  file = {C:\Users\USEBPERP\Zotero\storage\ZQQIHLJS\pdf.pdf}
}

@article{pearlCAUSALITYModelsReasoning,
  title = {{{CAUSALITY}}: {{Models}}, {{Reasoning}}, and {{Inference Second Edition}}},
  author = {Pearl, Judea},
  pages = {487},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\9BBEJBIE\Pearl - CAUSALITY Models, Reasoning, and Inference Second.pdf}
}

@article{pedregosaScikitlearnMachineLearning2011,
  ids = {pedregosa2011scikitlearn,pedregosa2011scikitlearna},
  title = {Scikit-Learn: {{Machine}} Learning in Python},
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830},
  url = {http://jmlr.org/papers/v12/pedregosa11a.html},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4J8NI3XN\\Pedregosa et al_2011_Scikit-learn2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LG5JTZD7\\Pedregosa et al_2011_Scikit-learn.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WV39595W\\Pedregosa et al_2011_Scikit-learn.pdf}
}

@article{pereiraDiversityOrientedDeep2021,
  title = {Diversity Oriented {{Deep Reinforcement Learning}} for Targeted Molecule Generation},
  author = {Pereira, Tiago and Abbasi, Maryam and Ribeiro, Bernardete and Arrais, Joel P.},
  date = {2021-12},
  journaltitle = {J Cheminform},
  volume = {13},
  number = {1},
  pages = {21},
  issn = {1758-2946},
  doi = {10.1186/s13321-021-00498-z},
  url = {https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00498-z},
  urldate = {2022-08-29},
  abstract = {In this work, we explore the potential of deep learning to streamline the process of identifying new potential drugs through the computational generation of molecules with interesting biological properties. Two deep neural networks compose our targeted generation framework: the Generator, which is trained to learn the building rules of valid molecules employing SMILES strings notation, and the Predictor which evaluates the newly generated compounds by predicting their affinity for the desired target. Then, the Generator is optimized through Reinforcement Learning to produce molecules with bespoken properties. The innovation of this approach is the exploratory strategy applied during the reinforcement training process that seeks to add novelty to the generated compounds. This training strategy employs two Generators interchangeably to sample new SMILES: the initially trained model that will remain fixed and a copy of the previous one that will be updated during the training to uncover the most promising molecules. The evolution of the reward assigned by the Predictor determines how often each one is employed to select the next token of the molecule. This strategy establishes a compromise between the need to acquire more information about the chemical space and the need to sample new molecules, with the experience gained so far. To demonstrate the effectiveness of the method, the Generator is trained to design molecules with an optimized coefficient of partition and also high inhibitory power against the Adenosine A2A and κ opioid receptors. The results reveal that the model can effectively adjust the newly generated molecules towards the wanted direction. More importantly, it was possible to find promising sets of unique and diverse molecules, which was the main purpose of the newly implemented strategy.},
  langid = {english},
  keywords = {Drug Design,Reinforcement Learning,RNN,SMILES},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7XG3YUWG\\Pereira et al. - 2021 - Diversity oriented Deep Reinforcement Learning for.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\S3X6RMSK\\Pereira et al. - 2021 - Diversity oriented Deep Reinforcement Learning for.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TDXY46QR\\Pereira et al. - 2021 - Diversity oriented Deep Reinforcement Learning for.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BKTGVV6S\\s13321-021-00498-z.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\WPY7FYWA\\s13321-021-00498-z.html}
}

@article{phillipsHasMolecularDocking2018,
  title = {Has {{Molecular Docking Ever Brought}} Us a {{Medicine}}?},
  author = {Phillips, Mark Andrew and Stewart, Marisa A. and Xie, Darby L. Woodling {and} Zhong-Ru},
  date = {2018-07-11},
  journaltitle = {Molecular Docking},
  doi = {10.5772/intechopen.72898},
  url = {https://www.intechopen.com/books/molecular-docking/has-molecular-docking-ever-brought-us-a-medicine-},
  urldate = {2019-05-20},
  abstract = {Molecular docking has been developed and improving for many years, but its ability to bring a medicine to the drug market effectively is still generally questioned. In this chapter, we introduce several successful cases including drugs for treatment of HIV, cancers, and other prevalent diseases. The technical details such as docking software, protein data bank (PDB) structures, and other computational methods employed are also collected and displayed. In most of the cases, the structures of drugs or drug candidates and the interacting residues on the target proteins are also presented. In addition, a few successful examples of drug repurposing using molecular docking are mentioned in this chapter. It should provide us with confidence that the docking will be extensively employed in the industry and basic research. Moreover, we should actively apply molecular docking and related technology to create new therapies for diseases.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JIQAUJAJ\\Phillips et al_2018_Has Molecular Docking Ever Brought us a Medicine.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\X2MTZETE\\has-molecular-docking-ever-brought-us-a-medicine-.html}
}

@online{PIIS1093326398,
  title = {{{PII}}: {{S1093-3263}}(98)00008-4 | {{Elsevier Enhanced Reader}}},
  shorttitle = {{{PII}}},
  doi = {10.1016/S1093-3263(98)00008-4},
  url = {https://reader.elsevier.com/reader/sd/pii/S1093326398000084?token=133726BD64218E7A2280300166979154CB66237C7FE45D71B73D7BC60856570B54B14A9F5CA2B558FEBBAC39A5329DB8&originRegion=eu-west-1&originCreation=20220608130211},
  urldate = {2022-06-08},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\52K82FVA\S1093326398000084.html}
}

@online{PIIS1359644697,
  title = {{{PII}}: {{S1359-6446}}(97)01138-0 | {{Elsevier Enhanced Reader}}},
  shorttitle = {{{PII}}},
  doi = {10.1016/S1359-6446(97)01138-0},
  url = {https://reader.elsevier.com/reader/sd/pii/S1359644697011380?token=34609D0FA1BAF07E963ED19360A9C753CFACD71E1F1E548B9BF404F9EE30C7F4D44A345B4528C54F69671E40C8E96833&originRegion=eu-west-1&originCreation=20220829092738},
  urldate = {2022-08-29},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\5IGX5KE5\\PII S1359-6446(97)01138-0  Elsevier Enhanced Rea.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DF2IYWZ4\\S1359644697011380.html}
}

@article{pimentelReviewNoveltyDetection2014,
  title = {A Review of Novelty Detection},
  author = {Pimentel, Marco A. F. and Clifton, David A. and Clifton, Lei and Tarassenko, Lionel},
  date = {2014-06-01},
  journaltitle = {Signal Processing},
  volume = {99},
  pages = {215--249},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2013.12.026},
  url = {http://www.sciencedirect.com/science/article/pii/S016516841300515X},
  urldate = {2019-07-08},
  abstract = {Novelty detection is the task of classifying test data that differ in some respect from the data that are available during training. This may be seen as “one-class classification”, in which a model is constructed to describe “normal” training data. The novelty detection approach is typically used when the quantity of available “abnormal” data is insufficient to construct explicit models for non-normal classes. Application includes inference in datasets from critical systems, where the quantity of available normal data is very large, such that “normality” may be accurately modelled. In this review we aim to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade.},
  keywords = {Machine learning,Novelty detection,One-class classification},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\FIW2IA9A\\Pimentel et al_2014_A review of novelty detection.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FRXXLDC6\\S016516841300515X.html}
}

@article{plattLearningGaussianProcess2001,
  title = {Learning a {{Gaussian Process Prior}} for {{Automatically Generating Music Playlists}}},
  author = {Platt, John C and Burges, Christopher J C and Swenson, Steven and Weare, Christopher and Zheng, Alice},
  date = {2001},
  journaltitle = {Advances in Neural Information Processing Systems},
  pages = {8},
  abstract = {This paper presents AutoDJ: a system for automatically generating music playlists based on one or more seed songs selected by a user. AutoDJ uses Gaussian Process Regression to learn a user preference function over songs. This function takes music metadata as inputs. This paper further introduces Kernel Meta-Training, which is a method of learning a Gaussian Process kernel from a distribution of functions that generates the learned function. For playlist generation, AutoDJ learns a kernel from a large set of albums. This learned kernel is shown to be more effective at predicting users’ playlists than a reasonable hand-designed kernel.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\YK9S38V8\Platt et al. - Learning a Gaussian Process Prior for Automaticall.pdf}
}

@article{plattProbabilisticOutputsSupport2000,
  title = {Probabilistic {{Outputs}} for {{Support Vector Machines}} and {{Comparisons}} to {{Regularized Likelihood Methods}}},
  author = {Platt, John},
  date = {2000-06-23},
  journaltitle = {Adv. Large Margin Classif.},
  volume = {10},
  abstract = {The output of a classifier should be a calibrated posterior probability to enable post-processing. Standard SVMs do not provide such probabilities. One method to create probabilities is to directly train a kernel classifier with a logit link function and a regularized maximum likelihood score. However, training with a maximum likelihood score will produce non-sparse kernel machines. Instead, we train an SVM, then train the parameters of an additional sigmoid function to map the SVM outputs into probabilities. This chapter compares classification error rate and likelihood scores for an SVM plus sigmoid versus a kernel method trained with a regularized likelihood error function. These methods are tested on three data-mining-style data sets. The SVM+sigmoid yields probabilities of comparable quality to the regularized maximum likelihood kernel method, while still retaining the sparseness of the SVM.},
  file = {C:\Users\USEBPERP\Zotero\storage\6EBH43PJ\Platt_2000_Probabilistic Outputs for Support Vector Machines and Comparisons to.pdf}
}

@article{polyakMethodsSpeedingConvergence1964,
  ids = {polyak1964methodsa},
  title = {Some Methods of Speeding up the Convergence of Iteration Methods},
  author = {Polyak, B. T.},
  date = {1964-01-01},
  journaltitle = {USSR Computational Mathematics and Mathematical Physics},
  volume = {4},
  number = {5},
  pages = {1--17},
  issn = {0041-5553},
  doi = {10.1016/0041-5553(64)90137-5},
  url = {http://www.sciencedirect.com/science/article/pii/0041555364901375},
  urldate = {2020-08-11},
  abstract = {For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, …, xn, …, which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 ⩽ t ⩽ ∞ is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t → ∞ (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1 we use k previous iterations xn, …, xn−k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7YGHGUEI\\0041555364901375.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\Y5WN663L\\0041555364901375.html}
}

@article{polykovskiyEntangledConditionalAdversarial2018,
  title = {Entangled {{Conditional Adversarial Autoencoder}} for de {{Novo Drug Discovery}}},
  author = {Polykovskiy, Daniil and Zhebrak, Alexander and Vetrov, Dmitry and Ivanenkov, Yan and Aladinskiy, Vladimir and Mamoshina, Polina and Bozdaganyan, Marine and Aliper, Alexander and Zhavoronkov, Alex and Kadurin, Artur},
  date = {2018-10-01},
  journaltitle = {Mol. Pharm.},
  volume = {15},
  number = {10},
  eprint = {30180591},
  eprinttype = {pmid},
  pages = {4398--4405},
  issn = {1543-8392},
  doi = {10.1021/acs.molpharmaceut.8b00839},
  abstract = {Modern computational approaches and machine learning techniques accelerate the invention of new drugs. Generative models can discover novel molecular structures within hours, while conventional drug discovery pipelines require months of work. In this article, we propose a new generative architecture, entangled conditional adversarial autoencoder, that generates molecular structures based on various properties, such as activity against a specific protein, solubility, or ease of synthesis. We apply the proposed model to generate a novel inhibitor of Janus kinase 3, implicated in rheumatoid arthritis, psoriasis, and vitiligo. The discovered molecule was tested in vitro and showed good activity and selectivity.},
  langid = {english},
  keywords = {adversarial autoencoders,conditional generation,disentanglement,Janus kinase},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2LPWKJ5Z\\Polykovskiy et al_2018_Entangled Conditional Adversarial Autoencoder for de Novo Drug Discovery.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\UGT4GZPZ\\acs.molpharmaceut.html}
}

@unpublished{polykovskiyMolecularSetsMOSES2018,
  title = {Molecular {{Sets}} ({{MOSES}}): {{A Benchmarking Platform}} for {{Molecular Generation Models}}},
  shorttitle = {Molecular {{Sets}} ({{MOSES}})},
  author = {Polykovskiy, Daniil and Zhebrak, Alexander and Sanchez-Lengeling, Benjamin and Golovanov, Sergey and Tatanov, Oktai and Belyaev, Stanislav and Kurbanov, Rauf and Artamonov, Aleksey and Aladinskiy, Vladimir and Veselov, Mark and Kadurin, Artur and Nikolenko, Sergey and Aspuru-Guzik, Alan and Zhavoronkov, Alex},
  date = {2018-11-29},
  eprint = {1811.12823},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.12823},
  urldate = {2019-05-06},
  abstract = {Deep generative models such as generative adversarial networks, variational autoencoders, and autoregressive models are rapidly growing in popularity for the discovery of new molecules and materials. In this work, we introduce MOlecular SEtS (MOSES), a benchmarking platform to support research on machine learning for drug discovery. MOSES implements several popular molecular generation models and includes a set of metrics that evaluate the diversity and quality of generated molecules. MOSES is meant to standardize the research on molecular generation and facilitate the sharing and comparison of new models. Additionally, we provide a large-scale comparison of existing state of the art models and elaborate on current challenges for generative models that might prove fertile ground for new research. Our platform and source code are freely available at https://github.com/molecularsets/moses.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4LVXB83C\\Polykovskiy et al_2018_Molecular Sets (MOSES)3.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\9QENFUJ9\\Polykovskiy et al_2018_Molecular Sets (MOSES)2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\S6Y9ZYJ8\\Polykovskiy et al_2018_Molecular Sets (MOSES).pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\J9MJYPKM\\1811.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\KYHHIEM2\\1811.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\LEXEW7WZ\\1811.html}
}

@article{polykovskiyMolecularSetsMOSES2020,
  title = {Molecular {{Sets}} ({{MOSES}}): {{A Benchmarking Platform}} for {{Molecular Generation Models}}},
  shorttitle = {Molecular {{Sets}} ({{MOSES}})},
  author = {Polykovskiy, Daniil and Zhebrak, Alexander and Sanchez-Lengeling, Benjamin and Golovanov, Sergey and Tatanov, Oktai and Belyaev, Stanislav and Kurbanov, Rauf and Artamonov, Aleksey and Aladinskiy, Vladimir and Veselov, Mark and Kadurin, Artur and Johansson, Simon and Chen, Hongming and Nikolenko, Sergey and Aspuru-Guzik, Alán and Zhavoronkov, Alex},
  date = {2020},
  journaltitle = {Frontiers in Pharmacology},
  volume = {11},
  issn = {1663-9812},
  url = {https://www.frontiersin.org/article/10.3389/fphar.2020.565644},
  urldate = {2022-05-02},
  abstract = {Generative models are becoming a tool of choice for exploring the molecular space. These models learn on a large training dataset and produce novel molecular structures with similar properties. Generated structures can be utilized for virtual screening or training semi-supervized predictive models in the downstream tasks. While there are plenty of generative models, it is unclear how to compare and rank them. In this work, we introduce a benchmarking platform called Molecular Sets (MOSES) to standardize training and comparison of molecular generative models. MOSES provides training and testing datasets, and a set of metrics to evaluate the quality and diversity of generated structures. We have implemented and compared several molecular generation models and suggest to use our results as reference points for further advancements in generative chemistry research. The platform and source code are available at https://github.com/molecularsets/moses.},
  file = {C:\Users\USEBPERP\Zotero\storage\D8BE7799\Polykovskiy et al. - 2020 - Molecular Sets (MOSES) A Benchmarking Platform fo.pdf}
}

@unpublished{pondenkandathLeveragingRandomLabel2018,
  title = {Leveraging {{Random Label Memorization}} for {{Unsupervised Pre-Training}}},
  author = {Pondenkandath, Vinaychandran and Alberti, Michele and Puran, Sammer and Ingold, Rolf and Liwicki, Marcus},
  date = {2018-11-05},
  eprint = {1811.01640},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.01640},
  urldate = {2021-09-09},
  abstract = {We present a novel approach to leverage large unlabeled datasets by pre-training state-of-the-art deep neural networks on randomly-labeled datasets. Specifically, we train the neural networks to memorize arbitrary labels for all the samples in a dataset and use these pre-trained networks as a starting point for regular supervised learning. Our assumption is that the "memorization infrastructure" learned by the network during the random-label training proves to be beneficial for the conventional supervised learning as well. We test the effectiveness of our pre-training on several video action recognition datasets (HMDB51, UCF101, Kinetics) by comparing the results of the same network with and without the random label pre-training. Our approach yields an improvement - ranging from 1.5\% on UCF-101 to 5\% on Kinetics - in classification accuracy, which calls for further research in this direction.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\77USPJB2\\Pondenkandath et al. - 2018 - Leveraging Random Label Memorization for Unsupervi.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XK3UMYVR\\1811.html}
}

@unpublished{poplinPredictingCardiovascularRisk2017,
  title = {Predicting {{Cardiovascular Risk Factors}} from {{Retinal Fundus Photographs}} Using {{Deep Learning}}},
  author = {Poplin, Ryan and Varadarajan, Avinash V. and Blumer, Katy and Liu, Yun and McConnell, Michael V. and Corrado, Greg S. and Peng, Lily and Webster, Dale R.},
  date = {2017-08-31},
  eprint = {1708.09843},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1708.09843},
  urldate = {2018-02-06},
  abstract = {Traditionally, medical discoveries are made by observing associations and then designing experiments to test these hypotheses. However, observing and quantifying associations in images can be difficult because of the wide variety of features, patterns, colors, values, shapes in real data. In this paper, we use deep learning, a machine learning technique that learns its own features, to discover new knowledge from retinal fundus images. Using models trained on data from 284,335 patients, and validated on two independent datasets of 12,026 and 999 patients, we predict cardiovascular risk factors not previously thought to be present or quantifiable in retinal images, such as such as age (within 3.26 years), gender (0.97 AUC), smoking status (0.71 AUC), HbA1c (within 1.39\%), systolic blood pressure (within 11.23mmHg) as well as major adverse cardiac events (0.70 AUC). We further show that our models used distinct aspects of the anatomy to generate each prediction, such as the optic disc or blood vessels, opening avenues of further research.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\PPF6IBIC\\Poplin et al_2017_Predicting Cardiovascular Risk Factors from Retinal Fundus Photographs using.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\47IKDNMS\\1708.html}
}

@article{popovaDeepReinforcementLearning2018,
  title = {Deep Reinforcement Learning for de Novo Drug Design},
  author = {Popova, Mariya and Isayev, Olexandr and Tropsha, Alexander},
  date = {2018-07-01},
  journaltitle = {Science Advances},
  volume = {4},
  number = {7},
  pages = {eaap7885},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aap7885},
  url = {http://advances.sciencemag.org/content/4/7/eaap7885},
  urldate = {2019-02-21},
  abstract = {We have devised and implemented a novel computational strategy for de novo design of molecules with desired properties termed ReLeaSE (Reinforcement Learning for Structural Evolution). On the basis of deep and reinforcement learning (RL) approaches, ReLeaSE integrates two deep neural networks—generative and predictive—that are trained separately but are used jointly to generate novel targeted chemical libraries. ReLeaSE uses simple representation of molecules by their simplified molecular-input line-entry system (SMILES) strings only. Generative models are trained with a stack-augmented memory network to produce chemically feasible SMILES strings, and predictive models are derived to forecast the desired properties of the de novo–generated compounds. In the first phase of the method, generative and predictive models are trained separately with a supervised learning algorithm. In the second phase, both models are trained jointly with the RL approach to bias the generation of new chemical structures toward those with the desired physical and/or biological properties. In the proof-of-concept study, we have used the ReLeaSE method to design chemical libraries with a bias toward structural complexity or toward compounds with maximal, minimal, or specific range of physical properties, such as melting point or hydrophobicity, or toward compounds with inhibitory activity against Janus protein kinase 2. The approach proposed herein can find a general use for generating targeted chemical libraries of novel compounds optimized for either a single desired property or multiple properties. We introduce an artificial intelligence approach to de novo design of molecules with desired physical or biological properties. We introduce an artificial intelligence approach to de novo design of molecules with desired physical or biological properties.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\GMXKGX23\\Popova et al_2018_Deep reinforcement learning for de novo drug design3.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\CY7KCITL\\eaap7885.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\LXM42A53\\eaap7885.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\Y6AU9FGE\\1711.html}
}

@article{preciatgonzalezComparativeEvaluationAtom2017,
  title = {Comparative Evaluation of Atom Mapping Algorithms for Balanced Metabolic Reactions: Application to {{Recon 3D}}},
  shorttitle = {Comparative Evaluation of Atom Mapping Algorithms for Balanced Metabolic Reactions},
  author = {Preciat Gonzalez, German A. and El Assal, Lemmer R. P. and Noronha, Alberto and Thiele, Ines and Haraldsdóttir, Hulda S. and Fleming, Ronan M. T.},
  date = {2017-06-14},
  journaltitle = {Journal of Cheminformatics},
  volume = {9},
  number = {1},
  pages = {39},
  issn = {1758-2946},
  doi = {10.1186/s13321-017-0223-1},
  url = {https://doi.org/10.1186/s13321-017-0223-1},
  urldate = {2019-09-30},
  abstract = {The mechanism of each chemical reaction in a metabolic network can be represented as a set of atom mappings, each of which relates an atom in a substrate metabolite to an atom of the same element in a product metabolite. Genome-scale metabolic network reconstructions typically represent biochemistry at the level of reaction stoichiometry. However, a more detailed representation at the underlying level of atom mappings opens the possibility for a broader range of biological, biomedical and biotechnological applications than with stoichiometry alone. Complete manual acquisition of atom mapping data for a genome-scale metabolic network is a laborious process. However, many algorithms exist to predict atom mappings. How do their predictions compare to each other and to manually curated atom mappings? For more than four thousand metabolic reactions in the latest human metabolic reconstruction, Recon 3D, we compared the atom mappings predicted by six atom mapping algorithms. We also compared these predictions to those obtained by manual curation of atom mappings for over five hundred reactions distributed among all top level Enzyme Commission number classes. Five of the evaluated algorithms had similarly high prediction accuracy of over 91\% when compared to manually curated atom mapped reactions. On average, the accuracy of the prediction was highest for reactions catalysed by oxidoreductases and lowest for reactions catalysed by ligases. In addition to prediction accuracy, the algorithms were evaluated on their accessibility, their advanced features, such as the ability to identify equivalent atoms, and their ability to map hydrogen atoms. In addition to prediction accuracy, we found that software accessibility and advanced features were fundamental to the selection of an atom mapping algorithm in practice.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\FHT6RD9L\\Preciat Gonzalez et al_2017_Comparative evaluation of atom mapping algorithms for balanced metabolic.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\R5J6RCW2\\s13321-017-0223-1.html}
}

@online{PredictingRetrosyntheticReactions2021,
  title = {Predicting {{Retrosynthetic Reactions Using Self-Corrected Transformer Neural Networks}}},
  date = {2021-01-22},
  url = {https://pubs.acs.org/doi/10.1021/acs.jcim.9b00949},
  urldate = {2021-01-22}
}

@online{PredictionOrganicReaction2021,
  ids = {2021predictiona},
  title = {Prediction of {{Organic Reaction Outcomes Using Machine Learning}} | {{ACS Central Science}}},
  date = {2021-01-20},
  url = {https://pubs.acs.org/doi/abs/10.1021/acscentsci.7b00064},
  urldate = {2021-01-20}
}

@article{PRELIMINARYVERSIONNOT,
  title = {{{PRELIMINARY VERSION DO NOT CITE}}},
  pages = {9},
  abstract = {Retrosynthesis is the task of predicting reactant molecules from a given product molecule, important in organic chemistry since finding a synthetic path is as demanding as discovering new chemical compounds. Recently, solving the retrosynthesis task automatically without human expertise has become an active topic of research with the aid of powerful deep learning models. Recent deep models are mostly based on Seq2Seq or GNN networks depending on the selection of molecular representation, sequence or graph, respectively. Current state-of-the-art models represent a molecule as a graph, but they require joint training with auxiliary prediction tasks, such as the most probable reaction template or the reaction center prediction. However, they require additional labels by experienced chemist, which is costly. Here, we propose a novel template-free model, Graph Truncated Attention (GTA) which leverages both sequence and graph representations by inserting graphical information into a Seq2Seq model. The proposed GTA model masks self-attention layer using adjacency matrix of product molecule in the encoder, and applies a new loss using atom-mappings acquired from an automated algorithm to cross-attention layer in the decoder. Our model achieves new state-of-the-art results such as exact match accuracy of top-1 51.1 \% and top-10 81.6 \% with USPTO-50k benchmark dataset and top-10 46.0 \% and top-10 70.0 \% with USPTO-full dataset, both without any reaction class information. GTA model surpasses prior graphbased template-free models over top-1 2 \% and top-10 7 \% for USPTO-50k and top-1 and top-10 both over 6 \% for USPTOfull.},
  langid = {english}
}

@unpublished{prengerWaveGlowFlowbasedGenerative2018,
  title = {{{WaveGlow}}: {{A Flow-based Generative Network}} for {{Speech Synthesis}}},
  shorttitle = {{{WaveGlow}}},
  author = {Prenger, Ryan and Valle, Rafael and Catanzaro, Bryan},
  date = {2018-10-30},
  eprint = {1811.00002},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/1811.00002},
  urldate = {2019-12-19},
  abstract = {In this paper we propose WaveGlow: a flow-based network capable of generating high quality speech from mel-spectrograms. WaveGlow combines insights from Glow and WaveNet in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression. WaveGlow is implemented using only a single network, trained using only a single cost function: maximizing the likelihood of the training data, which makes the training procedure simple and stable. Our PyTorch implementation produces audio samples at a rate of more than 500 kHz on an NVIDIA V100 GPU. Mean Opinion Scores show that it delivers audio quality as good as the best publicly available WaveNet implementation. All code will be made publicly available online.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\UEPZJ3M6\\Prenger et al_2018_WaveGlow.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\9E4EYVWF\\1811.html}
}

@article{preuerFrechetChemNetDistance2018,
  title = {Fréchet {{ChemNet Distance}}: {{A Metric}} for {{Generative Models}} for {{Molecules}} in {{Drug Discovery}}},
  shorttitle = {Fréchet {{ChemNet Distance}}},
  author = {Preuer, Kristina and Renz, Philipp and Unterthiner, Thomas and Hochreiter, Sepp and Klambauer, Günter},
  date = {2018-09-24},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {58},
  number = {9},
  pages = {1736--1741},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.8b00234},
  url = {https://doi.org/10.1021/acs.jcim.8b00234},
  urldate = {2020-03-10},
  abstract = {The new wave of successful generative models in machine learning has increased the interest in deep learning driven de novo drug design. However, method comparison is difficult because of various flaws of the currently employed evaluation metrics. We propose an evaluation metric for generative models called Fréchet ChemNet distance (FCD). The advantage of the FCD over previous metrics is that it can detect whether generated molecules are diverse and have similar chemical and biological properties as real molecules.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\FIY2DZV5\\Preuer et al_2018_Fréchet ChemNet Distance.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\EKI8Y5W3\\acs.jcim.html}
}

@article{prinzBelieveItNot2011,
  title = {Believe It or Not: How Much Can We Rely on Published Data on Potential Drug Targets?},
  shorttitle = {Believe It or Not},
  author = {Prinz, Florian and Schlange, Thomas and Asadullah, Khusru},
  date = {2011-09},
  journaltitle = {Nature Reviews Drug Discovery},
  volume = {10},
  number = {9},
  pages = {712--712},
  publisher = {Nature Publishing Group},
  issn = {1474-1784},
  doi = {10.1038/nrd3439-c1},
  url = {https://www.nature.com/articles/nrd3439-c1},
  urldate = {2020-07-09},
  issue = {9},
  langid = {english},
  keywords = {Drug discovery},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\B92H9D2U\\Prinz et al. - 2011 - Believe it or not how much can we rely on publish.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\D9RNA54I\\Prinz et al_2011_Believe it or not.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YEWPWPPA\\nrd3439-c1.html}
}

@online{PrototypeBasedCompoundDiscovery,
  title = {Prototype-{{Based Compound Discovery Using Deep Generative Models}} - {{Molecular Pharmaceutics}} ({{ACS Publications}})},
  url = {https://pubs.acs.org/doi/10.1021/acs.molpharmaceut.8b00474},
  urldate = {2019-05-06},
  file = {C:\Users\USEBPERP\Zotero\storage\FVWPIVEE\acs.molpharmaceut.html}
}

@online{ProximalLillyCollection,
  title = {The {{Proximal Lilly Collection}}: {{Mapping}}, {{Exploring}} and {{Exploiting Feasible Chemical Space}} | {{Journal}} of {{Chemical Information}} and {{Modeling}}},
  url = {https://pubs.acs.org/doi/10.1021/acs.jcim.6b00173},
  urldate = {2020-06-03},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LUTCT8LD\\The Proximal Lilly Collection Mapping, Exploring .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\F9D5H3CT\\acs.jcim.html}
}

@article{purvisGettingMeasureBiodiversity2000,
  title = {Getting the Measure of Biodiversity},
  author = {Purvis, Andy and Hector, Andy},
  date = {2000-05},
  journaltitle = {Nature},
  volume = {405},
  number = {6783},
  pages = {212--219},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/35012221},
  url = {https://www.nature.com/articles/35012221},
  urldate = {2022-09-01},
  abstract = {The term ‘biodiversity’ is a simple contraction of ‘biological diversity’, and at first sight the concept is simple too: biodiversity is the sum total of all biotic variation from the level of genes to ecosystems. The challenge comes in measuring such a broad concept in ways that are useful. We show that, although biodiversity can never be fully captured by a single number, study of particular facets has led to rapid, exciting and sometimes alarming discoveries. Phylogenetic and temporal analyses are shedding light on the ecological and evolutionary processes that have shaped current biodiversity. There is no doubt that humans are now destroying this diversity at an alarming rate. A vital question now being tackled is how badly this loss affects ecosystem functioning. Although current research efforts are impressive, they are tiny in comparison to the amount of unknown diversity and the urgency and importance of the task.},
  issue = {6783},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\F2SLZXFV\\Purvis and Hector - 2000 - Getting the measure of biodiversity.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\Y9K7GK7C\\35012221.html}
}

@article{putinAdversarialThresholdNeural2018,
  title = {Adversarial {{Threshold Neural Computer}} for {{Molecular}} de {{Novo Design}}},
  author = {Putin, Evgeny and Asadulaev, Arip and Vanhaelen, Quentin and Ivanenkov, Yan and Aladinskaya, Anastasia V. and Aliper, Alex and Zhavoronkov, Alex},
  date = {2018-03-23},
  journaltitle = {Mol. Pharmaceutics},
  issn = {1543-8384},
  doi = {10.1021/acs.molpharmaceut.7b01137},
  url = {https://doi.org/10.1021/acs.molpharmaceut.7b01137},
  urldate = {2018-09-19},
  abstract = {In this article, we propose the deep neural network Adversarial Threshold Neural Computer (ATNC). The ATNC model is intended for the de novo design of novel small-molecule organic structures. The model is based on generative adversarial network architecture and reinforcement learning. ATNC uses a Differentiable Neural Computer as a generator and has a new specific block, called adversarial threshold (AT). AT acts as a filter between the agent (generator) and the environment (discriminator + objective reward functions). Furthermore, to generate more diverse molecules we introduce a new objective reward function named Internal Diversity Clustering (IDC). In this work, ATNC is tested and compared with the ORGANIC model. Both models were trained on the SMILES string representation of the molecules, using four objective functions (internal similarity, Muegge druglikeness filter, presence or absence of sp3-rich fragments, and IDC). The SMILES representations of 15K druglike molecules from the ChemDiv collection were used as a training data set. For the different functions, ATNC outperforms ORGANIC. Combined with the IDC, ATNC generates 72\% of valid and 77\% of unique SMILES strings, while ORGANIC generates only 7\% of valid and 86\% of unique SMILES strings. For each set of molecules generated by ATNC and ORGANIC, we analyzed distributions of four molecular descriptors (number of atoms, molecular weight, logP, and tpsa) and calculated five chemical statistical features (internal diversity, number of unique heterocycles, number of clusters, number of singletons, and number of compounds that have not been passed through medicinal chemistry filters). Analysis of key molecular descriptors and chemical statistical features demonstrated that the molecules generated by ATNC elicited better druglikeness properties. We also performed in vitro validation of the molecules generated by ATNC; results indicated that ATNC is an effective method for producing hit compounds.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\WCHC4HG6\\Putin et al_2018_Adversarial Threshold Neural Computer for Molecular de Novo Design.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\D3IIXE3F\\acs.molpharmaceut.html}
}

@article{putinReinforcedAdversarialNeural2018,
  title = {Reinforced {{Adversarial Neural Computer}} for de {{Novo Molecular Design}}},
  author = {Putin, Evgeny and Asadulaev, Arip and Ivanenkov, Yan and Aladinskiy, Vladimir and Sanchez-Lengeling, Benjamin and Aspuru-Guzik, Alán and Zhavoronkov, Alex},
  date = {2018-06-25},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {58},
  number = {6},
  pages = {1194--1204},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.7b00690},
  url = {https://doi.org/10.1021/acs.jcim.7b00690},
  urldate = {2019-05-06},
  abstract = {In silico modeling is a crucial milestone in modern drug design and development. Although computer-aided approaches in this field are well-studied, the application of deep learning methods in this research area is at the beginning. In this work, we present an original deep neural network (DNN) architecture named RANC (Reinforced Adversarial Neural Computer) for the de novo design of novel small-molecule organic structures based on the generative adversarial network (GAN) paradigm and reinforcement learning (RL). As a generator RANC uses a differentiable neural computer (DNC), a category of neural networks, with increased generation capabilities due to the addition of an explicit memory bank, which can mitigate common problems found in adversarial settings. The comparative results have shown that RANC trained on the SMILES string representation of the molecules outperforms its first DNN-based counterpart ORGANIC by several metrics relevant to drug discovery: the number of unique structures, passing medicinal chemistry filters (MCFs), Muegge criteria, and high QED scores. RANC is able to generate structures that match the distributions of the key chemical features/descriptors (e.g., MW, logP, TPSA) and lengths of the SMILES strings in the training data set. Therefore, RANC can be reasonably regarded as a promising starting point to develop novel molecules with activity against different biological targets or pathways. In addition, this approach allows scientists to save time and covers a broad chemical space populated with novel and diverse compounds.},
  keywords = {Algorithms,Computer-Aided Design,Deep Learning,Drug Design,Drug Discovery,Equipment Design,Machine Learning,Neural Networks Computer},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BSPMNSDA\\Putin et al. - 2018 - Reinforced Adversarial Neural Computer for ide N.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SYCDMKVU\\Putin et al_2018_Reinforced Adversarial Neural Computer for de Novo Molecular Design.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DGPTJ4QV\\acs.jcim.html}
}

@online{PytorchPytorch,
  title = {Pytorch/Pytorch},
  url = {https://github.com/pytorch/pytorch},
  urldate = {2019-09-19},
  abstract = {Tensors and Dynamic neural networks in Python with strong GPU acceleration - pytorch/pytorch},
  langid = {english},
  organization = {GitHub},
  file = {C:\Users\USEBPERP\Zotero\storage\HIU8BJ3X\v1.1.html}
}

@article{r.jacobsGasTurbineEngine2020,
  title = {Gas {{Turbine Engine Condition Monitoring Using Gaussian Mixture}} and {{Hidden Markov Models}}},
  author = {R. Jacobs, William and L. Edwards, Huw and Li, Ping and Kadirkamanathan, Visakan and Mills, Andrew R.},
  date = {2020-11-20},
  journaltitle = {IJPHM},
  volume = {9},
  number = {2},
  issn = {2153-2648, 2153-2648},
  doi = {10.36001/ijphm.2018.v9i2.2734},
  url = {https://papers.phmsociety.org/index.php/ijphm/article/view/2734},
  urldate = {2023-01-25},
  abstract = {This paper investigates the problem of condition monitoring of complex dynamic systems, specifically the detection, localisation and quantification of transient faults. A data driven approach is developed for fault detection where the multidimensional data sequence is viewed as a stochastic process whose behaviour can be described by a hidden Markov model with two hidden states — i.e. ‘healthy / nominal’ and ‘unhealthy / faulty’. The fault detection is performed by first clustering in a multidimensional data space to define normal operating behaviour using a Gaussian-Uniform mixture model. The health status of the system at each data point is then determined by evaluating the posterior probabilities of the hidden states of a hidden Markov model. This allows the temporal relationship between sequential data points to be incorporated into the fault detection scheme. The proposed scheme is robust to noise and requires minimal tuning. A real-world case study is performed based on the detection of transient faults in the variable stator vane actuator of a gas turbine engine to demonstrate the successful application of the scheme. The results are used to demonstrate the generation of simple and easily interpretable analytics that can be used to monitor the evolution of the fault across time.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2NAL4IWB\\R. Jacobs et al. - 2020 - Gas Turbine Engine Condition Monitoring Using Gaus.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\P4UHHGZT\\R. Jacobs et al. - 2020 - Gas Turbine Engine Condition Monitoring Using Gaus.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\Q62GDIQ3\\R. Jacobs et al. - 2020 - Gas Turbine Engine Condition Monitoring Using Gaus.pdf}
}

@article{radfordLanguageModelsAre,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  pages = {24},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\PSRMGDBZ\Radford et al_Language Models are Unsupervised Multitask Learners.pdf}
}

@unpublished{radfordLearningTransferableVisual2021,
  ids = {radfordlearning},
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  date = {2021},
  eprint = {2103.00020},
  eprinttype = {arxiv},
  pages = {47},
  url = {https://arxiv.org/abs/2103.00020},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.},
  langid = {english},
  keywords = {CLIP,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,contrastive,openAI},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\HWEER2B8\\Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\NU7YYHPW\\Radford et al. - Learning Transferable Visual Models From Natural L.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FZDBUXXE\\2103.html}
}

@inproceedings{rajiActionableAuditingInvestigating2019,
  title = {Actionable {{Auditing}}: {{Investigating}} the {{Impact}} of {{Publicly Naming Biased Performance Results}} of {{Commercial AI Products}}},
  shorttitle = {Actionable {{Auditing}}},
  booktitle = {Proceedings of the 2019 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Raji, Inioluwa Deborah and Buolamwini, Joy},
  date = {2019-01-27},
  series = {{{AIES}} '19},
  pages = {429--435},
  publisher = {Association for Computing Machinery},
  location = {Honolulu, HI, USA},
  doi = {10.1145/3306618.3314244},
  url = {https://doi.org/10.1145/3306618.3314244},
  urldate = {2020-06-22},
  abstract = {Although algorithmic auditing has emerged as a key strategy to expose systematic biases embedded in software platforms, we struggle to understand the real-world impact of these audits, as scholarship on the impact of algorithmic audits on increasing algorithmic fairness and transparency in commercial systems is nascent. To analyze the impact of publicly naming and disclosing performance results of biased AI systems, we investigate the commercial impact of Gender Shades, the first algorithmic audit of gender and skin type performance disparities in commercial facial analysis models. This paper 1) outlines the audit design and structured disclosure procedure used in the Gender Shades study, 2) presents new performance metrics from targeted companies IBM, Microsoft and Megvii (Face++) on the Pilot Parliaments Benchmark (PPB) as of August 2018, 3) provides performance results on PPB by non-target companies Amazon and Kairos and, 4) explores differences in company responses as shared through corporate communications that contextualize differences in performance on PPB. Within 7 months of the original audit, we find that all three targets released new API versions. All targets reduced accuracy disparities between males and females and darker and lighter-skinned subgroups, with the most significant update occurring for the darker-skinned female subgroup, that underwent a 17.7\% - 30.4\% reduction in error between audit periods. Minimizing these disparities led to a 5.72\% to 8.3\% reduction in overall error on the Pilot Parliaments Benchmark (PPB) for target corporation APIs. The overall performance of non-targets Amazon and Kairos lags significantly behind that of the targets, with error rates of 8.66\% and 6.60\% overall, and error rates of 31.37\% and 22.50\% for the darker female subgroup, respectively.},
  isbn = {978-1-4503-6324-2},
  keywords = {artificial intelligence,commercial applications,computer vision,ethics,facial recognition,fairness,machine learning},
  file = {C:\Users\USEBPERP\Zotero\storage\Q9UVF3Z4\Raji_Buolamwini_2019_Actionable Auditing.pdf}
}

@article{ramakrishnanQuantumChemistryStructures2014,
  title = {Quantum Chemistry Structures and Properties of 134 Kilo Molecules},
  author = {Ramakrishnan, Raghunathan and Dral, Pavlo O. and Rupp, Matthias and family=Lilienfeld, given=O. Anatole, prefix=von, useprefix=false},
  date = {2014-08-05},
  journaltitle = {Scientific Data},
  volume = {1},
  pages = {140022},
  issn = {2052-4463},
  doi = {10.1038/sdata.2014.22},
  url = {https://www.nature.com/articles/sdata201422},
  urldate = {2018-09-20},
  abstract = {Computational de novo design of new drugs and materials requires rigorous and unbiased exploration of chemical compound space. However, large uncharted territories persist due to its size scaling combinatorially with molecular size. We report computed geometric, energetic, electronic, and thermodynamic properties for 134k stable small organic molecules made up of CHONF. These molecules correspond to the subset of all 133,885 species with up to nine heavy atoms (CONF) out of the GDB-17 chemical universe of 166 billion organic molecules. We report geometries minimal in energy, corresponding harmonic frequencies, dipole moments, polarizabilities, along with energies, enthalpies, and free energies of atomization. All properties were calculated at the B3LYP/6-31G(2df,p) level of quantum chemistry. Furthermore, for the predominant stoichiometry, C7H10O2, there are 6,095 constitutional isomers among the 134k molecules. We report energies, enthalpies, and free energies of atomization at the more accurate G4MP2 level of theory for all of them. As such, this data set provides quantum chemical properties for a relevant, consistent, and comprehensive chemical space of small organic molecules. This database may serve the benchmarking of existing methods, development of new methods, such as hybrid quantum mechanics/machine learning, and systematic identification of structure-property relationships.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JVJ9SUTG\\Ramakrishnan et al_2014_Quantum chemistry structures and properties of 134 kilo molecules.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6VP9Z57Q\\sdata201422.html}
}

@article{ramaswamyEfficientAlgorithmsMining2000,
  title = {Efficient Algorithms for Mining Outliers from Large Data Sets},
  author = {Ramaswamy, Sridhar and Rastogi, Rajeev and Shim, Kyuseok},
  date = {2000-05-16},
  journaltitle = {SIGMOD Rec.},
  volume = {29},
  number = {2},
  pages = {427--438},
  issn = {0163-5808},
  doi = {10.1145/335191.335437},
  url = {https://doi.org/10.1145/335191.335437},
  urldate = {2023-01-30},
  abstract = {In this paper, we propose a novel formulation for distance-based outliers that is based on the distance of a point from its kth nearest neighbor. We rank each point on the basis of its distance to its kth nearest neighbor and declare the top n points in this ranking to be outliers. In addition to developing relatively straightforward solutions to finding such outliers based on the classical nested-loop join and index join algorithms, we develop a highly efficient partition-based algorithm for mining outliers. This algorithm first partitions the input data set into disjoint subsets, and then prunes entire partitions as soon as it is determined that they cannot contain outliers. This results in substantial savings in computation. We present the results of an extensive experimental study on real-life and synthetic data sets. The results from a real-life NBA database highlight and reveal several expected and unexpected aspects of the database. The results from a study on synthetic data sets demonstrate that the partition-based algorithm scales well with respect to both data set size and data set dimensionality.},
  file = {C:\Users\USEBPERP\Zotero\storage\EF9796M6\Ramaswamy et al. - 2000 - Efficient algorithms for mining outliers from larg.pdf}
}

@unpublished{ramsauerHopfieldNetworksAll2020,
  ids = {ramsauer2020hopfielda,ramsauer2020hopfieldb,ramsauerHopfieldNetworksAll2020a},
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Schäfl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Gruber, Lukas and Holzleitner, Markus and Pavlović, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  date = {2020-07-16},
  eprint = {2008.02217},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2008.02217},
  urldate = {2020-08-10},
  abstract = {We show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called "Hopfield", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: https://github.com/ml-jku/hopfield-layers},
  version = {1},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\EE9UI6YQ\\Ramsauer et al. - 2020 - Hopfield Networks is All You Need.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\J6EUPMUM\\Ramsauer et al. - 2020 - Hopfield Networks is All You Need.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LZ2AK8BT\\Ramsauer et al. - 2020 - Hopfield Networks is All You Need.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YHGQ6LHR\\Ramsauer et al_2020_Hopfield Networks is All You Need.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7GHPDBS4\\2008.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\MIK9QT64\\2008.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\QMSZBZII\\2008.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\V86LT5ED\\2008.html}
}

@article{rauberHindsightPolicyGradients2018,
  title = {Hindsight Policy Gradients},
  author = {Rauber, Paulo and Ummadisingu, Avinash and Mutz, Filipe and Schmidhuber, Jürgen},
  date = {2018-09-27},
  url = {https://openreview.net/forum?id=Bkg2viA5FQ},
  urldate = {2019-04-24},
  abstract = {A reinforcement learning agent that needs to pursue different goals across episodes requires a goal-conditional policy. In addition to their potential to generalize desirable behavior to unseen...},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\STZ7YZ3L\\Rauber et al_2018_Hindsight policy gradients.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\MCM6AGHP\\forum.html}
}

@unpublished{ravuriClassificationAccuracyScore2019,
  title = {Classification {{Accuracy Score}} for {{Conditional Generative Models}}},
  author = {Ravuri, Suman and Vinyals, Oriol},
  date = {2019-05-26},
  eprint = {1905.10887},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.10887},
  urldate = {2019-10-22},
  abstract = {Deep generative models (DGMs) of images are now sufficiently mature that they produce nearly photorealistic samples and obtain scores similar to the data distribution on heuristics such as Frechet Inception Distance. These results, especially on large-scale datasets such as ImageNet, suggest that DGMs are learning the data distribution in a perceptually meaningful space, and can be used in downstream tasks. To test this latter hypothesis, we use class-conditional generative models from a number of model classes---variational autoencoder, autoregressive models, and generative adversarial networks---to infer the class labels of real data. We perform this inference by training the image classifier using only synthetic data, and using the classifier to predict labels on real data. The performance on this task, which we call Classification Accuracy Score (CAS), highlights some surprising results not captured by traditional metrics and comprise our contributions. First, when using a state-of-the-art GAN (BigGAN), Top-5 accuracy decreases by 41.6\% compared to the original data and conditional generative models from other model classes, such as high-resolution VQ-VAE and Hierarchical Autoregressive Models, substantially outperform GANs on this benchmark. Second, CAS automatically surfaces particular classes for which generative models failed to capture the data distribution, and were previously unknown in the literature. Third, we find traditional GAN metrics such as Frechet Inception Distance neither predictive of CAS nor useful when evaluating non-GAN models. Finally, we introduce Naive Augmentation Score, a variant of CAS where the image classifier is trained on both real and synthetic data, to demonstrate that naive augmentation improves classification performance in limited circumstances. In order to facilitate better diagnoses of generative models, we open-source the proposed metric.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\GQE7EYMU\\Ravuri_Vinyals_2019_Classification Accuracy Score for Conditional Generative Models2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HBGZJ3AC\\Ravuri_Vinyals_2019_Classification Accuracy Score for Conditional Generative Models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TZ38NA7D\\1905.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\VSMMVL2U\\1905.html}
}

@unpublished{razaviGeneratingDiverseHighFidelity2019,
  title = {Generating {{Diverse High-Fidelity Images}} with {{VQ-VAE-2}}},
  author = {Razavi, Ali and family=Oord, given=Aaron, prefix=van den, useprefix=false and Vinyals, Oriol},
  date = {2019-06-02},
  eprint = {1906.00446},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.00446},
  urldate = {2019-10-22},
  abstract = {We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\SANDCZ97\\Razavi et al_2019_Generating Diverse High-Fidelity Images with VQ-VAE-2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\84QAM2TC\\1906.html}
}

@software{RDKITButinadiversityPicking2022,
  title = {{{RDKIT}}: {{Butina-diversity}} Picking},
  date = {2022-08-21T14:07:34Z},
  origdate = {2016-12-13T15:06:20Z},
  url = {https://github.com/InformaticsMatters/pipelines/blob/b0830631bc77745ee5c71df2ea2c624124594802/src/python/pipelines/rdkit/cluster_butina.py},
  urldate = {2022-08-22},
  abstract = {Containerised components for cheminformatics and computational chemistry},
  organization = {Informatics Matters Ltd}
}

@online{RDKitDiversityPicker2014,
  title = {{{RDKit Diversity Picker}} and {{Score Biasing}} - {{Community Extensions}}},
  date = {2014-03-24T21:57:36+00:00},
  url = {https://forum.knime.com/t/rdkit-diversity-picker-and-score-biasing/3713},
  urldate = {2023-09-01},
  abstract = {Hi,  The additional feature that was implemented a while back of the RDKit Diversity Picker to select against a cpd set is really helpful.  Is it possible to extend its versatility further by allowing the Diversity selection to be biased towards compounds which are maximised or minimised in a column property. i.e. So this could be activity, "Chemical Attractiveness", or "Lipinski Violations".  i.e. So rather than picking the most diverse compound, it picks one of the most diverse compounds which...},
  langid = {english},
  organization = {KNIME Community Forum}
}

@online{ReactionPrediction_Hopfield,
  title = {{{ReactionPrediction}}\_{{Hopfield}}},
  url = {https://www.overleaf.com/project/601d5fd39ef17445df93b715},
  urldate = {2021-04-08},
  abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\SRFMYSZH\601d5fd39ef17445df93b715.html}
}

@online{ReadingsIntroductionProbability,
  title = {Readings | {{Introduction}} to {{Probability}} and {{Statistics}} | {{Mathematics}} | {{MIT OpenCourseWare}}},
  url = {https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/},
  urldate = {2019-03-21},
  abstract = {This section provides the assigned readings and reading questions that students are required to complete prior to attending class sessions.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\NKUZ8EBF\readings.html}
}

@online{RecentApplicationsMachine,
  title = {Recent Applications of Machine Learning in Medicinal Chemistry | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.bmcl.2018.06.046},
  url = {https://reader.elsevier.com/reader/sd/pii/S0960894X1830547X?token=2858961FA03473E1A59A67D69588E06457D7CB4C5F96B76148CE72F941D8C8A41BDDBB340F5BF6A2E10058C4C17CF499&originRegion=eu-west-1&originCreation=20220829122038},
  urldate = {2022-08-29},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2KUVM32M\\Recent applications of machine learning in medicin.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\4HM9RDLX\\S0960894X1830547X.html}
}

@online{RecipeTrainingNeural,
  title = {A {{Recipe}} for {{Training Neural Networks}}},
  url = {https://karpathy.github.io/2019/04/25/recipe/},
  urldate = {2019-04-26},
  file = {C:\Users\USEBPERP\Zotero\storage\STPGU23R\recipe.html}
}

@online{redWarnungVorOrbanisierung2020,
  title = {Warnung vor „Orbanisierung“ an Universitäten},
  author = {family=red, given=salzburg ORF, prefix=at, useprefix=false},
  date = {2020-11-10},
  url = {https://salzburg.orf.at/stories/3075196/},
  urldate = {2020-11-10},
  abstract = {An der Universität Salzburg und der Universität Mozarteum gehen wegen eines neuen Universitätsgesetz-Entwurfs die Wogen hoch. Die Senatsvorsitzenden sehen sich darin entmachtet und für politische Einflussnahme Tür und Tor geöffnet. Eine „Orbanisierung“ drohe.},
  langid = {ngerman},
  organization = {salzburg.ORF.at},
  file = {C:\Users\USEBPERP\Zotero\storage\DZQZBBDV\3075196.html}
}

@article{reeRegioSQM20ImprovedPrediction2021,
  title = {{{RegioSQM20}}: Improved Prediction of the Regioselectivity of Electrophilic Aromatic Substitutions},
  shorttitle = {{{RegioSQM20}}},
  author = {Ree, Nicolai and Göller, Andreas H. and Jensen, Jan H.},
  date = {2021-02-12},
  journaltitle = {Journal of Cheminformatics},
  volume = {13},
  number = {1},
  pages = {10},
  issn = {1758-2946},
  doi = {10.1186/s13321-021-00490-7},
  url = {https://doi.org/10.1186/s13321-021-00490-7},
  urldate = {2021-04-18},
  abstract = {We present RegioSQM20, a new version of RegioSQM (Chem Sci 9:660, 2018), which predicts the regioselectivities of electrophilic aromatic substitution (EAS) reactions from the calculation of proton affinities. The following improvements have been made: The open source semiempirical tight binding program xtb is used instead of the closed source MOPAC program. Any low energy tautomeric forms of the input molecule are identified and regioselectivity predictions are made for each form. Finally, RegioSQM20 offers a qualitative prediction of the reactivity of each tautomer (low, medium, or high) based on the reaction center with the highest proton affinity. The inclusion of tautomers increases the success rate from 90.7 to 92.7\%. RegioSQM20 is compared to two machine learning based models: one developed by Struble et al. (React Chem Eng 5:896, 2020) specifically for regioselectivity predictions of EAS reactions (WLN) and a more generally applicable reactivity predictor (IBM RXN) developed by Schwaller et al. (ACS Cent Sci 5:1572, 2019). RegioSQM20 and WLN offers roughly the same success rates for the entire data sets (without considering tautomers), while WLN is many orders of magnitude faster. The accuracy of the more general IBM RXN approach is somewhat lower: 76.3–85.0\%, depending on the data set. The code is freely available under the MIT open source license and will be made available as a webservice (regiosqm.org) in the near future.},
  issue = {1}
}

@online{Register,
  title = {Register},
  url = {https://www.overleaf.com/register?project_name=LIT%20DeepToxGen%20Project%20Report&user_first_name=Institute%20for%20Machine%20Learning},
  urldate = {2020-06-25},
  abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\47FL3WSN\register.html}
}

@unpublished{renLearningReweightExamples2018,
  title = {Learning to {{Reweight Examples}} for {{Robust Deep Learning}}},
  author = {Ren, Mengye and Zeng, Wenyuan and Yang, Bin and Urtasun, Raquel},
  date = {2018-03-23},
  eprint = {1803.09050},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.09050},
  urldate = {2019-05-02},
  abstract = {Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns. However, they can also easily overfit to training set biases and label noises. In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters. In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions. To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set. Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\IXW8KIYR\\Ren et al_2018_Learning to Reweight Examples for Robust Deep Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TV7MX24F\\1803.html}
}

@unpublished{renLikelihoodRatiosOutofDistribution2019,
  title = {Likelihood {{Ratios}} for {{Out-of-Distribution Detection}}},
  author = {Ren, Jie and Liu, Peter J. and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and DePristo, Mark A. and Dillon, Joshua V. and Lakshminarayanan, Balaji},
  date = {2019-06-06},
  eprint = {1906.02845},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.02845},
  urldate = {2019-08-14},
  abstract = {Discriminative neural networks offer little or no performance guarantees when deployed on data not generated by the same process as the training distribution. On such out-of-distribution (OOD) inputs, the prediction may not only be erroneous, but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic sequences, which holds the promise of early detection of diseases, but requires a model that can output low confidence predictions on OOD genomic sequences from new bacteria that were not present in the training data. We introduce a genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem. We investigate deep generative model based approaches for OOD detection and observe that the likelihood score is heavily affected by population level background statistics. We propose a likelihood ratio method for deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed method against existing approaches on the genomics dataset and show that our method achieves state-of-the-art performance. We demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4CGNH5BP\\Ren et al_2019_Likelihood Ratios for Out-of-Distribution Detection.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\KPDRETZT\\1906.html}
}

@article{renzBenchmarkingEfficiencyGenerative2024,
  title = {Benchmarking the {{Efficiency}} of {{Generative Models}} in {{Diverse}} de {{Novo Optimization}}},
  author = {Renz, Philipp},
  date = {2024},
  abstract = {De novo design seeks to generate molecules with desired properties. The diversity of the generated molecules is of crucial importance for the success of drug discovery projects. However, previous comparisons of generative models in diverse optimization suffer from two common problems. Firstly, popular diversity metrics are not suited to quantify diversity in the context of de novo design. Secondly, many studies do not consider the search budget needed to find high-scoring molecules, resulting in unfair comparisons. In this study, we address these shortcomings and benchmark a range of generative models in diverse optimization tasks resulting in a meaningful overview of current model performance.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\4LV8QE47\Renz - Benchmarking the Efficiency of Generative Models i.pdf}
}

@article{renzFailureModesMolecule2019,
  ids = {renz2019failurea,renzFailureModesMolecule2019a},
  title = {On Failure Modes in Molecule Generation and Optimization},
  author = {Renz, Philipp and Van Rompaey, Dries and Wegner, Jörg Kurt and Hochreiter, Sepp and Klambauer, Günter},
  date = {2019-12-01},
  journaltitle = {Drug Discovery Today: Technologies},
  series = {Artificial {{Intelligence}}},
  volume = {32--33},
  pages = {55--63},
  issn = {1740-6749},
  doi = {10.1016/j.ddtec.2020.09.003},
  url = {https://www.sciencedirect.com/science/article/pii/S1740674920300159},
  urldate = {2021-03-08},
  abstract = {There has been a wave of generative models for molecules triggered by advances in the field of Deep Learning. These generative models are often used to optimize chemical compounds towards particular properties or a desired biological activity. The evaluation of generative models remains challenging and suggested performance metrics or scoring functions often do not cover all relevant aspects of drug design projects. In this work, we highlight some unintended failure modes in molecular generation and optimization and how these evade detection by current performance metrics.},
  langid = {english},
  keywords = {De novo molecule generation,Generative models for molecules,Machine learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\V8CJRQ2A\\Renz et al. - 2019 - On failure modes in molecule generation and optimi.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XXZV9YSZ\\Renz et al. - 2019 - On failure modes in molecule generation and optimi.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\3L9PP2CN\\S1740674920300159.html}
}

@inproceedings{renzLowCountTimeSeries2023,
  title = {Low-{{Count Time Series Anomaly Detection}}},
  booktitle = {2023 {{IEEE}} 33rd {{International Workshop}} on {{Machine Learning}} for {{Signal Processing}} ({{MLSP}})},
  author = {Renz, Philipp and Cutajar, Kurt and Twomey, Niall and Cheung, Gavin K. C. and Xie, Hanting},
  date = {2023-09},
  pages = {1--6},
  issn = {2161-0371},
  doi = {10.1109/MLSP55844.2023.10285979},
  url = {https://ieeexplore.ieee.org/abstract/document/10285979},
  urldate = {2023-12-20},
  abstract = {Low-count time series describe sparse or intermittent events, which are prevalent in large-scale online platforms that capture and monitor diverse data types. Several distinct challenges surface when modelling low-count time series, particularly low signal-to-noise ratios (when anomaly signatures are provably undetectable), and non-uniform performance (when average metrics are not representative of local behaviour). The time series anomaly detection community currently lacks explicit tooling and processes to model and reliably detect anomalies in these settings. We address this gap by introducing a novel generative procedure for creating benchmark datasets comprising of low-count time series with anomalous segments. Via a mixture of theoretical and empirical analysis, our work explains how widely-used algorithms struggle with the distribution overlap between normal and anomalous segments. In order to mitigate this shortcoming, we then leverage our findings to demonstrate how anomaly score smoothing consistently improves performance. The practical utility of our analysis and recommendation is validated on a real-world dataset containing sales data for retail stores.},
  eventtitle = {2023 {{IEEE}} 33rd {{International Workshop}} on {{Machine Learning}} for {{Signal Processing}} ({{MLSP}})},
  file = {C:\Users\USEBPERP\Zotero\storage\GTFAED3T\Renz et al. - 2023 - Low-Count Time Series Anomaly Detection.pdf}
}

@article{renzUncertaintyEstimationMethods2019,
  ids = {renzuncertainty},
  title = {Uncertainty {{Estimation Methods}} to {{Support Decision-Making}} in {{Early Phases}} of {{Drug Discovery}}},
  author = {Renz, Philipp and Hochreiter, Sepp and Klambauer, Günter},
  date = {2019},
  journaltitle = {NeurIPS-2019 Workshop on  Safety and Robustness in Decision Making},
  abstract = {It takes about a decade to develop a new drug by a process in which a large number of decisions have to be made. Those decisions are critical for the success or failure of a multi-million dollar drug discovery project, which could save many lives or increase life quality. Decisions in early phases of drug discovery, such as the selection of certain series of chemical compounds, are particularly impactful on the success rate. Machine learning models are increasingly used to inform the decision making process by predicting desired effects, undesired effects, such as toxicity, molecular properties, or which wet-lab test to perform next. Thus, accurately quantifying the uncertainties of the models’ outputs is critical, for example, in order to calculate expected utilities, to estimate the risk and the potential gain. In this work, we review, assess and compare recent uncertainty estimation methods with respect to their use in drug discovery projects. We test both, which methods give well calibrated prediction and which ones perform well at misclassification detection. For the latter, we find the entropy of the predictive distribution performs best. Finally, we discuss the problem of defining out-of-distribution samples for prediction tasks on chemical compounds.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ID42GDAI\\Renz et al. - Uncertainty Estimation Methods to Support Decision.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\UN64LBR6\\Renz et al_Uncertainty Estimation Methods to Support Decision-Making in Early Phases of.pdf}
}

@online{ResearcherMechatronik,
  title = {Researcher:In {{Mechatronik}}},
  shorttitle = {Researcher},
  url = {https://jobs.voestalpine.com/index.php?ac=jobad&id=9668},
  urldate = {2022-07-05},
  langid = {english},
  organization = {voestalpine Jobportal},
  file = {C:\Users\USEBPERP\Zotero\storage\YHHURXXV\index.html}
}

@online{ResearcherMechatronikBei,
  title = {Researcher:in Mechatronik bei voestalpine AG | karriere.at},
  shorttitle = {Researcher},
  url = {https://www.karriere.at/jobs/6276743},
  urldate = {2022-07-05},
  abstract = {Job zu vergeben: Researcher:in Mechatronik in Linz bei der Firma voestalpine AG. Jetzt bewerben und Arbeit finden. Weitere Stellenangebote auf karriere.at.},
  langid = {ngerman}
}

@online{RetrosyntheticReactionPrediction,
  title = {Retrosynthetic {{Reaction Prediction Using Neural Sequence-to-Sequence Models}} | {{ACS Central Science}}},
  url = {https://pubs.acs.org/doi/abs/10.1021/acscentsci.7b00303},
  urldate = {2021-04-08},
  file = {C:\Users\USEBPERP\Zotero\storage\WTTNUX8T\acscentsci.html}
}

@article{reymondEnumerationChemicalSpace2012,
  title = {The Enumeration of Chemical Space: {{Enumeration}} of Chemical Space},
  shorttitle = {The Enumeration of Chemical Space},
  author = {Reymond, Jean-Louis and Ruddigkeit, Lars and Blum, Lorenz and family=Deursen, given=Ruud, prefix=van, useprefix=true},
  date = {2012-09},
  journaltitle = {Wiley Interdisciplinary Reviews: Computational Molecular Science},
  volume = {2},
  number = {5},
  pages = {717--733},
  issn = {17590876},
  doi = {10.1002/wcms.1104},
  url = {http://doi.wiley.com/10.1002/wcms.1104},
  urldate = {2018-05-22},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\622PP3Z6\Reymond et al_2012_The enumeration of chemical space.pdf}
}

@unpublished{rezendeStochasticBackpropagationApproximate2014,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  date = {2014-05-30},
  eprint = {1401.4082},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1401.4082},
  urldate = {2019-12-23},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7XT9QH2L\\Rezende et al_2014_Stochastic Backpropagation and Approximate Inference in Deep Generative Models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\U3DYPIHY\\1401.html}
}

@unpublished{rezendeVariationalInferenceNormalizing2016,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  date = {2016-06-14},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1505.05770},
  urldate = {2019-12-23},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\FBK2A5TC\\Rezende_Mohamed_2016_Variational Inference with Normalizing Flows.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WTRF6CQM\\1505.html}
}

@article{rinikerOpensourcePlatformBenchmark2013,
  title = {Open-Source Platform to Benchmark Fingerprints for Ligand-Based Virtual Screening},
  author = {Riniker, Sereina and Landrum, Gregory A.},
  date = {2013-05-30},
  journaltitle = {J Cheminform},
  volume = {5},
  number = {1},
  eprint = {23721588},
  eprinttype = {pmid},
  pages = {26},
  issn = {1758-2946},
  doi = {10.1186/1758-2946-5-26},
  abstract = {: Similarity-search methods using molecular fingerprints are an important tool for ligand-based virtual screening. A huge variety of fingerprints exist and their performance, usually assessed in retrospective benchmarking studies using data sets with known actives and known or assumed inactives, depends largely on the validation data sets used and the similarity measure used. Comparing new methods to existing ones in any systematic way is rather difficult due to the lack of standard data sets and evaluation procedures. Here, we present a standard platform for the benchmarking of 2D fingerprints. The open-source platform contains all source code, structural data for the actives and inactives used (drawn from three publicly available collections of data sets), and lists of randomly selected query molecules to be used for statistically valid comparisons of methods. This allows the exact reproduction and comparison of results for future studies. The results for 12 standard fingerprints together with two simple baseline fingerprints assessed by seven evaluation methods are shown together with the correlations between methods. High correlations were found between the 12 fingerprints and a careful statistical analysis showed that only the two baseline fingerprints were different from the others in a statistically significant way. High correlations were also found between six of the seven evaluation methods, indicating that despite their seeming differences, many of these methods are similar to each other.},
  langid = {english},
  pmcid = {PMC3686626},
  file = {C:\Users\USEBPERP\Zotero\storage\4TM8UTDH\Riniker_Landrum_2013_Open-source platform to benchmark fingerprints for ligand-based virtual.pdf}
}

@book{robertsonOrderRestrictedStatistical1988,
  title = {Order {{Restricted Statistical Inference}}},
  author = {Robertson, Tim and Wright, F. T. and Dykstra, Richard},
  date = {1988},
  eprint = {sqZfQgAACAAJ},
  eprinttype = {googlebooks},
  publisher = {Wiley},
  abstract = {This work attempts to provide a comprehensive treatment of the topic of statistical inference under inequality constraints, in which much of the theory is based on the principles ofr maximum likelihood estimation and likelihood ratio tests.},
  isbn = {978-0-471-91787-8},
  langid = {english},
  pagetotal = {521},
  keywords = {Psychology / Statistics}
}

@article{rogersExtendedConnectivityFingerprints2010,
  title = {Extended-{{Connectivity Fingerprints}}},
  author = {Rogers, David and Hahn, Mathew},
  date = {2010-05-24},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {50},
  number = {5},
  pages = {742--754},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/ci100050t},
  url = {https://doi.org/10.1021/ci100050t},
  urldate = {2019-09-19},
  abstract = {Extended-connectivity fingerprints (ECFPs) are a novel class of topological fingerprints for molecular characterization. Historically, topological fingerprints were developed for substructure and similarity searching. ECFPs were developed specifically for structure−activity modeling. ECFPs are circular fingerprints with a number of useful qualities: they can be very rapidly calculated; they are not predefined and can represent an essentially infinite number of different molecular features (including stereochemical information); their features represent the presence of particular substructures, allowing easier interpretation of analysis results; and the ECFP algorithm can be tailored to generate different types of circular fingerprints, optimized for different uses. While the use of ECFPs has been widely adopted and validated, a description of their implementation has not previously been presented in the literature.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\S2G7VYDT\\Rogers and Hahn - 2010 - Extended-Connectivity Fingerprints.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8TS3M9IF\\ci100050t.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\RAXEAKWK\\ci100050t.html}
}

@unpublished{rohekarModelingUncertaintyLearning2019,
  title = {Modeling {{Uncertainty}} by {{Learning}} a {{Hierarchy}} of {{Deep Neural Connections}}},
  author = {Rohekar, Raanan Y. and Gurwicz, Yaniv and Nisimov, Shami and Novik, Gal},
  date = {2019-05-30},
  eprint = {1905.13195},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.13195},
  urldate = {2019-06-04},
  abstract = {Quantifying and measuring uncertainty in deep neural networks, despite recent important advances, is still an open problem. Bayesian neural networks are a powerful solution, where the prior over network weights is a design choice, often a normal distribution or other distribution encouraging sparsity. However, this prior is agnostic to the generative process of the input data, which might lead to unwarranted generalization for out-of-distribution tested data. We suggest treating the generative process of the input data as a confounder for the relation between the input and the discriminative function, thereby conditioning the prior of the network weights on the distribution of the input. We propose an algorithm for modeling this confounder through neural connectivity patterns. This approach is ultimately translated into a new deep architecture---a compact hierarchy of networks. We demonstrate that sampling networks from this hierarchy, proportionally to their posterior, is efficient and enables estimating various types of uncertainties. Empirical evaluations of our method demonstrate significant improvement compared to state-of-the-art calibration and out-of-distribution detection methods.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BXIPSMNP\\Rohekar et al_2019_Modeling Uncertainty by Learning a Hierarchy of Deep Neural Connections.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BCJZT5ZM\\1905.html}
}

@unpublished{rolnickTacklingClimateChange2019,
  title = {Tackling {{Climate Change}} with {{Machine Learning}}},
  author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
  date = {2019-06-10},
  eprint = {1906.05433},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1906.05433},
  urldate = {2019-06-26},
  abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\CFV5ZUI2\\Rolnick et al_2019_Tackling Climate Change with Machine Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\3MWFXCF2\\1906.html}
}

@article{rose-stockwellDarkPsychologySocial,
  entrysubtype = {magazine},
  title = {The {{Dark Psychology}} of {{Social Networks}}},
  author = {Rose-Stockwell, Story by Jonathan Haidt {and} Tobias},
  journaltitle = {The Atlantic},
  issn = {1072-7825},
  url = {https://www.theatlantic.com/magazine/archive/2019/12/social-media-democracy/600763/},
  urldate = {2020-11-04},
  abstract = {Why it feels like everything is going haywire},
  file = {C:\Users\USEBPERP\Zotero\storage\5KDUR5CN\600763.html}
}

@article{roseUseProcrustesAnalysis1994,
  title = {The {{Use}} of {{Procrustes Analysis}} to {{Compare Different Property Sets}} for the {{Characterization}} of a {{Diverse Set}} of {{Compounds}}},
  author = {Rose, Valerie S. and Rahr, Elizabeth and Hudson, Brian D.},
  date = {1994},
  journaltitle = {Quantitative Structure-Activity Relationships},
  volume = {13},
  number = {2},
  pages = {152--158},
  issn = {1521-3838},
  doi = {10.1002/qsar.19940130205},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qsar.19940130205},
  urldate = {2022-06-10},
  abstract = {Procrustes analysis has been used to compare the similarity of 14 different sets of property descriptors for characterizing a diverse set of pharmaceutical compounds. The analysis generated 3 distinct clusters from the property sets, which could be generally described as a) mostly 2D physicochemical properties, b) physicochemical properties based on overlaid 3D molecular structures, c) connectivity indices. Compound clustering and selection methods based on similarity measures calculated from a property set in one cluster would therefore give quite different results to those from a set in a different cluster.},
  langid = {english},
  keywords = {multivariate analysis,parameters,principal co-ordinates,Procrustes,properties,similarity},
  file = {C:\Users\USEBPERP\Zotero\storage\JDWQ2KSY\qsar.html}
}

@article{rossettiNoncovalentSARSCoV2Mpro2022,
  title = {Non-Covalent {{SARS-CoV-2 Mpro}} Inhibitors Developed from in Silico Screen Hits},
  author = {Rossetti, Giacomo G. and Ossorio, Marianna A. and Rempel, Stephan and Kratzel, Annika and Dionellis, Vasilis S. and Barriot, Samia and Tropia, Laurence and Gorgulla, Christoph and Arthanari, Haribabu and Thiel, Volker and Mohr, Peter and Gamboni, Remo and Halazonetis, Thanos D.},
  date = {2022-02-15},
  journaltitle = {Sci Rep},
  volume = {12},
  number = {1},
  pages = {2505},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-06306-4},
  url = {https://www.nature.com/articles/s41598-022-06306-4},
  urldate = {2022-07-28},
  abstract = {Mpro, the main protease of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), is essential for the viral life cycle. Accordingly, several groups have performed in silico screens to identify Mpro inhibitors that might be used to treat SARS-CoV-2 infections. We selected more than five hundred compounds from the top-ranking hits of two very large in silico screens for on-demand synthesis. We then examined whether these compounds could bind to Mpro and inhibit its protease activity. Two interesting chemotypes were identified, which were further evaluated by characterizing an additional five hundred synthesis on-demand analogues. The compounds of the first chemotype denatured Mpro and were considered not useful for further development. The compounds of the second chemotype bound to and enhanced the melting temperature of Mpro. The most active compound from this chemotype inhibited Mpro in vitro with an IC50 value of 1~μM and suppressed replication of the SARS-CoV-2 virus in tissue culture cells. Its mode of binding to Mpro was determined by X-ray crystallography, revealing that it is a non-covalent inhibitor. We propose that the inhibitors described here could form the basis for medicinal chemistry efforts that could lead to the development of clinically relevant inhibitors.},
  issue = {1},
  langid = {english},
  keywords = {Screening,Small molecules,X-ray crystallography},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\MD5P4J8A\\Rossetti et al. - 2022 - Non-covalent SARS-CoV-2 Mpro inhibitors developed .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\27WX6586\\s41598-022-06306-4.html}
}

@article{ruddigkeitEnumeration166Billion2012,
  title = {Enumeration of 166 {{Billion Organic Small Molecules}} in the {{Chemical Universe Database GDB-17}}},
  author = {Ruddigkeit, Lars and family=Deursen, given=Ruud, prefix=van, useprefix=true and Blum, Lorenz C. and Reymond, Jean-Louis},
  date = {2012-11-26},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {52},
  number = {11},
  pages = {2864--2875},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/ci300415d},
  url = {https://doi.org/10.1021/ci300415d},
  urldate = {2020-03-17},
  abstract = {Drug molecules consist of a few tens of atoms connected by covalent bonds. How many such molecules are possible in total and what is their structure? This question is of pressing interest in medicinal chemistry to help solve the problems of drug potency, selectivity, and toxicity and reduce attrition rates by pointing to new molecular series. To better define the unknown chemical space, we have enumerated 166.4 billion molecules of up to 17 atoms of C, N, O, S, and halogens forming the chemical universe database GDB-17, covering a size range containing many drugs and typical for lead compounds. GDB-17 contains millions of isomers of known drugs, including analogs with high shape similarity to the parent drug. Compared to known molecules in PubChem, GDB-17 molecules are much richer in nonaromatic heterocycles, quaternary centers, and stereoisomers, densely populate the third dimension in shape space, and represent many more scaffold types.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\QH6BJ9AG\\Ruddigkeit et al_2012_Enumeration of 166 Billion Organic Small Molecules in the Chemical Universe.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HBCZ4PRX\\ci300415d.html}
}

@article{ruffUnifyingReviewDeep2021,
  title = {A {{Unifying Review}} of {{Deep}} and {{Shallow Anomaly Detection}}},
  author = {Ruff, Lukas and Kauffmann, Jacob R. and Vandermeulen, Robert A. and Montavon, Grégoire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G. and Müller, Klaus-Robert},
  date = {2021-05},
  journaltitle = {Proc. IEEE},
  volume = {109},
  number = {5},
  eprint = {2009.11732},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {756--795},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2021.3052449},
  url = {http://arxiv.org/abs/2009.11732},
  urldate = {2023-01-05},
  abstract = {Deep learning approaches to anomaly detection have recently improved the state of the art in detection performance on complex datasets such as large collections of images or text. These results have sparked a renewed interest in the anomaly detection problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review we aim to identify the common underlying principles as well as the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that is enriched by the use of recent explainability techniques, and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in anomaly detection.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\A7FZ88CS\\Ruff et al. - 2021 - A Unifying Review of Deep and Shallow Anomaly Dete.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\UEDCDFXX\\2009.html}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  date = {1986-10},
  journaltitle = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  url = {https://www.nature.com/articles/323533a0},
  urldate = {2020-11-10},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  issue = {6088},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\WP92J9YL\323533a0.html}
}

@article{rupakhetiStrategyDiscoverDiverse2015,
  title = {Strategy {{To Discover Diverse Optimal Molecules}} in the {{Small Molecule Universe}}},
  author = {Rupakheti, Chetan and Virshup, Aaron and Yang, Weitao and Beratan, David N.},
  date = {2015-03-23},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {55},
  number = {3},
  pages = {529--537},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/ci500749q},
  url = {https://doi.org/10.1021/ci500749q},
  urldate = {2023-09-29},
  abstract = {The small molecule universe (SMU) is defined as a set of over 1060 synthetically feasible organic molecules with molecular weight less than ∼500 Da. Exhaustive enumerations and evaluation of all SMU molecules for the purpose of discovering favorable structures is impossible. We take a stochastic approach and extend the ACSESS framework (Virshup et al. J. Am. Chem. Soc. 2013, 135, 7296–7303) to develop diversity oriented molecular libraries that can generate a set of compounds that is representative of the small molecule universe and that also biases the library toward favorable physical property values. We show that the approach is efficient compared to exhaustive enumeration and to existing evolutionary algorithms for generating such libraries by testing in the NKp fitness landscape model and in the fully enumerated GDB-9 chemical universe containing 3 × 105 molecules.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\3DIMVV9A\\Rupakheti et al. - 2015 - Strategy To Discover Diverse Optimal Molecules in .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XB6CFBC6\\ci500749q.html}
}

@unpublished{russakovskyImageNetLargeScale2014,
  title = {{{ImageNet Large Scale Visual Recognition Challenge}}},
  author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  date = {2014-09-01},
  eprint = {1409.0575},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1409.0575},
  urldate = {2018-05-24},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,I.4.8,I.5.2},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7AXUFEG2\\Russakovsky et al_2014_ImageNet Large Scale Visual Recognition Challenge.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SCHIIULJ\\1409.html}
}

@unpublished{ryuUncertaintyQuantificationMolecular2019,
  title = {Uncertainty Quantification of Molecular Property Prediction with {{Bayesian}} Neural Networks},
  author = {Ryu, Seongok and Kwon, Yongchan and Kim, Woo Youn},
  date = {2019-03-20},
  eprint = {1903.08375},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1903.08375},
  urldate = {2019-04-15},
  abstract = {Deep neural networks have outperformed existing machine learning models in various molecular applications. In practical applications, it is still difficult to make confident decisions because of the uncertainty in predictions arisen from insufficient quality and quantity of training data. Here, we show that Bayesian neural networks are useful to quantify the uncertainty of molecular property prediction with three numerical experiments. In particular, it enables us to decompose the predictive variance into the model- and data-driven uncertainties, which helps to elucidate the source of errors. In the logP predictions, we show that data noise affected the data-driven uncertainties more significantly than the model-driven ones. Based on this analysis, we were able to find unexpected errors in the Harvard Clean Energy Project dataset. Lastly, we show that the confidence of prediction is closely related to the predictive uncertainty by performing on bio-activity and toxicity classification problems.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\VUC8A7I6\\Ryu et al_2019_Uncertainty quantification of molecular property prediction with Bayesian.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DE4TJ24C\\1903.html}
}

@unpublished{sachaMoleculeEditGraph2020,
  ids = {sacha2020moleculea},
  title = {Molecule {{Edit Graph Attention Network}}: {{Modeling Chemical Reactions}} as {{Sequences}} of {{Graph Edits}}},
  shorttitle = {Molecule {{Edit Graph Attention Network}}},
  author = {Sacha, Mikołaj and Błaż, Mikołaj and Byrski, Piotr and Włodarczyk-Pruszyński, Paweł and Jastrzębski, Stanisław},
  date = {2020-06-27},
  eprint = {2006.15426},
  eprinttype = {arxiv},
  eprintclass = {physics, stat},
  url = {http://arxiv.org/abs/2006.15426},
  urldate = {2021-04-18},
  abstract = {One of the key challenges in automated synthesis planning is to generate diverse and reliable chemical reactions. Many reactions can be naturally represented using graph transformation rules referred broadly to as reaction templates. Using reaction templates enables accurate and interpretable predictions but can suffer from limited coverage of the reaction space. On the other hand, template-free methods can increase the coverage but can be prone to making trivial mistakes and are challenging to interpret. A promising idea for constructing more interpretable template-free models is to model a reaction as a sequence of graph edits of the substrates. We extend this idea to retrosynthesis and scale it up to large datasets. We propose Molecule Edit Graph Attention Network (MEGAN), a template-free neural model that encodes reaction as a sequence of graph edits. We achieve competitive performance on both retrosynthesis and forward synthesis and in particular state-of-the-art top-k accuracy for larger K values. Crucially, the latter shows excellent coverage of the reaction space of our model. In summary, MEGAN brings together the strong elements of template-free and template-based models and can be applied to both retro and forward synthesis tasks.},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Statistics - Machine Learning}
}

@unpublished{safranHowGoodSGD2019,
  title = {How {{Good}} Is {{SGD}} with {{Random Shuffling}}?},
  author = {Safran, Itay and Shamir, Ohad},
  date = {2019-07-31},
  eprint = {1908.00045},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1908.00045},
  urldate = {2019-08-26},
  abstract = {We study the performance of stochastic gradient descent (SGD) on smooth and strongly-convex finite-sum optimization problems. In contrast to the majority of existing theoretical works, which assume that individual functions are sampled with replacement, we focus here on popular but poorly-understood heuristics, which involve going over random permutations of the individual functions. This setting has been investigated in several recent works, but the optimal error rates remains unclear. In this paper, we provide lower bounds on the expected optimization error with these heuristics (using SGD with any constant step size), which elucidate their advantages and disadvantages. In particular, we prove that after \$k\$ passes over \$n\$ individual functions, if the functions are re-shuffled after every pass, the best possible optimization error for SGD is at least \$\textbackslash Omega\textbackslash left(1/(nk)\textasciicircum 2+1/nk\textasciicircum 3\textbackslash right)\$, which partially corresponds to recently derived upper bounds, and we conjecture to be tight. Moreover, if the functions are only shuffled once, then the lower bound increases to \$\textbackslash Omega(1/nk\textasciicircum 2)\$. Since there are strictly smaller upper bounds for random reshuffling, this proves an inherent performance gap between SGD with single shuffling and repeated shuffling. As a more minor contribution, we also provide a non-asymptotic \$\textbackslash Omega(1/k\textasciicircum 2)\$ lower bound (independent of \$n\$) for cyclic gradient descent, where no random shuffling takes place.},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\TAGW7FAD\\Safran_Shamir_2019_How Good is SGD with Random Shuffling.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AEGUCBGT\\1908.html}
}

@unpublished{sajjadiAssessingGenerativeModels2018,
  title = {Assessing {{Generative Models}} via {{Precision}} and {{Recall}}},
  author = {Sajjadi, Mehdi S. M. and Bachem, Olivier and Lucic, Mario and Bousquet, Olivier and Gelly, Sylvain},
  date = {2018-05-31},
  eprint = {1806.00035},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1806.00035},
  urldate = {2018-11-06},
  abstract = {Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\HZEMMFKR\\Sajjadi et al_2018_Assessing Generative Models via Precision and Recall.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\INIFYBEU\\1806.html}
}

@unpublished{salimansImprovedTechniquesTraining2016,
  title = {Improved {{Techniques}} for {{Training GANs}}},
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  date = {2016-06-10},
  eprint = {1606.03498},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1606.03498},
  urldate = {2018-05-24},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\RWZRCPD8\\Salimans et al_2016_Improved Techniques for Training GANs.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TC5P2DSN\\1606.html}
}

@unpublished{salimansPixelCNNImprovingPixelCNN2017,
  title = {{{PixelCNN}}++: {{Improving}} the {{PixelCNN}} with {{Discretized Logistic Mixture Likelihood}} and {{Other Modifications}}},
  shorttitle = {{{PixelCNN}}++},
  author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P.},
  date = {2017-01-19},
  eprint = {1701.05517},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1701.05517},
  urldate = {2019-10-21},
  abstract = {PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7EBZV2SP\\Salimans et al_2017_PixelCNN++.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BCUK94J6\\1701.html}
}

@online{salinasDeepARProbabilisticForecasting2019,
  title = {{{DeepAR}}: {{Probabilistic Forecasting}} with {{Autoregressive Recurrent Networks}}},
  shorttitle = {{{DeepAR}}},
  author = {Salinas, David and Flunkert, Valentin and Gasthaus, Jan},
  date = {2019-02-22},
  eprint = {1704.04110},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1704.04110},
  url = {http://arxiv.org/abs/1704.04110},
  urldate = {2022-08-16},
  abstract = {Probabilistic forecasting, i.e. estimating the probability distribution of a time series' future given its past, is a key enabler for optimizing business processes. In retail businesses, for example, forecasting demand is crucial for having the right inventory available at the right time at the right place. In this paper we propose DeepAR, a methodology for producing accurate probabilistic forecasts, based on training an auto regressive recurrent network model on a large number of related time series. We demonstrate how by applying deep learning techniques to forecasting, one can overcome many of the challenges faced by widely-used classical approaches to the problem. We show through extensive empirical evaluation on several real-world forecasting data sets accuracy improvements of around 15\% compared to state-of-the-art methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\APUZGJEI\\Salinas et al. - 2019 - DeepAR Probabilistic Forecasting with Autoregress.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\IQ7B98NG\\1704.html}
}

@online{salterThisGoodWay2019,
  type = {Forum post},
  title = {Is This a Good Way of Generating Diverse Solutions to a Problem Using a Genetic Algorithm?},
  author = {Salter, John},
  date = {2019-07-07},
  url = {https://cs.stackexchange.com/q/111585},
  urldate = {2022-06-21},
  organization = {Computer Science Stack Exchange},
  file = {C:\Users\USEBPERP\Zotero\storage\VP7VLQ98\is-this-a-good-way-of-generating-diverse-solutions-to-a-problem-using-a-genetic.html}
}

@unpublished{samantaDesigningRandomGraph2018,
  title = {Designing {{Random Graph Models Using Variational Autoencoders With Applications}} to {{Chemical Design}}},
  author = {Samanta, Bidisha and De, Abir and Ganguly, Niloy and Gomez-Rodriguez, Manuel},
  date = {2018-02-14},
  eprint = {1802.05283},
  eprinttype = {arxiv},
  eprintclass = {physics, stat},
  url = {http://arxiv.org/abs/1802.05283},
  urldate = {2018-09-18},
  abstract = {Deep generative models have been praised for their ability to learn smooth latent representation of images, text, and audio, which can then be used to generate new, plausible data. However, current generative models are unable to work with graphs due to their unique characteristics--their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes labels, and they come with a different number of nodes and edges. In this paper, we propose NeVAE, a novel variational autoencoder for graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. In addition, by using masking, the decoder is able to guarantee a set of local structural and functional properties in the generated graphs. Experiments reveal that our model is able to learn and mimic the generative process of several well-known random graph models and can be used to discover new molecules more effectively than several state of the art methods. Moreover, by utilizing Bayesian optimization over the continuous latent representation of molecules our model finds, we can also identify molecules that maximize certain desirable properties more effectively than alternatives.},
  keywords = {Computer Science - Machine Learning,Physics - Physics and Society,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\SDPIJMZ3\\Samanta et al_2018_Designing Random Graph Models Using Variational Autoencoders With Applications.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XJZ7G2P7\\1802.html}
}

@unpublished{samantaNeVAEDeepGenerative2018,
  title = {{{NeVAE}}: {{A Deep Generative Model}} for {{Molecular Graphs}}},
  shorttitle = {{{NeVAE}}},
  author = {Samanta, Bidisha and De, Abir and Jana, Gourhari and Chattaraj, Pratim Kumar and Ganguly, Niloy and Gomez-Rodriguez, Manuel},
  date = {2018-02-14},
  eprint = {1802.05283},
  eprinttype = {arxiv},
  eprintclass = {physics, stat},
  url = {http://arxiv.org/abs/1802.05283},
  urldate = {2019-05-06},
  abstract = {Deep generative models have been praised for their ability to learn smooth latent representation of images, text, and audio, which can then be used to generate new, plausible data. However, current generative models are unable to work with molecular graphs due to their unique characteristics--their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the node labels, and they come with a different number of nodes and edges. In this paper, we propose NeVAE, a novel variational autoencoder for molecular graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. In addition, by using masking, the decoder is able to guarantee a set of valid properties in the generated molecules. Experiments reveal that our model can discover plausible, diverse and novel molecules more effectively than several state of the art methods. Moreover, by utilizing Bayesian optimization over the continuous latent representation of molecules our model finds, we can also find molecules that maximize certain desirable properties more effectively than alternatives.},
  keywords = {Computer Science - Machine Learning,Physics - Physics and Society,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NTV6E9B9\\Samanta et al_2018_NeVAE.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YCJ3GK7H\\1802.html}
}

@article{samuelsonNoteMeasurementUtility1937,
  title = {A {{Note}} on {{Measurement}} of {{Utility}}},
  author = {Samuelson, Paul A.},
  date = {1937-02},
  journaltitle = {The Review of Economic Studies},
  volume = {4},
  number = {2},
  pages = {155},
  issn = {00346527},
  doi = {10.2307/2967612},
  url = {https://academic.oup.com/restud/article-lookup/doi/10.2307/2967612},
  urldate = {2019-03-01},
  file = {C:\Users\USEBPERP\Zotero\storage\BIBWPG4U\Samuelson_1937_A Note on Measurement of Utility.pdf}
}

@article{samuelStudiesMachineLearning1959,
  title = {Some {{Studies}} in {{Machine Learning Using}} the {{Game}} of {{Checkers}}},
  author = {Samuel, A. L.},
  date = {1959-07},
  journaltitle = {IBM Journal of Research and Development},
  volume = {3},
  number = {3},
  pages = {210--229},
  issn = {0018-8646},
  doi = {10.1147/rd.33.0210},
  abstract = {Two machine-learning procedures have been investigated in some detail using the game of checkers. Enough work has been done to verify the fact that a computer can be programmed so that it will learn to play a better game of checkers than can be played by the person who wrote the program. Furthermore, it can learn to do this in a remarkably short period of time (8 or 10 hours of machine-playing time) when given only the rules of the game, a sense of direction, and a redundant and incomplete list of parameters which are thought to have something to do with the game, but whose correct signs and relative weights are unknown and unspecified. The principles of machine learning verified by these experiments are, of course, applicable to many other situations.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LL8HTKT6\\Samuel_1959_Some Studies in Machine Learning Using the Game of Checkers.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\CUZY9MTD\\5392560.html}
}

@article{sanchez-lengelingInverseMolecularDesign2018,
  title = {Inverse Molecular Design Using Machine Learning: {{Generative}} Models for Matter Engineering},
  shorttitle = {Inverse Molecular Design Using Machine Learning},
  author = {Sanchez-Lengeling, Benjamin and Aspuru-Guzik, Alán},
  date = {2018-07-27},
  journaltitle = {Science},
  volume = {361},
  number = {6400},
  eprint = {30049875},
  eprinttype = {pmid},
  pages = {360--365},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aat2663},
  url = {https://science.sciencemag.org/content/361/6400/360},
  urldate = {2019-05-06},
  abstract = {{$<$}p{$>$}The discovery of new materials can bring enormous societal and technological progress. In this context, exploring completely the large space of potential materials is computationally intractable. Here, we review methods for achieving inverse design, which aims to discover tailored materials from the starting point of a particular desired functionality. Recent advances from the rapidly growing field of artificial intelligence, mostly from the subfield of machine learning, have resulted in a fertile exchange of ideas, where approaches to inverse molecular design are being proposed and employed at a rapid pace. Among these, deep generative models have been applied to numerous classes of materials: rational design of prospective drugs, synthetic routes to organic compounds, and optimization of photovoltaics and redox flow batteries, as well as a variety of other solid-state materials.{$<$}/p{$>$}},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\HJIFIJZN\\Sanchez-Lengeling_Aspuru-Guzik_2018_Inverse molecular design using machine learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6I53AG69\\tab-figures-data.html}
}

@article{santosComprehensiveMapMolecular2017,
  title = {A Comprehensive Map of Molecular Drug Targets},
  author = {Santos, Rita and Ursu, Oleg and Gaulton, Anna and Bento, A. Patrícia and Donadi, Ramesh S. and Bologa, Cristian G. and Karlsson, Anneli and Al-Lazikani, Bissan and Hersey, Anne and Oprea, Tudor I. and Overington, John P.},
  date = {2017-01},
  journaltitle = {Nat Rev Drug Discov},
  volume = {16},
  number = {1},
  eprint = {27910877},
  eprinttype = {pmid},
  pages = {19--34},
  issn = {1474-1784},
  doi = {10.1038/nrd.2016.230},
  abstract = {The success of mechanism-based drug discovery depends on the definition of the drug target. This definition becomes even more important as we try to link drug response to genetic variation, understand stratified clinical efficacy and safety, rationalize the differences between drugs in the same therapeutic class and predict drug utility in patient subgroups. However, drug targets are often poorly defined in the literature, both for launched drugs and for potential therapeutic agents in discovery and development. Here, we present an updated comprehensive map of molecular targets of approved drugs. We curate a total of 893 human and pathogen-derived biomolecules through which 1,578 US FDA-approved drugs act. These biomolecules include 667 human-genome-derived proteins targeted by drugs for human disease. Analysis of these drug targets indicates the continued dominance of privileged target families across disease areas, but also the growth of novel first-in-class mechanisms, particularly in oncology. We explore the relationships between bioactivity class and clinical success, as well as the presence of orthologues between human and animal models and between pathogen and human genomes. Through the collaboration of three independent teams, we highlight some of the ongoing challenges in accurately defining the targets of molecular therapeutics and present conventions for deconvoluting the complexities of molecular pharmacology and drug efficacy.},
  langid = {english},
  pmcid = {PMC6314433},
  keywords = {Databases Pharmaceutical,Drug Approval,Drug Delivery Systems,Drug Discovery,Drug Prescriptions,Genetic Variation,Genome Human,Humans,Pharmacogenetics,United States,United States Food and Drug Administration},
  file = {C:\Users\USEBPERP\Zotero\storage\L7MGKVWC\Santos et al_2017_A comprehensive map of molecular drug targets.pdf}
}

@unpublished{santurkarHowDoesBatch2018,
  title = {How {{Does Batch Normalization Help Optimization}}?},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  date = {2018-05-29},
  eprint = {1805.11604},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1805.11604},
  urldate = {2018-11-06},
  abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\XQSSY97M\\Santurkar et al_2018_How Does Batch Normalization Help Optimization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JTFWT2Y4\\1805.html}
}

@online{SatireNichtAlles,
  title = {Satire, die nicht alles darf: Die schwierige Geschichte von "Gute Nacht Österreich" im ORF},
  shorttitle = {Satire, die nicht alles darf},
  url = {https://www.derstandard.at/story/2000122195416/satire-die-nicht-alles-darf-die-schwierige-geschichte-von-gute},
  urldate = {2020-12-04},
  abstract = {Mit Grant des Generals geht Peter Kliens ambitioniertes Late-Night-Projekt seinem Ende in ORF 1 zu – Update: Wrabetz will GNÖ weiter im Fernsehen, aber anders},
  langid = {austrian},
  organization = {DER STANDARD},
  file = {C:\Users\USEBPERP\Zotero\storage\X8T2XUSN\satire-die-nicht-alles-darf-die-schwierige-geschichte-von-gute.html}
}

@online{sayle2DSimilarityDiversity2019,
  title = {{{2D Similarity}}, {{Diversity}} and {{Clustering}} in {{RDKit}}},
  author = {Sayle, Roger},
  date = {2019},
  url = {https://www.nextmovesoftware.com/talks/Sayle_2DSimilarityDiversityAndClusteringInRdkit_RDKITUGM_201909.pdf},
  urldate = {2023-11-17},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\JPQAEJSH\Sayle - 2019 - Clustering in RDKit 2019.pdf}
}

@article{sayleDiversitySelection2017,
  title = {Diversity Selection},
  author = {Sayle, Roger},
  date = {2017},
  pages = {25},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\AGPADGZM\Sayle - 2017 - Recent improvements to the rdkit.pdf}
}

@article{sayleImprovedCompoundLibrary,
  title = {Improved Compound Library Enhancement Using Artificial Intelligence Algorithms from Computer Chess},
  author = {Sayle, Roger and Mayfield, John and O’Boyle, Noel and Zorn, Nicolas},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\MRKGGSBB\Sayle et al. - improved compound library enhancement using artifi.pdf}
}

@article{sayleRecentImprovementsRdkit2017,
  title = {Recent Improvements to the Rdkit},
  author = {Sayle, Roger},
  date = {2017},
  pages = {25},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\BYT8XPVQ\Sayle - 2017 - Recent improvements to the rdkit.pdf}
}

@article{scannellDiagnosingDeclinePharmaceutical2012,
  title = {Diagnosing the Decline in Pharmaceutical {{R}}\&{{D}} Efficiency},
  author = {Scannell, Jack W. and Blanckley, Alex and Boldon, Helen and Warrington, Brian},
  date = {2012-03},
  journaltitle = {Nature Reviews Drug Discovery},
  volume = {11},
  number = {3},
  pages = {191--200},
  publisher = {Nature Publishing Group},
  issn = {1474-1784},
  doi = {10.1038/nrd3681},
  url = {https://www.nature.com/articles/nrd3681},
  urldate = {2020-07-09},
  abstract = {The number of new drugs approved per billion US dollars spent on research and development (R\&D) has fallen around 80-fold in inflation-adjusted terms since 1950, despite advances in many of the scientific and technological inputs into the R\&D process. Given the apparent lack of impact of proposed solutions to declining R\&D efficiency so far, Scannell and colleagues ask whether the underlying problems have been correctly diagnosed and discuss factors that they consider to be the primary causes.},
  issue = {3},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\H2KYD42B\nrd3681.html}
}

@article{scannellWhenQualityBeats2016,
  title = {When {{Quality Beats Quantity}}: {{Decision Theory}}, {{Drug Discovery}}, and the {{Reproducibility Crisis}}},
  shorttitle = {When {{Quality Beats Quantity}}},
  author = {Scannell, Jack W. and Bosley, Jim},
  date = {2016-02-10},
  journaltitle = {PLOS ONE},
  volume = {11},
  number = {2},
  pages = {e0147215},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0147215},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0147215},
  urldate = {2020-07-09},
  abstract = {A striking contrast runs through the last 60 years of biopharmaceutical discovery, research, and development. Huge scientific and technological gains should have increased the quality of academic science and raised industrial R\&D efficiency. However, academia faces a "reproducibility crisis"; inflation-adjusted industrial R\&D costs per novel drug increased nearly 100 fold between 1950 and 2010; and drugs are more likely to fail in clinical development today than in the 1970s. The contrast is explicable only if powerful headwinds reversed the gains and/or if many "gains" have proved illusory. However, discussions of reproducibility and R\&D productivity rarely address this point explicitly. The main objectives of the primary research in this paper are: (a) to provide quantitatively and historically plausible explanations of the contrast; and (b) identify factors to which R\&D efficiency is sensitive. We present a quantitative decision-theoretic model of the R\&D process. The model represents therapeutic candidates (e.g., putative drug targets, molecules in a screening library, etc.) within a “measurement space", with candidates' positions determined by their performance on a variety of assays (e.g., binding affinity, toxicity, in vivo efficacy, etc.) whose results correlate to a greater or lesser degree. We apply decision rules to segment the space, and assess the probability of correct R\&D decisions. We find that when searching for rare positives (e.g., candidates that will successfully complete clinical development), changes in the predictive validity of screening and disease models that many people working in drug discovery would regard as small and/or unknowable (i.e., an 0.1 absolute change in correlation coefficient between model output and clinical outcomes in man) can offset large (e.g., 10 fold, even 100 fold) changes in models’ brute-force efficiency. We also show how validity and reproducibility correlate across a population of simulated screening and disease models. We hypothesize that screening and disease models with high predictive validity are more likely to yield good answers and good treatments, so tend to render themselves and their diseases academically and commercially redundant. Perhaps there has also been too much enthusiasm for reductionist molecular models which have insufficient predictive validity. Thus we hypothesize that the average predictive validity of the stock of academically and industrially "interesting" screening and disease models has declined over time, with even small falls able to offset large gains in scientific knowledge and brute-force efficiency. The rate of creation of valid screening and disease models may be the major constraint on R\&D productivity.},
  langid = {english},
  keywords = {Drug discovery,Drug research and development,Drug screening,High throughput screening,Library screening,Normal distribution,Probability density,Signal to noise ratio},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\N32SNZZW\\Scannell_Bosley_2016_When Quality Beats Quantity.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QPSQXR6Z\\article.html}
}

@article{scarselliGraphNeuralNetwork2009,
  title = {The {{Graph Neural Network Model}}},
  author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  date = {2009-01},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {1},
  pages = {61--80},
  issn = {1941-0093},
  doi = {10.1109/TNN.2008.2005605},
  abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {acyclic graph,Algorithms,Artificial Intelligence,Biological system modeling,Biology,Chemistry,computer vision,Computer vision,cyclic graph,Data engineering,data mining,Data mining,Databases Factual,directed graph,graph neural network model,graph neural networks (GNNs),graph processing,graph theory,Graphical domains,Internet,learning (artificial intelligence),Linear Models,m-dimensional Euclidean space,molecular biology,molecular chemistry,neural nets,Neural networks,Neural Networks (Computer),Nonlinear Dynamics,parameter estimation,Parameter estimation,pattern recognition,Pattern recognition,Pattern Recognition Automated,recursive neural networks,Regression Analysis,Reproducibility of Results,Supervised learning,supervised learning algorithm,undirected graph},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\KAUI9NFU\\Scarselli et al_2009_The Graph Neural Network Model.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\4B4LUSMK\\4700287.html}
}

@article{scheuermanHowComputersSee2019,
  title = {How {{Computers See Gender}}: {{An Evaluation}} of {{Gender Classification}} in {{Commercial Facial Analysis Services}}},
  shorttitle = {How {{Computers See Gender}}},
  author = {Scheuerman, Morgan Klaus and Paul, Jacob M. and Brubaker, Jed R.},
  date = {2019-11-07},
  journaltitle = {Proc. ACM Hum.-Comput. Interact.},
  volume = {3},
  pages = {1--33},
  issn = {2573-0142, 2573-0142},
  doi = {10.1145/3359246},
  url = {https://dl.acm.org/doi/10.1145/3359246},
  urldate = {2020-06-22},
  abstract = {MORGAN KLAUS SCHEUERMAN, JACOB M. PAUL, and JED R. BRUBAKER, University of Colorado Boulder Investigations of facial analysis (FA) technologies—such as facial detection and facial recognition—have been central to discussions about Artificial Intelligence’s (AI) impact on human beings. Research on automatic gender recognition, the classification of gender by FA technologies, has raised potential concerns around issues of racial and gender bias. In this study, we augment past work with empirical data by conducting a systematic analysis of how gender classification and gender labeling in computer vision services operate when faced with gender diversity. We sought to understand how gender is concretely conceptualized and encoded into commercial facial analysis and image labeling technologies available today. We then conducted a two-phrase study: (1) a system analysis of ten commercial FA and image labeling services and (2) an evaluation of five services using a custom dataset of diverse genders using self-labeled Instagram images. Our analysis highlights how gender is codified into both classifiers and data standards. We found that FA services performed consistently worse on transgender individuals and were universally unable to classify non-binary genders. In contrast, image labeling often presented multiple gendered concepts. We also found that user perceptions about gender performance and identity contradict the way gender performance is encoded into the computer vision infrastructure. We discuss our findings from three perspectives of gender identity (self-identity, gender performativity, and demographic identity) and how these perspectives interact across three layers: the classification infrastructure, the third-party applications that make use of that infrastructure, and the individuals who interact with that software. We employ Bowker and Star’s concepts of “torque” and “residuality” to further discuss the social implications of gender classification. We conclude by outlining opportunities for creating more inclusive classification infrastructures and datasets, as well as with implications for policy. CCS Concepts: • Social and professional topics → User characteristics; Gender.},
  issue = {CSCW},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\TPIGV8SN\Scheuerman et al. - 2019 - How Computers See Gender An Evaluation of Gender .pdf}
}

@article{scheuermanHowWeVe2020,
  title = {How {{We}}'ve {{Taught Algorithms}} to {{See Identity}}: {{Constructing Race}} and {{Gender}} in {{Image Databases}} for {{Facial Analysis}}},
  shorttitle = {How {{We}}'ve {{Taught Algorithms}} to {{See Identity}}},
  author = {Scheuerman, Morgan Klaus and Wade, Kandrea and Lustig, Caitlin and Brubaker, Jed R.},
  date = {2020-05-28},
  journaltitle = {Proc. ACM Hum.-Comput. Interact.},
  volume = {4},
  pages = {1--35},
  issn = {2573-0142, 2573-0142},
  doi = {10.1145/3392866},
  url = {https://dl.acm.org/doi/10.1145/3392866},
  urldate = {2020-06-22},
  issue = {CSCW1},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\7Q4TYUBK\Scheuerman et al. - 2020 - How We've Taught Algorithms to See Identity Const.pdf}
}

@article{schmidhuberDeepLearningNeural2015,
  title = {Deep Learning in Neural Networks: {{An}} Overview},
  shorttitle = {Deep Learning in Neural Networks},
  author = {Schmidhuber, Jürgen},
  date = {2015-01-01},
  journaltitle = {Neural Networks},
  volume = {61},
  pages = {85--117},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2014.09.003},
  url = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
  urldate = {2020-04-20},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  langid = {english},
  keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\T9VPM828\\Schmidhuber_2015_Deep learning in neural networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\98H9473X\\S0893608014002135.html}
}

@article{schmidlAnomalyDetectionTime2022,
  title = {Anomaly Detection in Time Series: A Comprehensive Evaluation},
  shorttitle = {Anomaly Detection in Time Series},
  author = {Schmidl, Sebastian and Wenig, Phillip and Papenbrock, Thorsten},
  date = {2022-07-27},
  journaltitle = {Proc. VLDB Endow.},
  volume = {15},
  number = {9},
  pages = {1779--1797},
  issn = {2150-8097},
  doi = {10.14778/3538598.3538602},
  url = {https://doi.org/10.14778/3538598.3538602},
  urldate = {2023-01-06},
  abstract = {Detecting anomalous subsequences in time series data is an important task in areas ranging from manufacturing processes over finance applications to health care monitoring. An anomaly can indicate important events, such as production faults, delivery bottlenecks, system defects, or heart flicker, and is therefore of central interest. Because time series are often large and exhibit complex patterns, data scientists have developed various specialized algorithms for the automatic detection of such anomalous patterns. The number and variety of anomaly detection algorithms has grown significantly in the past and, because many of these solutions have been developed independently and by different research communities, there is no comprehensive study that systematically evaluates and compares the different approaches. For this reason, choosing the best detection technique for a given anomaly detection task is a difficult challenge. This comprehensive, scientific study carefully evaluates most state-of-the-art anomaly detection algorithms. We collected and re-implemented 71 anomaly detection algorithms from different domains and evaluated them on 976 time series datasets. The algorithms have been selected from different algorithm families and detection approaches to represent the entire spectrum of anomaly detection techniques. In the paper, we provide a concise overview of the techniques and their commonalities; we evaluate their individual strengths and weaknesses and, thereby, consider factors, such as effectiveness, efficiency, and robustness. Our experimental results should ease the algorithm selection problem and open up new research directions.},
  file = {C:\Users\USEBPERP\Zotero\storage\6X35DEUG\Schmidl et al. - 2022 - Anomaly detection in time series a comprehensive .pdf}
}

@article{schmidtComparingMolecularPatterns2019,
  title = {Comparing {{Molecular Patterns Using}} the {{Example}} of {{SMARTS}}: {{Theory}} and {{Algorithms}}},
  shorttitle = {Comparing {{Molecular Patterns Using}} the {{Example}} of {{SMARTS}}},
  author = {Schmidt, Robert and Ehmki, Emanuel S. R. and Ohm, Farina and Ehrlich, Hans-Christian and Mashychev, Andriy and Rarey, Matthias},
  date = {2019-06-24},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {59},
  number = {6},
  pages = {2560--2571},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.9b00250},
  url = {https://doi.org/10.1021/acs.jcim.9b00250},
  urldate = {2021-05-19},
  abstract = {Molecular patterns are widely used for compound filtering in molecular design endeavors. They describe structural properties that are connected with unwanted physical or chemical properties like reactivity or toxicity. With filter sets comprising hundreds of structural filters, an analytic approach to compare those patterns is needed. Here we present a novel approach to solve the generic pattern comparison problem. We introduce chemically inspired fingerprints for pattern nodes and edges to derive an easy-to-compare pattern representation. On two annotated pattern graphs we apply a maximum common subgraph algorithm enabling the calculation of pattern inclusion and similarity. The resulting algorithm can be used in many different ways. We can automatically derive pattern hierarchies or search in large pattern collections for more general or more specific patterns. To the best of our knowledge, the presented algorithm is the first of its kind enabling these types of chemical pattern analytics. Our new tool named SMARTScompare is an implementation of the approach for the SMARTS language, which is the quasi-standard for structural filters. We demonstrate the capabilities of SMARTScompare on a large collection of SMARTS patterns from real applications.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JQZQUC5T\\Schmidt et al. - 2019 - Comparing Molecular Patterns Using the Example of .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\KJAGJWHI\\acs.jcim.html}
}

@article{schneiderBigDataPharmaceutical2016,
  title = {Big {{Data}} from {{Pharmaceutical Patents}}: {{A Computational Analysis}} of {{Medicinal Chemists}}’ {{Bread}} and {{Butter}}},
  shorttitle = {Big {{Data}} from {{Pharmaceutical Patents}}},
  author = {Schneider, Nadine and Lowe, Daniel M. and Sayle, Roger A. and Tarselli, Michael A. and Landrum, Gregory A.},
  date = {2016-05-12},
  journaltitle = {J. Med. Chem.},
  volume = {59},
  number = {9},
  pages = {4385--4402},
  issn = {0022-2623},
  doi = {10.1021/acs.jmedchem.6b00153},
  url = {https://doi.org/10.1021/acs.jmedchem.6b00153},
  urldate = {2019-09-27},
  abstract = {Multiple recent studies have focused on unraveling the content of the medicinal chemist’s toolbox. Here, we present an investigation of chemical reactions and molecules retrieved from U.S. patents over the past 40 years (1976–2015). We used a sophisticated text-mining pipeline to extract 1.15 million unique whole reaction schemes, including reaction roles and yields, from pharmaceutical patents. The reactions were assigned to well-known reaction types such as Wittig olefination or Buchwald–Hartwig amination using an expert system. Analyzing the evolution of reaction types over time, we observe the previously reported bias toward reaction classes like amide bond formations or Suzuki couplings. Our study also shows a steady increase in the number of different reaction types used in pharmaceutical patents but a trend toward lower median yield for some of the reaction classes. Finally, we found that today’s typical product molecule is larger, more hydrophobic, and more rigid than 40 years ago.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\K7Z26UKY\\Schneider et al_2016_Big Data from Pharmaceutical Patents.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7MTUCDF4\\acs.jmedchem.html}
}

@article{schneiderComputerbasedNovoDesign2005,
  title = {Computer-Based de Novo Design of Drug-like Molecules},
  author = {Schneider, Gisbert and Fechner, Uli},
  date = {2005-08},
  journaltitle = {Nat Rev Drug Discov},
  volume = {4},
  number = {8},
  pages = {649--663},
  publisher = {Nature Publishing Group},
  issn = {1474-1784},
  doi = {10.1038/nrd1799},
  url = {https://www.nature.com/articles/nrd1799},
  urldate = {2022-09-15},
  abstract = {Molecular de novo design, which involves incremental construction of a ligand model within a model of the receptor or enzyme active site, produces novel molecular structures with desired pharmacological properties from scratch.De novo molecule-design software is confronted with a virtually infinite search space. As such a large space prohibits exhaustive searching, navigation in the de novo design process relies on the principle of local optimization.Basically, three questions have to be addressed by a de novo design program: how to assemble the candidate compounds; how to evaluate their potential quality; and how to sample the search space effectively.This review gives an overview of computer-based molecular de novo design methods on a conceptual level, considering these three questions, and focusing on the design of small, drug-like molecules. Successful examples of de novo design in the hit- and lead-finding stages of the drug discovery process are used to show that de novo design provides a method for lead identification.De novo design can therefore be regarded as a complement to other virtual techniques, such as database searching, and non-virtual techniques such as high-throughput screening. We also accentuate strengths and weaknesses of current de novo design approaches.},
  issue = {8},
  langid = {english},
  keywords = {Biomedicine,Biotechnology,Cancer Research,general,Medicinal Chemistry,Molecular Medicine,Pharmacology/Toxicology},
  file = {C:\Users\USEBPERP\Zotero\storage\RFK8I8JV\nrd1799.html}
}

@article{schneiderDevelopmentNovelFingerprint2015,
  ids = {schneider2015developmenta,schneider2015developmentb,schneiderDevelopmentNovelFingerprint2015a},
  title = {Development of a {{Novel Fingerprint}} for {{Chemical Reactions}} and {{Its Application}} to {{Large-Scale Reaction Classification}} and {{Similarity}}},
  author = {Schneider, Nadine and Lowe, Daniel M. and Sayle, Roger A. and Landrum, Gregory A.},
  date = {2015-01-26},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {55},
  number = {1},
  pages = {39--53},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/ci5006614},
  url = {https://doi.org/10.1021/ci5006614},
  urldate = {2019-01-09},
  abstract = {Fingerprint methods applied to molecules have proven to be useful for similarity determination and as inputs to machine-learning models. Here, we present the development of a new fingerprint for chemical reactions and validate its usefulness in building machine-learning models and in similarity assessment. Our final fingerprint is constructed as the difference of the atom-pair fingerprints of products and reactants and includes agents via calculated physicochemical properties. We validated the fingerprints on a large data set of reactions text-mined from granted United States patents from the last 40 years that have been classified using a substructure-based expert system. We applied machine learning to build a 50-class predictive model for reaction-type classification that correctly predicts 97\% of the reactions in an external test set. Impressive accuracies were also observed when applying the classifier to reactions from an in-house electronic laboratory notebook. The performance of the novel fingerprint for assessing reaction similarity was evaluated by a cluster analysis that recovered 48 out of 50 of the reaction classes with a median F-score of 0.63 for the clusters. The data sets used for training and primary validation as well as all python scripts required to reproduce the analysis are provided in the Supporting Information.},
  issue = {1},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\47GLAECY\\Schneider et al_2015_Development of a Novel Fingerprint for Chemical Reactions and Its Application.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\IGCKS9JS\\Schneider et al. - 2015 - Development of a Novel Fingerprint for Chemical Re.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\35JFRFNI\\ci5006614.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\DST2522E\\ci5006614.html}
}

@book{schneiderMolecularDesignConcepts2008,
  title = {Molecular {{Design}}: {{Concepts}} and {{Applications}}},
  shorttitle = {Molecular {{Design}}},
  author = {Schneider, Gisbert and Baringhaus, Karl-Heinz},
  date = {2008-02-26},
  publisher = {John Wiley \& Sons},
  abstract = {This first introductory-level textbook on the design of small molecules is written with the first-time user in mind. Aimed at students and scientists alike, it uses computer-based methods to design and analyze such small molecules as drugs, enzyme inhibitors, probes and markers for biomolecules. Both authors have extensive practical experience of modeling and design and share their knowledge of what can and cannot be done with computer-assisted design. Divided into four sections, the book begins with a look at molecular objects and design objectives, including molecular geometry, properties, recognition and dynamics. Two further sections deal with virtual synthesis and screening, while the final section covers navigation in chemical space. The result is a textbook that takes the modeler one step further, to the de novo design of functional molecules. With its study questions at the end of each learning unit, this is equally suitable for teaching and self-learning.},
  isbn = {978-3-527-31432-4},
  langid = {english},
  pagetotal = {284},
  keywords = {Science / Chemistry / General,Science / Chemistry / Physical & Theoretical}
}

@article{schneiderNovoDesignEdge2016,
  title = {De {{Novo Design}} at the {{Edge}} of {{Chaos}}},
  author = {Schneider, Petra and Schneider, Gisbert},
  date = {2016-05-12},
  journaltitle = {J. Med. Chem.},
  volume = {59},
  number = {9},
  pages = {4077--4086},
  issn = {0022-2623},
  doi = {10.1021/acs.jmedchem.5b01849},
  url = {https://doi.org/10.1021/acs.jmedchem.5b01849},
  urldate = {2018-02-08},
  abstract = {Computational medicinal chemistry offers viable strategies for finding, characterizing, and optimizing innovative pharmacologically active compounds. Technological advances in both computer hardware and software as well as biological chemistry have enabled a renaissance of computer-assisted “de novo” design of molecules with desired pharmacological properties. Here, we present our current perspective on the concept of automated molecule generation by highlighting chemocentric methods that may capture druglike chemical space, consider ligand promiscuity for hit and lead finding, and provide fresh ideas for the rational design of customized screening of compound libraries.},
  file = {C:\Users\USEBPERP\Zotero\storage\663NZKUU\Schneider_Schneider_2016_De Novo Design at the Edge of Chaos.pdf}
}

@book{schneiderNovoMolecularDesign2013,
  title = {De Novo {{Molecular Design}}},
  author = {Schneider, Gisbert},
  date = {2013},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9783527677016},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9783527677016.fmatter},
  urldate = {2020-04-20},
  abstract = {The prelims comprise: Common generic scaffolds of synthetic bioactive compounds Half-Title Page Related Titles Title Page Copyright Page Table of Contents List of Contributors Foreword Preface},
  isbn = {978-3-527-67701-6},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JQ6PFG4U\\2013_Front Matter.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\APRNNWPU\\9783527677016.html}
}

@article{schneiderWhatWhatNearly2016,
  ids = {schneider2016whata,schneiderWhatWhatNearly2016a},
  title = {What’s {{What}}: {{The}} ({{Nearly}}) {{Definitive Guide}} to {{Reaction Role Assignment}}},
  shorttitle = {What’s {{What}}},
  author = {Schneider, Nadine and Stiefl, Nikolaus and Landrum, Gregory A.},
  date = {2016-12-27},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {56},
  number = {12},
  pages = {2336--2346},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.6b00564},
  url = {https://doi.org/10.1021/acs.jcim.6b00564},
  urldate = {2019-01-16},
  abstract = {When analyzing chemical reactions it is essential to know which molecules are actively involved in the reaction and which educts will form the product molecules. Assigning reaction roles, like reactant, reagent, or product, to the molecules of a chemical reaction might be a trivial problem for hand-curated reaction schemes but it is more difficult to automate, an essential step when handling large amounts of reaction data. Here, we describe a new fingerprint-based and data-driven approach to assign reaction roles which is also applicable to rather unbalanced and noisy reaction schemes. Given a set of molecules involved and knowing the product(s) of a reaction we assign the most probable reactants and sort out the remaining reagents. Our approach was validated using two different data sets: one hand-curated data set comprising about 680 diverse reactions extracted from patents which span more than 200 different reaction types and include up to 18 different reactants. A second set consists of 50\,000 randomly picked reactions from US patents. The results of the second data set were compared to results obtained using two different atom-to-atom mapping algorithms. For both data sets our method assigns the reaction roles correctly for the vast majority of the reactions, achieving an accuracy of 88\% and 97\% respectively. The median time needed, about 8 ms, indicates that the algorithm is fast enough to be applied to large collections. The new method is available as part of the RDKit toolkit and the data sets and Jupyter notebooks used for evaluation of the new method are available in the Supporting Information of this publication.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\GSGLTHMG\\Schneider et al. - 2016 - What’s What The (Nearly) Definitive Guide to Reac.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\NYHCDTAU\\Schneider et al_2016_What’s What.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TWZP8HQ4\\Schneider et al_2016_What’s What2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VIEFPXVQ\\Schneider et al. - 2016 - What’s What The (Nearly) Definitive Guide to Reac.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FKZ8JVLX\\acs.jcim.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\M3WIHFWU\\acs.jcim.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\N5FPRT6E\\acs.jcim.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\U9THEIWJ\\acs.jcim.html}
}

@unpublished{schoelkopfCausalAnticausalLearning2012,
  title = {On {{Causal}} and {{Anticausal Learning}}},
  author = {Schoelkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
  date = {2012-06-27},
  eprint = {1206.6471},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1206.6471},
  urldate = {2019-07-01},
  abstract = {We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\QZ7MGKSI\\Schoelkopf et al_2012_On Causal and Anticausal Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\75MEGV7F\\1206.html}
}

@online{scholkopfEstimatingSupportHighDimensional2006,
  type = {research-article},
  title = {Estimating the {{Support}} of a {{High-Dimensional Distribution}}},
  author = {Schölkopf, Bernhard and Platt, John C. and Shawe-Taylor, John and Smola, Alex J. and Williamson, Robert C.},
  date = {2006-03-13},
  doi = {10.1162/089976601750264965},
  url = {https://www.mitpressjournals.org/doix/abs/10.1162/089976601750264965},
  urldate = {2019-07-08},
  abstract = {Estimating the Support of a High-Dimensional Distribution},
  langid = {english},
  organization = {http://dx.doi.org/10.1162/089976601750264965},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\CJJ373AA\\Schölkopf et al_2006_Estimating the Support of a High-Dimensional Distribution.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HJHLT2CC\\089976601750264965.html}
}

@unpublished{schreckLearningRetrosyntheticPlanning2019,
  title = {Learning Retrosynthetic Planning through Self-Play},
  author = {Schreck, John S. and Coley, Connor W. and Bishop, Kyle J. M.},
  date = {2019-01-19},
  eprint = {1901.06569},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1901.06569},
  urldate = {2019-02-07},
  abstract = {The problem of retrosynthetic planning can be framed as one player game, in which the chemist (or a computer program) works backwards from a molecular target to simpler starting materials though a series of choices regarding which reactions to perform. This game is challenging as the combinatorial space of possible choices is astronomical, and the value of each choice remains uncertain until the synthesis plan is completed and its cost evaluated. Here, we address this problem using deep reinforcement learning to identify policies that make (near) optimal reaction choices during each step of retrosynthetic planning. Using simulated experience or self-play, we train neural networks to estimate the expected synthesis cost or value of any given molecule based on a representation of its molecular structure. We show that learned policies based on this value network outperform heuristic approaches in synthesizing unfamiliar molecules from available starting materials using the fewest number of reactions. We discuss how the learned policies described here can be incorporated into existing synthesis planning tools and how they can be adapted to changes in the synthesis cost objective or material availability.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\N45WCG29\\Schreck et al_2019_Learning retrosynthetic planning through self-play.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\5D96MJEA\\1901.html}
}

@article{schreckLearningRetrosyntheticPlanning2019a,
  title = {Learning {{Retrosynthetic Planning}} through {{Simulated Experience}}},
  author = {Schreck, John S. and Coley, Connor W. and Bishop, Kyle J. M.},
  date = {2019-05-31},
  journaltitle = {ACS Cent. Sci.},
  issn = {2374-7943},
  doi = {10.1021/acscentsci.9b00055},
  url = {https://doi.org/10.1021/acscentsci.9b00055},
  urldate = {2019-06-04},
  abstract = {The problem of retrosynthetic planning can be framed as a one-player game, in which the chemist (or a computer program) works backward from a molecular target to simpler starting materials through a series of choices regarding which reactions to perform. This game is challenging as the combinatorial space of possible choices is astronomical, and the value of each choice remains uncertain until the synthesis plan is completed and its cost evaluated. Here, we address this search problem using deep reinforcement learning to identify policies that make (near) optimal reaction choices during each step of retrosynthetic planning according to a user-defined cost metric. Using a simulated experience, we train a neural network to estimate the expected synthesis cost or value of any given molecule based on a representation of its molecular structure. We show that learned policies based on this value network can outperform a heuristic approach that favors symmetric disconnections when synthesizing unfamiliar molecules from available starting materials using the fewest number of reactions. We discuss how the learned policies described here can be incorporated into existing synthesis planning tools and how they can be adapted to changes in the synthesis cost objective or material availability.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4JJKMNF5\\Schreck et al. - 2019 - Learning Retrosynthetic Planning through Simulated.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6DCX99ZT\\Schreck et al. - 2019 - Learning Retrosynthetic Planning through Simulated.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LR8TPC47\\Schreck et al_2019_Learning Retrosynthetic Planning through Simulated Experience.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\P5DPR8QT\\Schreck et al_2019_Learning Retrosynthetic Planning through Simulated Experience2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7FHHPHE7\\acscentsci.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\HS7YES39\\acscentsci.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\U2LDTXNK\\acscentsci.html}
}

@online{schulmanProximalPolicyOptimization2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  date = {2017-08-28},
  eprint = {1707.06347},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1707.06347},
  url = {http://arxiv.org/abs/1707.06347},
  urldate = {2023-11-08},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\UBGGVNI2\\Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\CD9Y28BT\\1707.html}
}

@book{schuttMachineLearningMeets2020,
  title = {Machine {{Learning Meets Quantum Physics}}},
  editor = {Schütt, Kristof T. and Chmiela, Stefan and family=Lilienfeld, given=O. Anatole, prefix=von, useprefix=true and Tkatchenko, Alexandre and Tsuda, Koji and Müller, Klaus-Robert},
  date = {2020},
  series = {Lecture {{Notes}} in {{Physics}}},
  volume = {968},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-40245-7},
  url = {http://link.springer.com/10.1007/978-3-030-40245-7},
  urldate = {2020-08-13},
  isbn = {978-3-030-40244-0 978-3-030-40245-7},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\VPS5RKEU\Schütt et al_2020_Machine Learning Meets Quantum Physics.pdf}
}

@article{schwallerFoundTranslationPredicting2018,
  ids = {schwaller2018founda,schwallerFoundTranslationPredicting2018a},
  title = {“{{Found}} in {{Translation}}”: Predicting Outcomes of Complex Organic Chemistry Reactions Using Neural Sequence-to-Sequence Models},
  shorttitle = {“{{Found}} in {{Translation}}”},
  author = {Schwaller, Philippe and Gaudin, Théophile and Lányi, Dávid and Bekas, Costas and Laino, Teodoro},
  date = {2018},
  journaltitle = {Chemical Science},
  volume = {9},
  number = {28},
  pages = {6091--6098},
  publisher = {Royal Society of Chemistry},
  doi = {10.1039/C8SC02339E},
  url = {https://pubs.rsc.org/en/content/articlelanding/2018/sc/c8sc02339e},
  urldate = {2019-01-07},
  issue = {28},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\PXSTSQ8B\\Schwaller et al. - 2018 - “Found in Translation” predicting outcomes of com.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\W8T2XV5D\\Schwaller et al_2018_“Found in Translation”.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\3WTPE29S\\c8sc02339e.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\SFLPG2NQ\\c8sc02339e.html}
}

@article{schwallerMappingSpaceChemical2020,
  title = {Mapping the {{Space}} of {{Chemical Reactions}} Using {{Attention-Based Neural Networks}}},
  author = {Schwaller, Philippe and Probst, Daniel and Vaucher, Alain C. and Nair, Vishnu H. and Kreutter, David and Laino, Teodoro and Reymond, Jean-Louis},
  date = {2020-12-11},
  publisher = {ChemRxiv},
  doi = {10.26434/chemrxiv.9897365.v4},
  url = {/articles/preprint/Data-Driven_Chemical_Reaction_Classification_with_Attention-Based_Neural_Networks/9897365/4},
  urldate = {2021-02-10},
  abstract = {Organic reactions are usually assigned to classes grouping reactions with similar reagents and mechanisms. Reaction classes facilitate communication of complex concepts and efficient navigation through chemical reaction space. However, the classification process is a tedious task, requiring the identification of the corresponding reaction class template via annotation of the number of molecules in the reactions, the reaction center and the distinction between reactants and reagents. In this work, we show that transformer-based models can infer reaction classes from non-annotated, simple text-based representations of chemical reactions. Our best model reaches a classification accuracy of 98.2\%. We also show that the learned representations can be used as reaction fingerprints which capture fine-grained differences between reaction classes better than traditional reaction fingerprints. The unprecedented insights into chemical reaction space enabled by our learned fingerprints is illustrated by an interactive reaction atlas providing visual clustering and similarity searching. Code: https://github.com/rxn4chemistry/rxnfpTutorials: https://rxn4chemistry.github.io/rxnfp/Interactive reaction atlas: https://rxn4chemistry.github.io/rxnfp//tmaps/tmap\_ft\_10k.html},
  langid = {english}
}

@unpublished{schwallerMolecularTransformerChemical2018,
  title = {Molecular {{Transformer}} for {{Chemical Reaction Prediction}} and {{Uncertainty Estimation}}},
  author = {Schwaller, Philippe and Laino, Teodoro and Gaudin, Théophile and Bolgar, Peter and Bekas, Costas and Lee, Alpha A.},
  date = {2018-11-06},
  eprint = {1811.02633},
  eprinttype = {arxiv},
  eprintclass = {physics},
  url = {http://arxiv.org/abs/1811.02633},
  urldate = {2019-01-17},
  abstract = {Organic synthesis is one of the key stumbling blocks in medicinal chemistry. A necessary yet unsolved step in planning synthesis is solving the forward problem: given reactants and reagents, predict the products. Similar to other works, we treat reaction prediction as a machine translation problem between SMILES strings of reactants-reagents and the products. We show that a multi-head attention Molecular Transformer model outperforms all algorithms in the literature, achieving a top-1 accuracy above 90\% on a common benchmark dataset. Our algorithm requires no handcrafted rules, and accurately predicts subtle chemical transformations. Crucially, our model can accurately estimate its own uncertainty, with an uncertainty score that is 89\% accurate in terms of classifying whether a prediction is correct. Furthermore, we show that the model is able to handle inputs without reactant-reagent split and including stereochemistry, which makes our method universally applicable.},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LEL9A4R2\\Schwaller et al_2018_Molecular Transformer for Chemical Reaction Prediction and Uncertainty.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\A5I7KXY8\\1811.html}
}

@article{schwallerMolecularTransformerModel2019,
  ids = {schwaller2019moleculara,schwallerMolecularTransformerModel2019a},
  title = {Molecular {{Transformer}}: {{A Model}} for {{Uncertainty-Calibrated Chemical Reaction Prediction}}},
  shorttitle = {Molecular {{Transformer}}},
  author = {Schwaller, Philippe and Laino, Teodoro and Gaudin, Théophile and Bolgar, Peter and Hunter, Christopher A. and Bekas, Costas and Lee, Alpha A.},
  date = {2019-09-25},
  journaltitle = {ACS Cent. Sci.},
  volume = {5},
  number = {9},
  pages = {1572--1583},
  publisher = {American Chemical Society},
  issn = {2374-7943},
  doi = {10.1021/acscentsci.9b00576},
  url = {https://doi.org/10.1021/acscentsci.9b00576},
  urldate = {2019-10-02},
  abstract = {Organic synthesis is one of the key stumbling blocks in medicinal chemistry. A necessary yet unsolved step in planning synthesis is solving the forward problem: Given reactants and reagents, predict the products. Similar to other work, we treat reaction prediction as a machine translation problem between simplified molecular-input line-entry system (SMILES) strings (a text-based representation) of reactants, reagents, and the products. We show that a multihead attention Molecular Transformer model outperforms all algorithms in the literature, achieving a top-1 accuracy above 90\% on a common benchmark data set. Molecular Transformer makes predictions by inferring the correlations between the presence and absence of chemical motifs in the reactant, reagent, and product present in the data set. Our model requires no handcrafted rules and accurately predicts subtle chemical transformations. Crucially, our model can accurately estimate its own uncertainty, with an uncertainty score that is 89\% accurate in terms of classifying whether a prediction is correct. Furthermore, we show that the model is able to handle inputs without a reactant–reagent split and including stereochemistry, which makes our method universally applicable.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4EGDQMZU\\Schwaller et al_2019_Molecular Transformer.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\G3SMDQFI\\Schwaller et al. - 2019 - Molecular Transformer A Model for Uncertainty-Cal.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\L4534W2M\\acscentsci.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\ME49MVQT\\acscentsci.html}
}

@inreference{SciencePressConference2020,
  title = {Science by Press Conference},
  booktitle = {Wikipedia},
  date = {2020-08-04T06:36:14Z},
  url = {https://en.wikipedia.org/w/index.php?title=Science_by_press_conference&oldid=971109571},
  urldate = {2020-11-22},
  abstract = {Science by press conference (or science by press release) is the practice by which scientists put an unusual focus on publicizing results of research in the media. The term is usually used disparagingly. It is intended to associate the target with people promoting scientific "findings" of questionable scientific merit who turn to the media for attention when they are unlikely to win the approval of the professional scientific community. Premature publicity violates a cultural value of most of the scientific community, which is that findings should be subjected to independent review with a "thorough examination by the scientific community" before they are widely publicized. The standard practice is to publish a paper in a peer-reviewed scientific journal. This idea has many merits, including that the scientific community has a responsibility to conduct itself in a deliberative, non-attention seeking way; and that its members should be oriented more towards the pursuit of insight than fame. Science by press conference in its most egregious forms can be undertaken on behalf of an individual researcher seeking fame, a corporation seeking to sway public opinion or investor perception, or a political or ideological movement.},
  langid = {english},
  annotation = {Page Version ID: 971109571},
  file = {C:\Users\USEBPERP\Zotero\storage\Z5T7VMAJ\index.html}
}

@article{seglerGeneratingFocusedMolecule2018,
  title = {Generating {{Focused Molecule Libraries}} for {{Drug Discovery}} with {{Recurrent Neural Networks}}},
  author = {Segler, Marwin H. S. and Kogej, Thierry and Tyrchan, Christian and Waller, Mark P.},
  date = {2018-01-24},
  journaltitle = {ACS Cent. Sci.},
  volume = {4},
  number = {1},
  pages = {120--131},
  publisher = {American Chemical Society},
  issn = {2374-7943},
  doi = {10.1021/acscentsci.7b00512},
  url = {https://doi.org/10.1021/acscentsci.7b00512},
  urldate = {2020-04-20},
  abstract = {In de novo drug design, computational strategies are used to generate novel molecules with good affinity to the desired biological target. In this work, we show that recurrent neural networks can be trained as generative models for molecular structures, similar to statistical language models in natural language processing. We demonstrate that the properties of the generated molecules correlate very well with the properties of the molecules used to train the model. In order to enrich libraries with molecules active toward a given biological target, we propose to fine-tune the model with small sets of molecules, which are known to be active against that target. Against Staphylococcus aureus, the model reproduced 14\% of 6051 hold-out test molecules that medicinal chemists designed, whereas against Plasmodium falciparum (Malaria), it reproduced 28\% of 1240 test molecules. When coupled with a scoring function, our model can perform the complete de novo drug design cycle to generate large sets of novel molecules for drug discovery.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\46NXJX4M\\Segler et al_2018_Generating Focused Molecule Libraries for Drug Discovery with Recurrent Neural.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AJ554YPQ\\acscentsci.html}
}

@unpublished{seglerGeneratingFocussedMolecule2017,
  title = {Generating {{Focussed Molecule Libraries}} for {{Drug Discovery}} with {{Recurrent Neural Networks}}},
  author = {Segler, Marwin H. S. and Kogej, Thierry and Tyrchan, Christian and Waller, Mark P.},
  date = {2017-01-05},
  eprint = {1701.01329},
  eprinttype = {arxiv},
  eprintclass = {physics, stat},
  url = {http://arxiv.org/abs/1701.01329},
  urldate = {2018-09-17},
  abstract = {In de novo drug design, computational strategies are used to generate novel molecules with good affinity to the desired biological target. In this work, we show that recurrent neural networks can be trained as generative models for molecular structures, similar to statistical language models in natural language processing. We demonstrate that the properties of the generated molecules correlate very well with the properties of the molecules used to train the model. In order to enrich libraries with molecules active towards a given biological target, we propose to fine-tune the model with small sets of molecules, which are known to be active against that target. Against Staphylococcus aureus, the model reproduced 14\% of 6051 hold-out test molecules that medicinal chemists designed, whereas against Plasmodium falciparum (Malaria) it reproduced 28\% of 1240 test molecules. When coupled with a scoring function, our model can perform the complete de novo drug design cycle to generate large sets of novel molecules for drug discovery.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Learning,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Physics - Chemical Physics,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\9KD8YFXR\\Segler et al_2017_Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WHQMJSPR\\Segler et al_2017_Generating Focussed Molecule Libraries for Drug Discovery with Recurrent Neural2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2FVNUHVJ\\1701.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\C9JRV2IH\\1701.html}
}

@article{seglerNeuralSymbolicMachineLearning2017,
  ids = {segler2017neuralsymbolica,seglerNeuralSymbolicMachineLearning2017a},
  title = {Neural-{{Symbolic Machine Learning}} for {{Retrosynthesis}} and {{Reaction Prediction}}},
  author = {Segler, Marwin H. S. and Waller, Mark P.},
  date = {2017},
  journaltitle = {Chemistry},
  volume = {23},
  number = {25},
  pages = {5966--5971},
  issn = {1521-3765},
  doi = {10.1002/chem.201605499},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/chem.201605499},
  urldate = {2019-01-09},
  abstract = {Reaction prediction and retrosynthesis are the cornerstones of organic chemistry. Rule-based expert systems have been the most widespread approach to computationally solve these two related challenges to date. However, reaction rules often fail because they ignore the molecular context, which leads to reactivity conflicts. Herein, we report that deep neural networks can learn to resolve reactivity conflicts and to prioritize the most suitable transformation rules. We show that by training our model on 3.5 million reactions taken from the collective published knowledge of the entire discipline of chemistry, our model exhibits a top10-accuracy of 95 \% in retrosynthesis and 97 \% for reaction prediction on a validation set of almost 1 million reactions.},
  issue = {25},
  langid = {english},
  keywords = {artificial intelligence,machine learning,retrosynthesis,synthesis design,total synthesis},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BUJ5T2LE\\Segler and Waller - 2017 - Neural-Symbolic Machine Learning for Retrosynthesi.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\MSN9JINC\\Segler_Waller_2017_Neural-Symbolic Machine Learning for Retrosynthesis and Reaction Prediction.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\E6JGEHPN\\chem.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\G39XHFZR\\chem.html}
}

@article{seglerPlanningChemicalSyntheses2018,
  ids = {segler2018learning,segler2018planninga},
  title = {Planning Chemical Syntheses with Deep Neural Networks and Symbolic {{AI}}},
  author = {Segler, Marwin H. S. and Preuss, Mike and Waller, Mark P.},
  date = {2018-03},
  journaltitle = {Nature},
  volume = {555},
  number = {7698},
  eprint = {1708.04202},
  eprinttype = {arxiv},
  pages = {604--610},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature25978},
  url = {https://www.nature.com/articles/nature25978},
  urldate = {2019-01-09},
  abstract = {To plan the syntheses of small organic molecules, chemists use retrosynthesis, a problem-solving technique in which target molecules are recursively transformed into increasingly simpler precursors. Computer-aided retrosynthesis would be a valuable tool but at present it is slow and provides results of unsatisfactory quality. Here we use Monte Carlo tree search and symbolic artificial intelligence (AI) to discover retrosynthetic routes. We combined Monte Carlo tree search with an expansion policy network that guides the search, and a filter network to pre-select the most promising retrosynthetic steps. These deep neural networks were trained on essentially all reactions ever published in organic chemistry. Our system solves for almost twice as many molecules, thirty times faster than the traditional computer-aided search method, which is based on extracted rules and hand-designed heuristics. In a double-blind AB test, chemists on average considered our computer-generated routes to be equivalent to reported literature routes.},
  issue = {7698},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Physics - Chemical Physics},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4JHAB9CY\\Segler et al_2018_Planning chemical syntheses with deep neural networks and symbolic AI2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BW7E522P\\Segler et al. - 2018 - Planning chemical syntheses with deep neural netwo.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\G23ACIBC\\nature25978.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\IAGGRTGD\\nature25978.html}
}

@unpublished{seglerWorldProgramsModelBased2019,
  title = {World {{Programs}} for {{Model-Based Learning}} and {{Planning}} in {{Compositional State}} and {{Action Spaces}}},
  author = {Segler, Marwin H. S.},
  date = {2019-12-30},
  eprint = {1912.13007},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.13007},
  urldate = {2020-04-01},
  abstract = {Some of the most important tasks take place in environments which lack cheap and perfect simulators, thus hampering the application of model-free reinforcement learning (RL). While model-based RL aims to learn a dynamics model, in a more general case the learner does not know a priori what the action space is. Here we propose a formalism where the learner induces a world program by learning a dynamics model and the actions in graph-based compositional environments by observing state-state transition examples. Then, the learner can perform RL with the world program as the simulator for complex planning tasks. We highlight a recent application, and propose a challenge for the community to assess world program-based planning.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\T7MBHCHR\\Segler_2019_World Programs for Model-Based Learning and Planning in Compositional State and.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\G2X8TUB8\\1912.html}
}

@article{seidlImprovingFewZeroShot2022,
  title = {Improving {{Few-}} and {{Zero-Shot Reaction Template Prediction Using Modern Hopfield Networks}}},
  author = {Seidl, Philipp and Renz, Philipp and Dyubankova, Natalia and Neves, Paulo and Verhoeven, Jonas and Wegner, Jörg K. and Segler, Marwin and Hochreiter, Sepp and Klambauer, Günter},
  date = {2022-05-09},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {62},
  number = {9},
  pages = {2111--2120},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.1c01065},
  url = {https://doi.org/10.1021/acs.jcim.1c01065},
  urldate = {2022-09-16},
  abstract = {Finding synthesis routes for molecules of interest is essential in the discovery of new drugs and materials. To find such routes, computer-assisted synthesis planning (CASP) methods are employed, which rely on a single-step model of chemical reactivity. In this study, we introduce a template-based single-step retrosynthesis model based on Modern Hopfield Networks, which learn an encoding of both molecules and reaction templates in order to predict the relevance of templates for a given molecule. The template representation allows generalization across different reactions and significantly improves the performance of template relevance prediction, especially for templates with few or zero training examples. With inference speed up to orders of magnitude faster than baseline methods, we improve or match the state-of-the-art performance for top-k exact match accuracy for k ≥ 3 in the retrosynthesis benchmark USPTO-50k. Code to reproduce the results is available at github.com/ml-jku/mhn-react.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\IHUPKZL3\\Seidl et al. - 2022 - Improving Few- and Zero-Shot Reaction Template Pre.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TUBFSKAA\\acs.jcim.html}
}

@unpublished{seidlModernHopfieldNetworks2021,
  title = {Modern {{Hopfield Networks}} for {{Few-}} and {{Zero-Shot Reaction Prediction}}},
  author = {Seidl, Philipp and Renz, Philipp and Dyubankova, Natalia and Neves, Paulo and Verhoeven, Jonas and Wegner, Jörg K. and Hochreiter, Sepp and Klambauer, Günter},
  date = {2021-04-07},
  eprint = {2104.03279},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio, stat},
  url = {http://arxiv.org/abs/2104.03279},
  urldate = {2021-04-13},
  abstract = {An essential step in the discovery of new drugs and materials is the synthesis of a molecule that exists so far only as an idea to test its biological and physical properties. While computer-aided design of virtual molecules has made large progress, computer-assisted synthesis planning (CASP) to realize physical molecules is still in its infancy and lacks a performance level that would enable large-scale molecule discovery. CASP supports the search for multi-step synthesis routes, which is very challenging due to high branching factors in each synthesis step and the hidden rules that govern the reactions. The central and repeatedly applied step in CASP is reaction prediction, for which machine learning methods yield the best performance. We propose a novel reaction prediction approach that uses a deep learning architecture with modern Hopfield networks (MHNs) that is optimized by contrastive learning. An MHN is an associative memory that can store and retrieve chemical reactions in each layer of a deep learning architecture. We show that our MHN contrastive learning approach enables few- and zero-shot learning for reaction prediction which, in contrast to previous methods, can deal with rare, single, or even no training example(s) for a reaction. On a well established benchmark, our MHN approach pushes the state-of-the-art performance up by a large margin as it improves the predictive top-100 accuracy from \$0.858\textbackslash pm0.004\$ to \$0.959\textbackslash pm0.004\$. This advance might pave the way to large-scale molecule discovery.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Statistics - Machine Learning}
}

@unpublished{seidlModernHopfieldNetworks2021a,
  title = {Modern {{Hopfield Networks}} for {{Few-}} and {{Zero-Shot Reaction Template Prediction}}},
  author = {Seidl, Philipp and Renz, Philipp and Dyubankova, Natalia and Neves, Paulo and Verhoeven, Jonas and Segler, Marwin and Wegner, Jörg K. and Hochreiter, Sepp and Klambauer, Günter},
  date = {2021-06-15},
  eprint = {2104.03279},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio, stat},
  url = {http://arxiv.org/abs/2104.03279},
  urldate = {2021-08-24},
  abstract = {Finding synthesis routes for molecules of interest is an essential step in the discovery of new drugs and materials. To find such routes, computer-assisted synthesis planning (CASP) methods are employed which rely on a model of chemical reactivity. In this study, we model single-step retrosynthesis in a template-based approach using modern Hopfield networks (MHNs). We adapt MHNs to associate different modalities, reaction templates and molecules, which allows the model to leverage structural information about reaction templates. This approach significantly improves the performance of template relevance prediction, especially for templates with few or zero training examples. With inference speed several times faster than that of baseline methods, we improve predictive performance for top-k exact match accuracy for \$\textbackslash mathrm\{k\}\textbackslash geq5\$ in the retrosynthesis benchmark USPTO-50k.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\IYAXPQTG\\Seidl et al. - 2021 - Modern Hopfield Networks for Few- and Zero-Shot Re.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LPYKWTQC\\2104.html}
}

@article{sellwoodArtificialIntelligenceDrug2018,
  title = {Artificial Intelligence in Drug Discovery},
  author = {Sellwood, Matthew A and Ahmed, Mohamed and Segler, Marwin HS and Brown, Nathan},
  date = {2018-08-13},
  journaltitle = {Future Medicinal Chemistry},
  volume = {10},
  number = {17},
  pages = {2025--2028},
  issn = {1756-8919},
  doi = {10.4155/fmc-2018-0212},
  url = {https://www.future-science.com/doi/full/10.4155/fmc-2018-0212},
  urldate = {2019-10-02},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\47JQH98J\\Sellwood et al. - 2018 - Artificial intelligence in drug discovery.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\X9C9NIQ3\\Sellwood et al_2018_Artificial intelligence in drug discovery.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DETAAZJW\\fmc-2018-0212.html}
}

@incollection{seneciChemicalDiversityDriving2002,
  title = {Chemical Diversity as a Driving Force to Design and Put in Practice Synthetic Strategies Leading to Combinatorial Libraries for Lead Discovery and Lead Optimization},
  booktitle = {Pharmacochemistry {{Library}}},
  author = {Seneci, Pierfausto},
  editor = {family=Goot, given=Henk, prefix=van der, useprefix=true},
  date = {2002-01-01},
  series = {Trends in {{Drug Research III}}},
  volume = {32},
  pages = {147--160},
  publisher = {Elsevier},
  doi = {10.1016/S0165-7208(02)80016-2},
  url = {https://www.sciencedirect.com/science/article/pii/S0165720802800162},
  urldate = {2022-07-04},
  abstract = {This chapter discusses the chemical diversity as a driving force to design and put in practice synthetic strategies leading to combinatorial libraries for lead discovery and lead optimization. Target identification and target validation are now crucial milestones, as the unraveling of the human genome is providing thousands of new, uncharacterized genes as potential targets for the cure of important diseases. Research laboratories able to identify and validate targets better and faster than competitors will be significantly advantaged, and combinatorial approaches and tools will provide relevant benefits at this stage. The full potential of chemical diversity and combinatorial libraries is evident in the following three steps of the process, that is, (1) from target to hit; (2) from hit to lead; and (3) from lead to clinical candidate. Once a target is validated, the hit identification phase kicks off and chemical disciplines start to be heavily involved. While biologists develop a suitable biological assay to identify compounds interacting with the target, chemists will assemble a suitable chemical collection (diversity), which is screened on the biological assay (screening) eventually leading to active compounds, or hits (structure determination). If the target is novel and poorly characterized, chemical diversity is the main driver to assemble a screening collection. A collection spanning the so-called diversity space is more likely to identify an active compound, or hit, than a collection that is clustered in an area of the same space.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\CF2HRH9C\S0165720802800162.html}
}

@online{SeniorAppliedScience,
  title = {Senior {{Applied Science Manager}}, {{Prime Video Automated Excellence}}},
  url = {https://www.amazon.jobs/en/jobs/2158372/senior-applied-science-manager-prime-video-automated-excellence},
  urldate = {2022-08-31},
  abstract = {Job summaryThe Automated Excellence team leverage applied science to protect each phase of the Software Development Lifecycle with intelligent automation, from writing code through to monitoring live production systems - transforming Prime Video’s software engineers into superhuman developers. We need to apply intelligent automation because the distribution of Prime Video’s application is so broad and so fragmented. The app is installed on ‘Living Room’ devices around the globe - such as TVs, consoles, and set-top-boxes – which together comprise hundreds of distinct device types. The app is also diverse in feature breadth (comprising video on demand, linear TV, and live events), and diverse in offer type (supporting subscription-based, pay per view, advertising-supported, and third party channel models). This functional and technical fragmentation drives combinatorial scale which exceeds the limits of standard software development toolchains. For example, it is not possible for teams to consistently and reliably audit the alarm thresholds for thousands of distinct system metrics across so many different dimensions, but this is something we can train models to do. We are exploring a range of research areas in our mission to turn Prime Video’s software engineers into superhuman developers. These include automated reasoning with formal methods, forecasting and anomaly detection with machine learning (ML), and robotic process automation (RPA) using AI and Computer Vision.Key job responsibilities• Own the roadmap for Prime Video’s Automated Excellence team, making the right long-term and short-term trade offs to deliver the most impact we can for our customers.• Develop the team, owning the organizational structure, the hiring and development of science and engineering leaders, and attracting and retaining top scientists and engineers. • Collaborate with researchers within Amazon and within the wider scientific community, publishing novel results in peer-reviewed journals and conferences, and supporting academic institutions, events, journals, and tracks.},
  langid = {english},
  organization = {amazon.jobs},
  file = {C:\Users\USEBPERP\Zotero\storage\6WKEFUM5\senior-applied-science-manager-prime-video-automated-excellence.html}
}

@unpublished{shafaeiDoesYourModel2018,
  title = {Does {{Your Model Know}} the {{Digit}} 6 {{Is Not}} a {{Cat}}? {{A Less Biased Evaluation}} of "{{Outlier}}" {{Detectors}}},
  shorttitle = {Does {{Your Model Know}} the {{Digit}} 6 {{Is Not}} a {{Cat}}?},
  author = {Shafaei, Alireza and Schmidt, Mark and Little, James J.},
  date = {2018-09-12},
  eprint = {1809.04729},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1809.04729},
  urldate = {2019-05-02},
  abstract = {In the real world, a learning system could receive an input that looks nothing like anything it has seen during training, and this can lead to unpredictable behaviour. We thus need to know whether any given input belongs to the population distribution of the training data to prevent unpredictable behaviour in deployed systems. A recent surge of interest on this problem has led to the development of sophisticated techniques in the deep learning literature. However, due to the absence of a standardized problem formulation or an exhaustive evaluation, it is not evident if we can rely on these methods in practice. What makes this problem different from a typical supervised learning setting is that we cannot model the diversity of out-of-distribution samples in practice. The distribution of outliers used in training may not be the same as the distribution of outliers encountered in the application. Therefore, classical approaches that learn inliers vs. outliers with only two datasets can yield optimistic results. We introduce OD-test, a three-dataset evaluation scheme as a practical and more reliable strategy to assess progress on this problem. The OD-test benchmark provides a straightforward means of comparison for methods that address the out-of-distribution sample detection problem. We present an exhaustive evaluation of a broad set of methods from related areas on image classification tasks. Furthermore, we show that for realistic applications of high-dimensional images, the existing methods have low accuracy. Our analysis reveals areas of strength and weakness of each method.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\KL2MMAGM\\Shafaei et al_2018_Does Your Model Know the Digit 6 Is Not a Cat.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TAXZU7X5\\1809.html}
}

@article{shallueMeasuringEffectsData2019,
  title = {Measuring the {{Effects}} of {{Data Parallelism}} on {{Neural Network Training}}},
  author = {Shallue, Christopher J. and Lee, Jaehoon and Antognini, Joseph and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E.},
  date = {2019},
  journaltitle = {Journal of Machine Learning Research},
  volume = {20},
  number = {112},
  pages = {1--49},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v20/18-789.html},
  urldate = {2019-09-02},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JLALA8VE\\Shallue et al_2019_Measuring the Effects of Data Parallelism on Neural Network Training2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\NXP9FSH6\\Shallue et al. - 2019 - Measuring the Effects of Data Parallelism on Neura.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FQS6KPYM\\18-789.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\HUKQQNW3\\1811.html}
}

@incollection{shamirWithoutReplacementSamplingStochastic2016,
  ids = {shamirWithoutReplacementSamplingStochastic2016a},
  title = {Without-{{Replacement Sampling}} for {{Stochastic Gradient Methods}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Shamir, Ohad},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  date = {2016},
  pages = {46--54},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/6245-without-replacement-sampling-for-stochastic-gradient-methods.pdf},
  urldate = {2019-08-26},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\GNGFG4Y8\\Shamir_2016_Without-Replacement Sampling for Stochastic Gradient Methods.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\RSBM2DCR\\Shamir_2016_Without-Replacement Sampling for Stochastic Gradient Methods2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TZ8KQWUN\\6245-without-replacement-sampling-for-stochastic-gradient-methods.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\X95PWIN9\\6245-without-replacement-sampling-for-stochastic-gradient-methods.html}
}

@unpublished{shangEdgeAttentionbasedMultiRelational2018,
  title = {Edge {{Attention-based Multi-Relational Graph Convolutional Networks}}},
  author = {Shang, Chao and Liu, Qinqing and Chen, Ko-Shin and Sun, Jiangwen and Lu, Jin and Yi, Jinfeng and Bi, Jinbo},
  date = {2018-02-13},
  eprint = {1802.04944},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.04944},
  urldate = {2018-09-17},
  abstract = {Graph convolutional network (GCN) is generalization of convolutional neural network (CNN) to work with arbitrarily structured graphs. A binary adjacency matrix is commonly used in training a GCN. Recently, the attention mechanism allows the network to learn a dynamic and adaptive aggregation of the neighborhood. We propose a new GCN model on the graphs where edges are characterized in multiple views or precisely in terms of multiple relationships. For instance, in chemical graph theory, compound structures are often represented by the hydrogen-depleted molecular graph where nodes correspond to atoms and edges correspond to chemical bonds. Multiple attributes can be important to characterize chemical bonds, such as atom pair (the types of atoms that a bond connects), aromaticity, and whether a bond is in a ring. The different attributes lead to different graph representations for the same molecule. There is growing interests in both chemistry and machine learning fields to directly learn molecular properties of compounds from the molecular graph, instead of from fingerprints predefined by chemists. The proposed GCN model, which we call edge attention-based multi-relational GCN (EAGCN), jointly learns attention weights and node features in graph convolution. For each bond attribute, a real-valued attention matrix is used to replace the binary adjacency matrix. By designing a dictionary for the edge attention, and forming the attention matrix of each molecule by looking up the dictionary, the EAGCN exploits correspondence between bonds in different molecules. The prediction of compound properties is based on the aggregated node features, which is independent of the varying molecule (graph) size. We demonstrate the efficacy of the EAGCN on multiple chemical datasets: Tox21, HIV, Freesolv, and Lipophilicity, and interpret the resultant attention weights.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\USEBPERP\Zotero\storage\3HU2W2JB\1802.html}
}

@article{shannonCommunicationTheorySecrecy1949,
  title = {Communication {{Theory}} of {{Secrecy Systems}}*},
  author = {Shannon, C. E.},
  date = {1949-10},
  journaltitle = {Bell System Technical Journal},
  volume = {28},
  number = {4},
  pages = {656--715},
  issn = {00058580},
  doi = {10.1002/j.1538-7305.1949.tb00928.x},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6769090},
  urldate = {2020-09-29},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\PKGYVTBK\Shannon - 1949 - Communication Theory of Secrecy Systems.pdf}
}

@article{shannonMathematicalTheoryCommunication,
  ids = {shannon1948mathematical},
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, C E},
  pages = {55},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4TC7DQ86\\Shannon - A Mathematical Theory of Communication.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\79QDWNTF\\Shannon_1948_A Mathematical Theory of Communication.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7J6H6X93\\shannon1948.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ALCBZW75\\Shannon - 1948 - A mathematical theory of communication.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\4YG2B43P\\6773024.html}
}

@article{shannonPredictionEntropyPrinted,
  title = {Prediction and {{Entropy}} of {{Printed English}}},
  author = {Shannon, C E},
  pages = {12},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\F86Z3FAR\Shannon - Prediction and Entropy of Printed English.pdf}
}

@unpublished{shazeerOutrageouslyLargeNeural2017,
  title = {Outrageously {{Large Neural Networks}}: {{The Sparsely-Gated Mixture-of-Experts Layer}}},
  shorttitle = {Outrageously {{Large Neural Networks}}},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  date = {2017-01-23},
  eprint = {1701.06538},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1701.06538},
  urldate = {2019-01-04},
  abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\QVHDAI3N\\Shazeer et al_2017_Outrageously Large Neural Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\LCHDXGRZ\\1701.html}
}

@article{shenTimeSeriesAnomaly2021,
  title = {Time {{Series Anomaly Detection}} with {{Multiresolution Ensemble Decoding}}},
  author = {Shen, Lifeng and Yu, Zhongzhong and Ma, Qianli and Kwok, James T.},
  date = {2021-05-18},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {11},
  pages = {9567--9575},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i11.17152},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/17152},
  urldate = {2023-01-06},
  abstract = {Recurrent autoencoder is a popular model for time series anomaly detection, in which outliers or abnormal segments are identified by their high reconstruction errors. However, existing recurrent autoencoders can easily suffer from overfitting and error accumulation due to sequential decoding. In this paper, we propose a simple yet efficient recurrent network ensemble called Recurrent Autoencoder with Multiresolution Ensemble Decoding (RAMED). By using decoders with different decoding lengths and a new coarse-to-fine fusion mechanism, lower-resolution information can help long-range decoding for decoders with higher-resolution outputs. A multiresolution shape-forcing loss is further introduced to encourage decoders' outputs at multiple resolutions to match the input's global temporal shape. Finally, the output from the decoder with the highest resolution is used to obtain an anomaly score at each time step. Extensive empirical studies on real-world benchmark data sets demonstrate that the proposed RAMED model outperforms recent strong baselines on time series anomaly detection.},
  issue = {11},
  langid = {english},
  keywords = {Time-Series/Data Streams},
  file = {C:\Users\USEBPERP\Zotero\storage\EKDUY9KJ\Shen et al. - 2021 - Time Series Anomaly Detection with Multiresolution.pdf}
}

@inproceedings{shenTimeseriesAnomalyDetection2020,
  title = {Timeseries {{Anomaly Detection}} Using {{Temporal Hierarchical One-Class Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Shen, Lifeng and Li, Zhuocong and Kwok, James},
  date = {2020},
  volume = {33},
  pages = {13016--13026},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2020/hash/97e401a02082021fd24957f852e0e475-Abstract.html},
  urldate = {2023-01-06},
  abstract = {Real-world timeseries have complex underlying temporal dynamics and the detection of anomalies is challenging. In this paper, we propose the Temporal Hierarchical One-Class (THOC) network, a temporal one-class classification model for timeseries anomaly detection. It captures temporal dynamics in multiple scales by using a dilated recurrent neural network with skip connections. Using multiple hyperspheres obtained with a hierarchical clustering process, a one-class objective called Multiscale Vector Data Description is defined. This allows the temporal dynamics to be well captured by a set of multi-resolution temporal clusters. To further facilitate representation learning, the hypersphere centers are encouraged to be orthogonal to each other, and a self-supervision task in the temporal domain is added. The whole model can be trained end-to-end. Extensive empirical studies on various real-world timeseries demonstrate that the proposed THOC network outperforms recent strong deep learning baselines on timeseries anomaly detection.},
  file = {C:\Users\USEBPERP\Zotero\storage\ML7BXAPZ\Shen et al. - 2020 - Timeseries Anomaly Detection using Temporal Hierar.pdf}
}

@article{sheridanRelativeImportanceDomain2015,
  title = {The {{Relative Importance}} of {{Domain Applicability Metrics}} for {{Estimating Prediction Errors}} in {{QSAR Varies}} with {{Training Set Diversity}}},
  author = {Sheridan, Robert P.},
  date = {2015-06-22},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {55},
  number = {6},
  pages = {1098--1107},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.5b00110},
  url = {https://doi.org/10.1021/acs.jcim.5b00110},
  urldate = {2019-07-02},
  abstract = {In QSAR, a statistical model is generated from a training set of molecules (represented by chemical descriptors) and their biological activities (an “activity model”). The aim of the field of domain applicability (DA) is to estimate the uncertainty of prediction of a specific molecule on a specific activity model. A number of DA metrics have been proposed in the literature for this purpose. A quantitative model of the prediction uncertainty (an “error model”) can be built using one or more of these metrics. A previous publication from our laboratory (Sheridan, R. P. J. Chem. Inf. Model. 2013, 53, 2837−2850) suggested that QSAR methods such as random forest could be used to build error models by fitting unsigned prediction errors against DA metrics. The QSAR paradigm contains two useful techniques: descriptor importance can determine which DA metrics are most useful, and cross-validation can be used to tell which subset of DA metrics is sufficient to estimate the unsigned errors. Previously we studied 10 large, diverse data sets and seven DA metrics. For those data sets for which it is possible to build a significant error model from those seven metrics, only two metrics were sufficient to account for almost all of the information in the error model. These were TREE\_SD (the variation of prediction among random forest trees) and PREDICTED (the predicted activity itself). In this paper we show that when data sets are less diverse, as for example in QSAR models of molecules in a single chemical series, these two DA metrics become less important in explaining prediction error, and the DA metric SIMILARITYNEAREST1 (the similarity of the molecule being predicted to the closest training set compound) becomes more important. Our recommendation is that when the mean pairwise similarity (measured with the Carhart AP descriptor and the Dice similarity index) within a QSAR training set is less than 0.5, one can use only TREE\_SD,\,PREDICTED to form the error model, but otherwise one should use TREE\_SD,\,PREDICTED,\,SIMILARITYNEAREST1.},
  file = {C:\Users\USEBPERP\Zotero\storage\HM5E9XTU\acs.jcim.html}
}

@article{sheridanTimeSplitCrossValidationMethod2013,
  title = {Time-{{Split Cross-Validation}} as a {{Method}} for {{Estimating}} the {{Goodness}} of {{Prospective Prediction}}.},
  author = {Sheridan, Robert P.},
  date = {2013-04-22},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {53},
  number = {4},
  pages = {783--790},
  issn = {1549-9596},
  doi = {10.1021/ci400084k},
  url = {https://doi.org/10.1021/ci400084k},
  urldate = {2019-07-02},
  abstract = {Cross-validation is a common method to validate a QSAR model. In cross-validation, some compounds are held out as a test set, while the remaining compounds form a training set. A model is built from the training set, and the test set compounds are predicted on that model. The agreement of the predicted and observed activity values of the test set (measured by, say, R2) is an estimate of the self-consistency of the model and is sometimes taken as an indication of the predictivity of the model. This estimate of predictivity can be optimistic or pessimistic compared to true prospective prediction, depending how compounds in the test set are selected. Here, we show that time-split selection gives an R2 that is more like that of true prospective prediction than the R2 from random selection (too optimistic) or from our analog of leave-class-out selection (too pessimistic). Time-split selection should be used in addition to random selection as a standard for cross-validation in QSAR model building.},
  file = {C:\Users\USEBPERP\Zotero\storage\J3UUMV4X\ci400084k.html}
}

@unpublished{shiGraphGraphsFramework2020,
  ids = {shi2020graph},
  title = {A {{Graph}} to {{Graphs Framework}} for {{Retrosynthesis Prediction}}},
  author = {Shi, Chence and Xu, Minkai and Guo, Hongyu and Zhang, Ming and Tang, Jian},
  date = {2020},
  eprint = {2003.12725},
  eprinttype = {arxiv},
  pages = {10},
  abstract = {A fundamental problem in computational chemistry is to find a set of reactants to synthesize a target molecule, a.k.a. retrosynthesis prediction. Existing state-of-the-art methods rely on matching the target molecule with a large set of reaction templates, which are very computationally expensive and also suffer from the problem of coverage. In this paper, we propose a novel template-free approach called G2Gs by transforming a target molecular graph into a set of reactant molecular graphs. G2Gs first splits the target molecular graph into a set of synthons by identifying the reaction centers, and then translates the synthons to the final reactant graphs via a variational graph translation framework. Experimental results show that G2Gs significantly outperforms existing template-free approaches by up to 63\% in terms of the top-1 accuracy and achieves a performance close to that of state-of-the-art templatebased approaches, but does not require domain knowledge and is much more scalable.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4VWFQPCR\\Shi et al. - A Graph to Graphs Framework for Retrosynthesis Pre.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\PTES87EL\\Shi et al. - 2020 - A Graph to Graphs Framework for Retrosynthesis Pre.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YUBYKDDH\\2003.html}
}

@online{shipmonTimeSeriesAnomaly2017,
  title = {Time {{Series Anomaly Detection}}; {{Detection}} of Anomalous Drops with Limited Features and Sparse Examples in Noisy Highly Periodic Data},
  author = {Shipmon, Dominique T. and Gurevitch, Jason M. and Piselli, Paolo M. and Edwards, Stephen T.},
  date = {2017-08-11},
  eprint = {1708.03665},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1708.03665},
  urldate = {2023-01-06},
  abstract = {Google uses continuous streams of data from industry partners in order to deliver accurate results to users. Unexpected drops in traffic can be an indication of an underlying issue and may be an early warning that remedial action may be necessary. Detecting such drops is non-trivial because streams are variable and noisy, with roughly regular spikes (in many different shapes) in traffic data. We investigated the question of whether or not we can predict anomalies in these data streams. Our goal is to utilize Machine Learning and statistical approaches to classify anomalous drops in periodic, but noisy, traffic patterns. Since we do not have a large body of labeled examples to directly apply supervised learning for anomaly classification, we approached the problem in two parts. First we used TensorFlow to train our various models including DNNs, RNNs, and LSTMs to perform regression and predict the expected value in the time series. Secondly we created anomaly detection rules that compared the actual values to predicted values. Since the problem requires finding sustained anomalies, rather than just short delays or momentary inactivity in the data, our two detection methods focused on continuous sections of activity rather than just single points. We tried multiple combinations of our models and rules and found that using the intersection of our two anomaly detection methods proved to be an effective method of detecting anomalies on almost all of our models. In the process we also found that not all data fell within our experimental assumptions, as one data stream had no periodicity, and therefore no time based model could predict it.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7WU64IC8\\Shipmon et al. - 2017 - Time Series Anomaly Detection\; Detection of anomal.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JZ6M5TMM\\1708.html}
}

@article{shmueliExplainPredict2010,
  title = {To {{Explain}} or to {{Predict}}?},
  author = {Shmueli, Galit},
  date = {2010-08-01},
  journaltitle = {Statist. Sci.},
  volume = {25},
  number = {3},
  issn = {0883-4237},
  doi = {10.1214/10-STS330},
  url = {https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full},
  urldate = {2022-09-12},
  abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\8GI5IPDQ\Shmueli - 2010 - To Explain or to Predict.pdf}
}

@article{shultzTwoDecadesInfluence2019,
  title = {Two {{Decades}} under the {{Influence}} of the {{Rule}} of {{Five}} and the {{Changing Properties}} of {{Approved Oral Drugs}}},
  author = {Shultz, Michael D.},
  date = {2019-02-28},
  journaltitle = {J. Med. Chem.},
  volume = {62},
  number = {4},
  pages = {1701--1714},
  issn = {0022-2623},
  doi = {10.1021/acs.jmedchem.8b00686},
  url = {https://doi.org/10.1021/acs.jmedchem.8b00686},
  urldate = {2019-05-20},
  abstract = {Two decades have passed since the rule of five ushered in the concept of “drug-like” properties. Attempts to quantify, correlate, and categorize molecules based on Ro5 parameters evolved into the introduction of efficiency metrics with far reaching consequences in decision making by industry leaders and scientists seeking to discover new medicines. Examination of oral drug parameters approved before and after the original Ro5 analysis demonstrates that some parameters such as clogP and HBD remained constant while the cutoffs for parameters such as molecular weight and HBA have increased substantially over the past 20 years. The time dependent increase in the molecular weight of oral drugs during the past 20 years provides compelling evidence to disprove the hypothesis that molecular weight is a “drug-like” property. This analysis does not validate parameters that have not changed as being “drug-like” but instead calls into question the entire hypothesis that “drug-like” properties exist.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NJDC2WZZ\\Shultz_2019_Two Decades under the Influence of the Rule of Five and the Changing Properties.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\C73GUUGR\\acs.jmedchem.html}
}

@unpublished{shwartz-zivOpeningBlackBox2017,
  title = {Opening the {{Black Box}} of {{Deep Neural Networks}} via {{Information}}},
  author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
  date = {2017-03-02},
  eprint = {1703.00810},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1703.00810},
  urldate = {2019-01-04},
  abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the \textbackslash textit\{Information Plane\}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on \{\textbackslash emph compression\} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\MFYMJDCV\\Shwartz-Ziv_Tishby_2017_Opening the Black Box of Deep Neural Networks via Information.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2I7B3TBU\\1703.html}
}

@online{siebertSystematicReviewPython2021,
  title = {A Systematic Review of {{Python}} Packages for Time Series Analysis},
  author = {Siebert, Julien and Groß, Janek and Schroth, Christof},
  date = {2021-06-22},
  eprint = {2104.07406},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.07406},
  urldate = {2023-02-13},
  abstract = {This paper presents a systematic review of Python packages with a focus on time series analysis. The objective is to provide (1) an overview of the different time series analysis tasks and preprocessing methods implemented, and (2) an overview of the development characteristics of the packages (e.g., documentation, dependencies, and community size). This review is based on a search of literature databases as well as GitHub repositories. Following the filtering process, 40 packages were analyzed. We classified the packages according to the analysis tasks implemented, the methods related to data preparation, and the means for evaluating the results produced (methods and access to evaluation data). We also reviewed documentation aspects, the licenses, the size of the packages' community, and the dependencies used. Among other things, our results show that forecasting is by far the most frequently implemented task, that half of the packages provide access to real datasets or allow generating synthetic data, and that many packages depend on a few libraries (the most used ones being numpy, scipy and pandas). We hope that this review can help practitioners and researchers navigate the space of Python packages dedicated to time series analysis. We will provide an updated list of the reviewed packages online at https://siebert-julien.github.io/time-series-analysis-python/.},
  pubstate = {preprint},
  keywords = {68-04,Computer Science - Mathematical Software,I.5.5},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\9DDFNVZZ\\Siebert et al. - 2021 - A systematic review of Python packages for time se.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\NHRTSWHB\\2104.html}
}

@online{SignLinkedIn,
  title = {Sign {{Up}} | {{LinkedIn}}},
  url = {/signup/cold-join},
  urldate = {2020-03-25},
  abstract = {500 million+ members | Manage your professional identity. Build and engage with your professional network. Access knowledge, insights and opportunities.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\5FVCSCP6\cold-join.html}
}

@article{silverDeterministicPolicyGradient,
  title = {Deterministic {{Policy Gradient Algorithms}}},
  author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  pages = {9},
  abstract = {In this paper we consider deterministic policy gradient algorithms for reinforcement learning with continuous actions. The deterministic policy gradient has a particularly appealing form: it is the expected gradient of the action-value function. This simple form means that the deterministic policy gradient can be estimated much more efficiently than the usual stochastic policy gradient. To ensure adequate exploration, we introduce an off-policy actor-critic algorithm that learns a deterministic target policy from an exploratory behaviour policy. We demonstrate that deterministic policy gradient algorithms can significantly outperform their stochastic counterparts in high-dimensional action spaces.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\RC73L2MH\Silver et al. - Deterministic Policy Gradient Algorithms.pdf}
}

@article{silverMasteringGameGo2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and family=Driessche, given=George, prefix=van den, useprefix=true and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  date = {2016-01},
  journaltitle = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  url = {http://www.nature.com/articles/nature16961},
  urldate = {2019-03-04},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\36TSQZC4\\Silver et al_2016_Mastering the game of Go with deep neural networks and tree search.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7KJJ8M35\\nature16961.html}
}

@article{silverMasteringGameGo2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and family=Driessche, given=George, prefix=van den, useprefix=true and Graepel, Thore and Hassabis, Demis},
  date = {2017-10},
  journaltitle = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  url = {https://www.nature.com/articles/nature24270},
  urldate = {2019-03-04},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\HU8VVMJE\\Silver et al_2017_Mastering the game of Go without human knowledge.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ADX7GGRM\\nature24270.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\NXVVR9YN\\nature24270.html}
}

@article{silverPredictronEndToEndLearning,
  title = {The {{Predictron}}: {{End-To-End Learning}} and {{Planning}}},
  author = {Silver, David and family=Hasselt, given=Hado, prefix=van, useprefix=true and Hessel, Matteo and Schaul, Tom and Guez, Arthur and Harley, Tim and Dulac-Arnold, Gabriel and Reichert, David and Rabinowitz, Neil and Barreto, Andre and Degris, Thomas},
  pages = {9},
  abstract = {One of the key challenges of artificial intelligence is to learn models that are effective in the context of planning. In this document we introduce the predictron architecture. The predictron consists of a fully abstract model, represented by a Markov reward process, that can be rolled forward multiple “imagined” planning steps. Each forward pass of the predictron accumulates internal rewards and values over multiple planning depths. The predictron is trained end-toend so as to make these accumulated values accurately approximate the true value function. We applied the predictron to procedurally generated random mazes and a simulator for the game of pool. The predictron yielded significantly more accurate predictions than conventional deep neural network architectures.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\G36VFX82\Silver et al. - The Predictron End-To-End Learning and Planning.pdf}
}

@unpublished{simmGraphInformerNetworks2019,
  title = {Graph {{Informer Networks}} for {{Molecules}}},
  author = {Simm, Jaak and Arany, Adam and De Brouwer, Edward and Moreau, Yves},
  date = {2019-07-25},
  eprint = {1907.11318},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.11318},
  urldate = {2019-08-08},
  abstract = {In machine learning, chemical molecules are often represented by sparse high-dimensional vectorial fingerprints. However, a more natural mathematical object for molecule representation is a graph, which is much more challenging to handle from a machine learning perspective. In recent years, several deep learning architectures have been proposed to directly learn from the graph structure of chemical molecules, including graph convolution (Duvenaud et al., 2015) and graph gating networks (Li et al., 2015). Here, we introduce Graph Informer, a route-based multi-head attention mechanism inspired by transformer networks (Vaswani et al., 2017), which incorporates features for node pairs. We show empirically that the proposed method gives significant improvements over existing approaches in prediction tasks for 13C nuclear magnetic resonance spectra and for drug bioactivity. These results indicate that our method is well suited for both node-level and graph-level prediction tasks.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JE9TW5WE\\Simm et al_2019_Graph Informer Networks for Molecules.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\K2626BRD\\1907.html}
}

@unpublished{simonovskyGraphVAEGenerationSmall2018,
  title = {{{GraphVAE}}: {{Towards Generation}} of {{Small Graphs Using Variational Autoencoders}}},
  shorttitle = {{{GraphVAE}}},
  author = {Simonovsky, Martin and Komodakis, Nikos},
  date = {2018-02-09},
  eprint = {1802.03480},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.03480},
  urldate = {2018-09-18},
  abstract = {Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of molecule generation.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\QTCPGVAY\\Simonovsky_Komodakis_2018_GraphVAE.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\P9D6IEMQ\\1802.html}
}

@article{singhUnlabeledDataNow,
  title = {Unlabeled Data: {{Now}} It Helps, Now It Doesn’t},
  author = {Singh, Aarti and Nowak, Robert D and Zhu, Xiaojin},
  pages = {8},
  abstract = {Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conflicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a finite sample analysis that characterizes the value of unlabeled data and quantifies the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can significantly outperform supervised learning, in finite sample regimes and sometimes also in terms of error convergence rates.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\7XY534IR\Singh et al_Unlabeled data.pdf}
}

@article{skoraczynskiPredictingOutcomesOrganic2017,
  title = {Predicting the Outcomes of Organic Reactions via Machine Learning: Are Current Descriptors Sufficient?},
  shorttitle = {Predicting the Outcomes of Organic Reactions via Machine Learning},
  author = {Skoraczyński, G. and Dittwald, P. and Miasojedow, B. and Szymkuć, S. and Gajewska, E. P. and Grzybowski, B. A. and Gambin, A.},
  date = {2017-06-15},
  journaltitle = {Sci Rep},
  volume = {7},
  number = {1},
  pages = {1--9},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-02303-0},
  url = {https://www.nature.com/articles/s41598-017-02303-0},
  urldate = {2019-10-23},
  abstract = {As machine learning/artificial intelligence algorithms are defeating chess masters and, most recently, GO champions, there is interest – and hope – that they will prove equally useful in assisting chemists in predicting outcomes of organic reactions. This paper demonstrates, however, that the applicability of machine learning to the problems of chemical reactivity over diverse types of chemistries remains limited – in particular, with the currently available chemical descriptors, fundamental mathematical theorems impose upper bounds on the accuracy with which raction yields and times can be predicted. Improving the performance of machine-learning methods calls for the development of fundamentally new chemical descriptors.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DAGS6T2Z\\Skoraczyński et al. - 2017 - Predicting the outcomes of organic reactions via m.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\X7ZWPXR5\\Skoraczyński et al_2017_Predicting the outcomes of organic reactions via machine learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZIB9TMLI\\s41598-017-02303-0.html}
}

@unpublished{skvaraAreGenerativeDeep2018,
  title = {Are Generative Deep Models for Novelty Detection Truly Better?},
  author = {Škvára, Vít and Pevný, Tomáš and Šmídl, Václav},
  date = {2018-07-13},
  eprint = {1807.05027},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1807.05027},
  urldate = {2019-05-02},
  abstract = {Many deep models have been recently proposed for anomaly detection. This paper presents comparison of selected generative deep models and classical anomaly detection methods on an extensive number of non--image benchmark datasets. We provide statistical comparison of the selected models, in many configurations, architectures and hyperparamaters. We arrive to conclusion that performance of the generative models is determined by the process of selection of their hyperparameters. Specifically, performance of the deep generative models deteriorates with decreasing amount of anomalous samples used in hyperparameter selection. In practical scenarios of anomaly detection, none of the deep generative models systematically outperforms the kNN.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\B3KGNJCY\\Škvára et al_2018_Are generative deep models for novelty detection truly better.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\EEJ33FTK\\1807.html}
}

@online{smith230StartupsUsing,
  title = {230 {{Startups Using Artificial Intelligence}} in {{Drug Discovery}}},
  author = {Smith, Simon},
  url = {https://blog.benchsci.com/startups-using-artificial-intelligence-in-drug-discovery},
  urldate = {2021-01-04},
  abstract = {Describes startups using artificial intelligence in drug discovery, from identifying candidates, to conducting clinical trials, to publishing data.},
  langid = {american},
  file = {C:\Users\USEBPERP\Zotero\storage\4SF64JB3\startups-using-artificial-intelligence-in-drug-discovery.html}
}

@article{smithActiveLearningStrategies2014,
  title = {Active {{Learning Strategies}} for {{Phenotypic Profiling}} of {{High-Content Screens}}},
  author = {Smith, Kevin and Horvath, Peter},
  date = {2014-06},
  journaltitle = {J Biomol Screen},
  volume = {19},
  number = {5},
  eprint = {24643256},
  eprinttype = {pmid},
  pages = {685--695},
  issn = {1552-454X},
  doi = {10.1177/1087057114527313},
  abstract = {High-content screening is a powerful method to discover new drugs and carry out basic biological research. Increasingly, high-content screens have come to rely on supervised machine learning (SML) to perform automatic phenotypic classification as an essential step of the analysis. However, this comes at a cost, namely, the labeled examples required to train the predictive model. Classification performance increases with the number of labeled examples, and because labeling examples demands time from an expert, the training process represents a significant time investment. Active learning strategies attempt to overcome this bottleneck by presenting the most relevant examples to the annotator, thereby achieving high accuracy while minimizing the cost of obtaining labeled data. In this article, we investigate the impact of active learning on single-cell-based phenotype recognition, using data from three large-scale RNA interference high-content screens representing diverse phenotypic profiling problems. We consider several combinations of active learning strategies and popular SML methods. Our results show that active learning significantly reduces the time cost and can be used to reveal the same phenotypic targets identified using SML. We also identify combinations of active learning strategies and SML methods which perform better than others on the phenotypic profiling problems we studied.},
  langid = {english},
  keywords = {active learning,Algorithms,Artificial Intelligence,HeLa Cells,High-content screening,High-Throughput Screening Assays,Humans,machine learning,Machine Learning,Microscopy,Models Statistical,multiparametric analysis,Pattern Recognition Automated,Phenotype,phenotypic discovery,Problem-Based Learning,Ribosomes,RNA,RNA Interference,RNA Small Interfering,Semliki forest virus,Uukuniemi virus},
  file = {C:\Users\USEBPERP\Zotero\storage\BC4KF4PK\Smith_Horvath_2014_Active Learning Strategies for Phenotypic Profiling of High-Content Screens.pdf}
}

@unpublished{smithBayesianPerspectiveGeneralization2018,
  title = {A {{Bayesian Perspective}} on {{Generalization}} and {{Stochastic Gradient Descent}}},
  author = {Smith, Samuel L. and Le, Quoc V.},
  date = {2018-02-14},
  eprint = {1710.06451},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1710.06451},
  urldate = {2020-02-10},
  abstract = {We consider two questions at the heart of machine learning; how can we predict if a minimum will generalize to the test set, and why does stochastic gradient descent find minima that generalize well? Our work responds to Zhang et al. (2016), who showed deep neural networks can easily memorize randomly labeled training data, despite generalizing well on real labels of the same inputs. We show that the same phenomenon occurs in small linear models. These observations are explained by the Bayesian evidence, which penalizes sharp minima but is invariant to model parameterization. We also demonstrate that, when one holds the learning rate fixed, there is an optimum batch size which maximizes the test set accuracy. We propose that the noise introduced by small mini-batches drives the parameters towards minima whose evidence is large. Interpreting stochastic gradient descent as a stochastic differential equation, we identify the "noise scale" \$g = \textbackslash epsilon (\textbackslash frac\{N\}\{B\} - 1) \textbackslash approx \textbackslash epsilon N/B\$, where \$\textbackslash epsilon\$ is the learning rate, \$N\$ the training set size and \$B\$ the batch size. Consequently the optimum batch size is proportional to both the learning rate and the size of the training set, \$B\_\{opt\} \textbackslash propto \textbackslash epsilon N\$. We verify these predictions empirically.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\IK9IQQ55\\Smith_Le_2018_A Bayesian Perspective on Generalization and Stochastic Gradient Descent.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QXMEPCKA\\1710.html}
}

@article{smithDealingBiasArtificial2019,
  entrysubtype = {newspaper},
  title = {Dealing {{With Bias}} in {{Artificial Intelligence}}},
  author = {Smith, Craig S.},
  date = {2019-11-19T05:00:23-05:00},
  journaltitle = {The New York Times},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2019/11/19/technology/artificial-intelligence-bias.html},
  urldate = {2020-06-24},
  abstract = {Three women with extensive experience in A.I. spoke on the topic and how to confront it.},
  journalsubtitle = {Technology},
  langid = {american},
  keywords = {Artificial Intelligence,Computers and the Internet,Discrimination,Gebru Timnit,Koller Daphne,Russakovsky Olga,Women and Girls}
}

@unpublished{smithDisciplinedApproachNeural2018,
  title = {A Disciplined Approach to Neural Network Hyper-Parameters: {{Part}} 1 -- Learning Rate, Batch Size, Momentum, and Weight Decay},
  shorttitle = {A Disciplined Approach to Neural Network Hyper-Parameters},
  author = {Smith, Leslie N.},
  date = {2018-04-24},
  eprint = {1803.09820},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.09820},
  urldate = {2020-09-28},
  abstract = {Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums. Files to help replicate the results reported here are available.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\AUWHE36L\\Smith_2018_A disciplined approach to neural network hyper-parameters.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\EGA86SJS\\1803.html}
}

@unpublished{smithDonDecayLearning2018,
  title = {Don't {{Decay}} the {{Learning Rate}}, {{Increase}} the {{Batch Size}}},
  author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
  date = {2018-02-23},
  eprint = {1711.00489},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1711.00489},
  urldate = {2020-02-05},
  abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate \$\textbackslash epsilon\$ and scaling the batch size \$B \textbackslash propto \textbackslash epsilon\$. Finally, one can increase the momentum coefficient \$m\$ and scale \$B \textbackslash propto 1/(1-m)\$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to \$76.1\textbackslash\%\$ validation accuracy in under 30 minutes.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DFSEND27\\Smith et al_2018_Don't Decay the Learning Rate, Increase the Batch Size.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7287NYUP\\1711.html}
}

@article{solomonGenomeEditingAnimals2020,
  title = {Genome Editing in Animals: Why {{FDA}} Regulation Matters},
  shorttitle = {Genome Editing in Animals},
  author = {Solomon, Steven M.},
  date = {2020-02},
  journaltitle = {Nat Biotechnol},
  volume = {38},
  number = {2},
  pages = {142--143},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/s41587-020-0413-7},
  url = {http://www.nature.com/articles/s41587-020-0413-7},
  urldate = {2022-08-29},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\DWP85EUM\Solomon - 2020 - Genome editing in animals why FDA regulation matt.pdf}
}

@unpublished{somnathLearningGraphModels2020,
  ids = {somnath2020learninga,somnath2020learningb},
  title = {Learning {{Graph Models}} for {{Template-Free Retrosynthesis}}},
  author = {Somnath, Vignesh Ram and Bunne, Charlotte and Coley, Connor W. and Krause, Andreas and Barzilay, Regina},
  date = {2020-06-12},
  eprint = {2006.07038},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.07038},
  urldate = {2021-04-08},
  abstract = {Retrosynthesis prediction is a fundamental problem in organic synthesis, where the task is to identify precursor molecules that can be used to synthesize a target molecule. Despite recent advancements in neural retrosynthesis algorithms, they are unable to fully recapitulate the strategies employed by chemists and do not generalize well to infrequent reaction types. In this paper, we propose a graph-based approach that capitalizes on the idea that the graph topology of precursor molecules is largely unaltered during the reaction. The model first predicts the set of graph edits transforming the target into incomplete molecules called synthons. Next, the model learns to expand synthons into complete molecules by attaching relevant leaving groups. Since the model operates at the level of molecular fragments, it avoids full generation, greatly simplifying the underlying architecture and improving its ability to generalize. The model yields \$11.7\textbackslash\%\$ absolute improvement over state-of-the-art approaches on the USPTO-50k dataset, and a \$4\textbackslash\%\$ absolute improvement on a rare reaction subset of the same dataset.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\CEBKNWCF\\Somnath et al. - 2020 - Learning Graph Models for Template-Free Retrosynth.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HHVBAYEK\\2006.html}
}

@unpublished{songDistributionCalibrationRegression2019,
  title = {Distribution {{Calibration}} for {{Regression}}},
  author = {Song, Hao and Diethe, Tom and Kull, Meelis and Flach, Peter},
  date = {2019-05-15},
  eprint = {1905.06023},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.06023},
  urldate = {2019-05-27},
  abstract = {We are concerned with obtaining well-calibrated output distributions from regression models. Such distributions allow us to quantify the uncertainty that the model has regarding the predicted target value. We introduce the novel concept of distribution calibration, and demonstrate its advantages over the existing definition of quantile calibration. We further propose a post-hoc approach to improving the predictions from previously trained regression models, using multi-output Gaussian Processes with a novel Beta link function. The proposed method is experimentally verified on a set of common regression models and shows improvements for both distribution-level and quantile-level calibration.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LRS8PXFU\\Song et al_2019_Distribution Calibration for Regression.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\N83UJ4NZ\\1905.html}
}

@unpublished{songImprovedTechniquesTraining2020,
  title = {Improved {{Techniques}} for {{Training Score-Based Generative Models}}},
  author = {Song, Yang and Ermon, Stefano},
  date = {2020-06-16},
  eprint = {2006.09011},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.09011},
  urldate = {2020-06-18},
  abstract = {Score-based generative models can produce high quality image samples comparable to GANs, without requiring adversarial optimization. However, existing training procedures are limited to images of low resolution (typically below 32x32), and can be unstable under some settings. We provide a new theoretical analysis of learning and sampling from score models in high dimensional spaces, explaining existing failure modes and motivating new solutions that generalize across datasets. To enhance stability, we also propose to maintain an exponential moving average of model weights. With these improvements, we can effortlessly scale score-based generative models to images with unprecedented resolutions ranging from 64x64 to 256x256. Our score-based models can generate high-fidelity samples that rival best-in-class GANs on various image datasets, including CelebA, FFHQ, and multiple LSUN categories.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BZXGPHDP\\Song_Ermon_2020_Improved Techniques for Training Score-Based Generative Models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\3KMWZPTY\\2006.html}
}

@article{sousaGenerativeDeepLearning2021,
  title = {Generative {{Deep Learning}} for {{Targeted Compound Design}}},
  author = {Sousa, Tiago and Correia, João and Pereira, Vítor and Rocha, Miguel},
  date = {2021-11-22},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {61},
  number = {11},
  pages = {5343--5361},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.0c01496},
  url = {https://doi.org/10.1021/acs.jcim.0c01496},
  urldate = {2024-01-05},
  abstract = {In the past few years, de novo molecular design has increasingly been using generative models from the emergent field of Deep Learning, proposing novel compounds that are likely to possess desired properties or activities. De novo molecular design finds applications in different fields ranging from drug discovery and materials sciences to biotechnology. A panoply of deep generative models, including architectures as Recurrent Neural Networks, Autoencoders, and Generative Adversarial Networks, can be trained on existing data sets and provide for the generation of novel compounds. Typically, the new compounds follow the same underlying statistical distributions of properties exhibited on the training data set Additionally, different optimization strategies, including transfer learning, Bayesian optimization, reinforcement learning, and conditional generation, can direct the generation process toward desired aims, regarding their biological activities, synthesis processes or chemical features. Given the recent emergence of these technologies and their relevance, this work presents a systematic and critical review on deep generative models and related optimization methods for targeted compound design, and their applications.},
  file = {C:\Users\USEBPERP\Zotero\storage\3Y9LI3UM\Sousa et al. - 2021 - Generative Deep Learning for Targeted Compound Des.pdf}
}

@online{SphereExclusion,
  title = {Sphere {{Exclusion}}},
  url = {https://hub.knime.com/infocom/extensions/jp.co.infocom.cheminfo.jchem.feature/latest/jp.co.infocom.cheminfo.jchem.sphereexclusion.SphereExclusionNodeFactory},
  urldate = {2023-09-01},
  abstract = {Sphere exclusion is a simple, intuitive selection method. Clustering begins by selecting an initial structure, including all structures that meet a defined sim…},
  langid = {english},
  organization = {KNIME Community Hub},
  file = {C:\Users\USEBPERP\Zotero\storage\IKRFLYG6\jp.co.infocom.cheminfo.jchem.sphereexclusion.html}
}

@online{SphereExclusionClustering,
  title = {Sphere {{Exclusion}} Clustering | {{Chemaxon Docs}}},
  url = {https://docs.chemaxon.com/display/docs/sphere-exclusion-clustering.md},
  urldate = {2022-07-28},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\TF7EPZYK\\sphere-exclusion-clustering.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\XP6E6XWF\\sphere-exclusion-clustering.html}
}

@online{SpreadCoronavirusMink2020,
  title = {Spread of Coronavirus in Mink Hits 'scary Buttons' but Experts Urge Calm},
  date = {2020-11-05T23:28:37+00:00},
  url = {https://www.statnews.com/2020/11/05/spread-of-mutated-coronavirus-in-danish-mink-hits-all-the-scary-buttons-but-fears-may-be-overblown/},
  urldate = {2020-11-06},
  abstract = {Mutations in the virus that causes Covid-19 have prompted alarm that a more severe virus could infect people. Experts are skeptical.},
  langid = {american},
  organization = {STAT},
  file = {C:\Users\USEBPERP\Zotero\storage\GXRMXUBC\spread-of-mutated-coronavirus-in-danish-mink-hits-all-the-scary-buttons-but-fears-may-be-overbl.html}
}

@article{steinbeckChemistryDevelopmentKit2003,
  title = {The {{Chemistry Development Kit}} ({{CDK}}): {{An Open-Source Java Library}} for {{Chemo-}} and {{Bioinformatics}}},
  shorttitle = {The {{Chemistry Development Kit}} ({{CDK}})},
  author = {Steinbeck, Christoph and Han, Yongquan and Kuhn, Stefan and Horlacher, Oliver and Luttmann, Edgar and Willighagen, Egon},
  date = {2003-03-01},
  journaltitle = {J. Chem. Inf. Comput. Sci.},
  volume = {43},
  number = {2},
  pages = {493--500},
  issn = {0095-2338},
  doi = {10.1021/ci025584y},
  url = {https://doi.org/10.1021/ci025584y},
  urldate = {2018-06-27},
  abstract = {The Chemistry Development Kit (CDK) is a freely available open-source Java library for Structural Chemo- and Bioinformatics. Its architecture and capabilities as well as the development as an open-source project by a team of international collaborators from academic and industrial institutions is described. The CDK provides methods for many common tasks in molecular informatics, including 2D and 3D rendering of chemical structures, I/O routines, SMILES parsing and generation, ring searches, isomorphism checking, structure diagram generation, etc. Application scenarios as well as access information for interested users and potential contributors are given.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\C3PUVU3F\\Steinbeck et al_2003_The Chemistry Development Kit (CDK).pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\5GPYH6BN\\ci025584y.html}
}

@article{steinwartClassificationFrameworkAnomaly,
  title = {A {{Classiﬁcation Framework}} for {{Anomaly Detection}}},
  author = {Steinwart, Ingo and Hush, Don and Scovel, Clint},
  abstract = {One way to describe anomalies is by saying that anomalies are not concentrated. This leads to the problem of finding level sets for the data generating density. We interpret this learning problem as a binary classification problem and compare the corresponding classification risk with the standard performance measure for the density level problem. In particular it turns out that the empirical classification risk can serve as an empirical performance measure for the anomaly detection problem. This allows us to compare different anomaly detection algorithms empirically, i.e. with the help of a test set. Furthermore, by the above interpretation we can give a strong justification for the well-known heuristic of artificially sampling “labeled” samples, provided that the sampling plan is well chosen. In particular this enables us to propose a support vector machine (SVM) for anomaly detection for which we can easily establish universal consistency. Finally, we report some experiments which compare our SVM to other commonly used methods including the standard one-class SVM.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\FZC357GK\Steinwart et al. - A Classiﬁcation Framework for Anomaly Detection.pdf}
}

@article{stephensBayesianRetrospectiveMultipleChangepoint1994,
  title = {Bayesian {{Retrospective Multiple-Changepoint Identification}}},
  author = {Stephens, D. A.},
  date = {1994},
  journaltitle = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  volume = {43},
  number = {1},
  eprint = {2986119},
  eprinttype = {jstor},
  pages = {159--178},
  publisher = {[Wiley, Royal Statistical Society]},
  issn = {0035-9254},
  doi = {10.2307/2986119},
  url = {https://www.jstor.org/stable/2986119},
  urldate = {2023-01-27},
  abstract = {Changepoint identification is important in many data analysis problems, such as industrial control and medical diagnosis--given a data sequence, we wish to make inference about the location of one or more points of the sequence at which there is a change in the model or parameters driving the system. For long data sequences, however, analysis (especially in the multiple-changepoint case) can become computationally prohibitive, and for complex non-linear models analytical and conventional numerical techniques are infeasible. We discuss the use of a sampling-based technique, the Gibbs sampler, in multiple-changepoint problems and demonstrate how it can be used to reduce the computational load involved considerably. Also, often it is reasonable to presume that the data model itself is continuous with respect to time, i.e. continuous at the changepoints. This necessitates a continuous parameter representation of the changepoint problem, which also leads to computational difficulties. We demonstrate how inferences can be made readily in such problems by using the Gibbs sampler. We study three examples: a simple discrete two-changepoint problem based on a binomial data model; a continuous switching linear regression problem; a continuous, non-linear, multiple-changepoint problem.},
  file = {C:\Users\USEBPERP\Zotero\storage\RX7FNKM6\Stephens - 1994 - Bayesian Retrospective Multiple-Changepoint Identi.pdf}
}

@article{stiglerEpicStoryMaximum2007,
  title = {The {{Epic Story}} of {{Maximum Likelihood}}},
  author = {Stigler, Stephen M.},
  date = {2007-11},
  journaltitle = {Statist. Sci.},
  volume = {22},
  number = {4},
  eprint = {0804.2996},
  eprinttype = {arxiv},
  pages = {598--620},
  issn = {0883-4237},
  doi = {10.1214/07-STS249},
  url = {http://arxiv.org/abs/0804.2996},
  urldate = {2019-06-03},
  abstract = {At a superficial level, the idea of maximum likelihood must be prehistoric: early hunters and gatherers may not have used the words ``method of maximum likelihood'' to describe their choice of where and how to hunt and gather, but it is hard to believe they would have been surprised if their method had been described in those terms. It seems a simple, even unassailable idea: Who would rise to argue in favor of a method of minimum likelihood, or even mediocre likelihood? And yet the mathematical history of the topic shows this ``simple idea'' is really anything but simple. Joseph Louis Lagrange, Daniel Bernoulli, Leonard Euler, Pierre Simon Laplace and Carl Friedrich Gauss are only some of those who explored the topic, not always in ways we would sanction today. In this article, that history is reviewed from back well before Fisher to the time of Lucien Le Cam's dissertation. In the process Fisher's unpublished 1930 characterization of conditions for the consistency and efficiency of maximum likelihood estimates is presented, and the mathematical basis of his three proofs discussed. In particular, Fisher's derivation of the information inequality is seen to be derived from his work on the analysis of variance, and his later approach via estimating functions was derived from Euler's Relation for homogeneous functions. The reaction to Fisher's work is reviewed, and some lessons drawn.},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\M4ECZUYF\\Stigler_2007_The Epic Story of Maximum Likelihood.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SPMVWQQR\\0804.html}
}

@article{stokesDeepLearningApproach2020,
  title = {A {{Deep Learning Approach}} to {{Antibiotic Discovery}}},
  author = {Stokes, Jonathan M. and Yang, Kevin and Swanson, Kyle and Jin, Wengong and Cubillos-Ruiz, Andres and Donghia, Nina M. and MacNair, Craig R. and French, Shawn and Carfrae, Lindsey A. and Bloom-Ackermann, Zohar and Tran, Victoria M. and Chiappino-Pepe, Anush and Badran, Ahmed H. and Andrews, Ian W. and Chory, Emma J. and Church, George M. and Brown, Eric D. and Jaakkola, Tommi S. and Barzilay, Regina and Collins, James J.},
  date = {2020-02},
  journaltitle = {Cell},
  volume = {180},
  number = {4},
  pages = {688-702.e13},
  issn = {00928674},
  doi = {10.1016/j.cell.2020.01.021},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0092867420301021},
  urldate = {2023-10-02},
  abstract = {Due to the rapid emergence of antibiotic-resistant bacteria, there is a growing need to discover new antibiotics. To address this challenge, we trained a deep neural network capable of predicting molecules with antibacterial activity. We performed predictions on multiple chemical libraries and discovered a molecule from the Drug Repurposing Hub—halicin—that is structurally divergent from conventional antibiotics and displays bactericidal activity against a wide phylogenetic spectrum of pathogens including Mycobacterium tuberculosis and carbapenem-resistant Enterobacteriaceae. Halicin also effectively treated Clostridioides difficile and pan-resistant Acinetobacter baumannii infections in murine models. Additionally, from a discrete set of 23 empirically tested predictions from {$>$}107 million molecules curated from the ZINC15 database, our model identified eight antibacterial compounds that are structurally distant from known antibiotics. This work highlights the utility of deep learning approaches to expand our antibiotic arsenal through the discovery of structurally distinct antibacterial molecules.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\AEG8RCQ8\Stokes et al. - 2020 - A Deep Learning Approach to Antibiotic Discovery.pdf}
}

@online{StrategyDiscoverDiverse,
  title = {Strategy {{To Discover Diverse Optimal Molecules}} in the {{Small Molecule Universe}} - [Scite Report]},
  url = {https://scite.ai/reports/strategy-to-discover-diverse-optimal-ElGbLj},
  urldate = {2023-09-29}
}

@article{strathernImprovingRatingsAudit1997,
  title = {‘{{Improving}} Ratings’: Audit in the {{British University}} System},
  shorttitle = {‘{{Improving}} Ratings’},
  author = {Strathern, Marilyn},
  date = {1997-07},
  journaltitle = {European Review},
  volume = {5},
  number = {3},
  pages = {305--321},
  publisher = {Cambridge University Press},
  issn = {1474-0575, 1062-7987},
  doi = {10.1002/(SICI)1234-981X(199707)5:3<305::AID-EURO184>3.0.CO;2-4},
  url = {https://www.cambridge.org/core/journals/european-review/article/improving-ratings-audit-in-the-british-university-system/FC2EE640C0C44E3DB87C29FB666E9AAB},
  urldate = {2020-03-12},
  abstract = {This paper gives an anthropological comment on what has been called the ‘audit explosion’, the proliferation of procedures for evaluating performance. In higher education the subject of audit (in this sense) is not so much the education of the students as the institutional provision for their education. British universities, as institutions, are increasingly subject to national scrutiny for teaching, research and administrative competence. In the wake of this scrutiny comes a new cultural apparatus of expectations and technologies. While the metaphor of financial auditing points to the important values of accountability, audit does more than monitor—it has a life of its own that jeopardizes the life it audits. The runaway character of assessment practices is analysed in terms of cultural practice. Higher education is intimately bound up with the origins of such practices, and is not just the latter day target of them. © 1997 by John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\IENKR7TT\FC2EE640C0C44E3DB87C29FB666E9AAB.html}
}

@online{StreamZVgPeanut2021,
  title = {{{StreamZ}}.vg {{The Peanut Butter Falcon}} 2019 {{German AC3 WEBRip}} X264-{{HQX}}},
  date = {2021-02-13},
  url = {https://streamz.vg/wdu?wprw0tn2rv5falx8d0wkljxzoiwynxwkmngmg4p7zvn9gkiukef5yc4nu1liyi3l},
  urldate = {2021-02-13}
}

@article{strieth-kalthoffMachineLearningRopes2020,
  title = {Machine Learning the Ropes: Principles, Applications and Directions in Synthetic Chemistry},
  shorttitle = {Machine Learning the Ropes},
  author = {Strieth-Kalthoff, Felix and Sandfort, Frederik and Segler, Marwin H. S. and Glorius, Frank},
  date = {2020},
  journaltitle = {Chem. Soc. Rev.},
  pages = {10.1039.C9CS00786E},
  issn = {0306-0012, 1460-4744},
  doi = {10.1039/C9CS00786E},
  url = {http://xlink.rsc.org/?DOI=C9CS00786E},
  urldate = {2020-08-10},
  abstract = {Chemists go ML! This tutorial review provides easy access to the fundamentals of machine learning from a synthetic chemist's perspective. Its diverse applications for molecular design, synthesis planning, or reactivity prediction are summarized.           ,              Machine learning (ML) has emerged as a general, problem-solving paradigm with many applications in computer vision, natural language processing, digital safety, or medicine. By recognizing complex patterns in data, ML bears the potential to modernise the way how many chemical challenges are approached. In this review, an introduction to ML is given from the perspective of synthetic chemistry: starting from the fundamentals regarding algorithms and best-practice workflows, the review covers different applications of machine learning in synthesis planning, property prediction, molecular design, and reactivity prediction. In particular, different approaches of representing and utilizing organic molecules will be discussed – providing synthetic chemists both with the understanding and the tools required to apply machine learning in the context of their research, and pointers for further studying.},
  langid = {english}
}

@article{strubleCurrentFutureRoles2020,
  ids = {struble2020currenta,struble2020currentb,strubleCurrentFutureRoles2020a},
  title = {Current and {{Future Roles}} of {{Artificial Intelligence}} in {{Medicinal Chemistry Synthesis}}},
  author = {Struble, Thomas J. and Alvarez, Juan C. and Brown, Scott P. and Chytil, Milan and Cisar, Justin and DesJarlais, Renee L. and Engkvist, Ola and Frank, Scott A. and Greve, Daniel R. and Griffin, Daniel J. and Hou, Xinjun and Johannes, Jeffrey W. and Kreatsoulas, Constantine and Lahue, Brian and Mathea, Miriam and Mogk, Georg and Nicolaou, Christos A. and Palmer, Andrew D. and Price, Daniel J. and Robinson, Richard I. and Salentin, Sebastian and Xing, Li and Jaakkola, Tommi and family=Green, given=William. H., given-i={{William}}H and Barzilay, Regina and Coley, Connor W. and Jensen, Klavs F.},
  date = {2020-08-27},
  journaltitle = {J. Med. Chem.},
  volume = {63},
  number = {16},
  pages = {8667--8682},
  publisher = {American Chemical Society},
  issn = {0022-2623},
  doi = {10.1021/acs.jmedchem.9b02120},
  url = {https://doi.org/10.1021/acs.jmedchem.9b02120},
  urldate = {2021-04-08},
  abstract = {Artificial intelligence and machine learning have demonstrated their potential role in predictive chemistry and synthetic planning of small molecules; there are at least a few reports of companies employing in silico synthetic planning into their overall approach to accessing target molecules. A data-driven synthesis planning program is one component being developed and evaluated by the Machine Learning for Pharmaceutical Discovery and Synthesis (MLPDS) consortium, comprising MIT and 13 chemical and pharmaceutical company members. Together, we wrote this perspective to share how we think predictive models can be integrated into medicinal chemistry synthesis workflows, how they are currently used within MLPDS member companies, and the outlook for this field.},
  issue = {16},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2XDSBEP5\\Struble et al. - 2020 - Current and Future Roles of Artificial Intelligenc.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\6UZQ6CLX\\Struble et al. - 2020 - Current and Future Roles of Artificial Intelligenc.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\R7HY5CJW\\Struble et al. - 2020 - Current and Future Roles of Artificial Intelligenc.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2EG6ZKSS\\acs.jmedchem.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\6HY6M5KA\\acs.jmedchem.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\IMMN58XU\\acs.jmedchem.html}
}

@article{StudyUSOligarchy2014,
  entrysubtype = {newspaper},
  title = {Study: {{US}} Is an Oligarchy, Not a Democracy},
  shorttitle = {Study},
  date = {2014-04-17},
  journaltitle = {BBC News},
  url = {https://www.bbc.com/news/blogs-echochambers-27074746},
  urldate = {2020-12-29},
  abstract = {What in the World: A new report finds that an elite few dominate US policy, the human error behind South Korea's ferry tragedy, and Algeria's uneasy status quo election.},
  journalsubtitle = {Echo Chambers},
  langid = {british},
  file = {C:\Users\USEBPERP\Zotero\storage\3LI8N4XG\blogs-echochambers-27074746.html}
}

@article{sumitaHuntingOrganicMolecules2018,
  title = {Hunting for {{Organic Molecules}} with {{Artificial Intelligence}}: {{Molecules Optimized}} for {{Desired Excitation Energies}}},
  shorttitle = {Hunting for {{Organic Molecules}} with {{Artificial Intelligence}}},
  author = {Sumita, Masato and Yang, Xiufeng and Ishihara, Shinsuke and Tamura, Ryo and Tsuda, Koji},
  date = {2018-09-26},
  journaltitle = {ACS Cent Sci},
  volume = {4},
  number = {9},
  eprint = {30276245},
  eprinttype = {pmid},
  pages = {1126--1133},
  issn = {2374-7943},
  doi = {10.1021/acscentsci.8b00213},
  abstract = {This work presents a proof-of-concept study in artificial-intelligence-assisted (AI-assisted) chemistry where a machine-learning-based molecule generator is coupled with density functional theory (DFT) calculations, synthesis, and measurement. Although deep-learning-based molecule generators have shown promise, it is unclear to what extent they can be useful in real-world materials development. To assess the reliability of AI-assisted chemistry, we prepared a platform using a molecule generator and a DFT simulator, and attempted to generate novel photofunctional molecules whose lowest excited states lie at desired energetic levels. A 10 day run on the 12-core server discovered 86 potential photofunctional molecules around target lowest excitation levels, designated as 200, 300, 400, 500, and 600 nm. Among the molecules discovered, six were synthesized, and five were confirmed to reproduce DFT predictions in ultraviolet visible absorption measurements. This result shows the potential of AI-assisted chemistry to discover ready-to-synthesize novel molecules with modest computational resources.},
  langid = {english},
  pmcid = {PMC6161049},
  file = {C:\Users\USEBPERP\Zotero\storage\4R4DULUE\Sumita et al_2018_Hunting for Organic Molecules with Artificial Intelligence.pdf}
}

@unpublished{sunEnergybasedViewRetrosynthesis2020,
  ids = {sun2020energybaseda,sun2020energybasedb},
  title = {Energy-Based {{View}} of {{Retrosynthesis}}},
  author = {Sun, Ruoxi and Dai, Hanjun and Li, Li and Kearnes, Steven and Dai, Bo},
  date = {2020-09-28},
  eprint = {2007.13437},
  eprinttype = {arxiv},
  url = {https://openreview.net/forum?id=0Hj3tFCSjUd},
  urldate = {2021-05-06},
  abstract = {Retrosynthesis—the process of identifying a set of reactants to synthesize a target molecule—is of vital importance to material design and drug discovery. Existing machine learning approaches based...},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Quantitative Biology - Quantitative Methods},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6VAT2YKH\\Sun et al. - 2020 - Energy-based View of Retrosynthesis.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\CRSFPMRE\\Sun et al. - 2020 - Energy-based View of Retrosynthesis.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DSNUUEUP\\Sun et al. - 2020 - Energy-based View of Retrosynthesis.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\EUE3U7YL\\2007.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\Y233JA8R\\forum.html}
}

@article{sunWhy90Clinical2022,
  title = {Why 90\% of Clinical Drug Development Fails and How to Improve It?},
  author = {Sun, Duxin and Gao, Wei and Hu, Hongxiang and Zhou, Simon},
  date = {2022-07},
  journaltitle = {Acta Pharm Sin B},
  volume = {12},
  number = {7},
  eprint = {35865092},
  eprinttype = {pmid},
  pages = {3049--3062},
  issn = {2211-3835},
  doi = {10.1016/j.apsb.2022.02.002},
  abstract = {Ninety percent of clinical drug development fails despite implementation of many successful strategies, which raised the question whether certain aspects in target validation and drug optimization are overlooked? Current drug optimization overly emphasizes potency/specificity using structure‒activity-relationship (SAR) but overlooks tissue exposure/selectivity in disease/normal tissues using structure‒tissue exposure/selectivity-relationship (STR), which may mislead the drug candidate selection and impact the balance of clinical dose/efficacy/toxicity. We propose structure‒tissue exposure/selectivity-activity relationship (STAR) to improve drug optimization, which classifies drug candidates based on drug's potency/selectivity, tissue exposure/selectivity, and required dose for balancing clinical efficacy/toxicity. Class I drugs have high specificity/potency and high tissue exposure/selectivity, which needs low dose to achieve superior clinical efficacy/safety with high success rate. Class II drugs have high specificity/potency and low tissue exposure/selectivity, which requires high dose to achieve clinical efficacy with high toxicity and needs to be cautiously evaluated. Class III drugs have relatively low (adequate) specificity/potency but high tissue exposure/selectivity, which requires low dose to achieve clinical efficacy with manageable toxicity but are often overlooked. Class IV drugs have low specificity/potency and low tissue exposure/selectivity, which achieves inadequate efficacy/safety, and should be terminated early. STAR may improve drug optimization and clinical studies for the success of clinical drug development.},
  langid = {english},
  pmcid = {PMC9293739},
  keywords = {Clinical trial,Drug development,Drug optimization,Structure‒tissue exposure/selectivity relationship (STR),Structure‒tissue exposure/selectivity–activity relationship (STAR)}
}

@inproceedings{sutskeverImportanceInitializationMomentum2013,
  title = {On the Importance of Initialization and Momentum in Deep Learning},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 28},
  author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  date = {2013-06-16},
  series = {{{ICML}}'13},
  pages = {III-1139--III-1147},
  publisher = {JMLR.org},
  location = {Atlanta, GA, USA},
  abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.}
}

@unpublished{sutskeverSequenceSequenceLearning2014,
  ids = {sutskever2014sequence},
  title = {Sequence to {{Sequence Learning}} with {{Neural Networks}}},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  date = {2014-09-10},
  eprint = {1409.3215},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1409.3215},
  urldate = {2018-07-02},
  abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\28KIAJ6Z\\Sutskever et al_2014_Sequence to Sequence Learning with Neural Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\3ZQ7CUXR\\Sutskever et al_2014_Sequence to Sequence Learning with Neural Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GJA6N5E6\\1409.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\S5BXVZ6W\\1409.html}
}

@book{suttonIntroductionReinforcementLearning1998,
  title = {Introduction to {{Reinforcement Learning}}},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {1998},
  edition = {1},
  publisher = {MIT Press},
  location = {Cambridge, MA, USA},
  abstract = {From the Publisher:In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.},
  isbn = {978-0-262-19398-6},
  file = {C:\Users\USEBPERP\Zotero\storage\X5CJF8E8\Sutton_Barto_1998_Introduction to Reinforcement Learning.pdf}
}

@article{suttonLearningPredictMethods1988,
  title = {Learning to Predict by the Methods of Temporal Differences},
  author = {Sutton, Richard S.},
  date = {1988-08-01},
  journaltitle = {Mach Learn},
  volume = {3},
  number = {1},
  pages = {9--44},
  issn = {1573-0565},
  doi = {10.1007/BF00115009},
  url = {https://doi.org/10.1007/BF00115009},
  urldate = {2019-03-04},
  abstract = {This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.},
  langid = {english},
  keywords = {connectionism,credit assignment,evaluation functions,Incremental learning,prediction},
  file = {C:\Users\USEBPERP\Zotero\storage\C7SVNMWH\Sutton_1988_Learning to predict by the methods of temporal differences.pdf}
}

@incollection{suttonPolicyGradientMethods2000,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 12},
  author = {Sutton, Richard S and McAllester, David A. and Singh, Satinder P. and Mansour, Yishay},
  editor = {Solla, S. A. and Leen, T. K. and Müller, K.},
  date = {2000},
  pages = {1057--1063},
  publisher = {MIT Press},
  url = {http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf},
  urldate = {2019-02-14},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4Y8WAUDI\\Sutton et al_2000_Policy Gradient Methods for Reinforcement Learning with Function Approximation.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\R7T4QAZE\\1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.html}
}

@article{svenssonConformalRegressionQuantitative2018,
  title = {Conformal {{Regression}} for {{Quantitative Structure}}–{{Activity Relationship Modeling}}—{{Quantifying Prediction Uncertainty}}},
  author = {Svensson, Fredrik and Aniceto, Natalia and Norinder, Ulf and Cortes-Ciriano, Isidro and Spjuth, Ola and Carlsson, Lars and Bender, Andreas},
  date = {2018-05-29},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {58},
  number = {5},
  pages = {1132--1140},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.8b00054},
  url = {https://doi.org/10.1021/acs.jcim.8b00054},
  urldate = {2019-04-23},
  abstract = {Making predictions with an associated confidence is highly desirable as it facilitates decision making and resource prioritization. Conformal regression is a machine learning framework that allows the user to define the required confidence and delivers predictions that are guaranteed to be correct to the selected extent. In this study, we apply conformal regression to model molecular properties and bioactivity values and investigate different ways to scale the resultant prediction intervals to create as efficient (i.e., narrow) regressors as possible. Different algorithms to estimate the prediction uncertainty were used to normalize the prediction ranges, and the different approaches were evaluated on 29 publicly available data sets. Our results show that the most efficient conformal regressors are obtained when using the natural exponential of the ensemble standard deviation from the underlying random forest to scale the prediction intervals, but other approaches were almost as efficient. This approach afforded an average prediction range of 1.65 pIC50 units at the 80\% confidence level when applied to bioactivity modeling. The choice of nonconformity function has a pronounced impact on the average prediction range with a difference of close to one log unit in bioactivity between the tightest and widest prediction range. Overall, conformal regression is a robust approach to generate bioactivity predictions with associated confidence.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\9S8B2JF2\\Svensson et al_2018_Conformal Regression for Quantitative Structure–Activity Relationship.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\G2JEDDL9\\acs.jcim.html}
}

@unpublished{svozilApologyMoney2011,
  title = {An {{Apology}} for {{Money}}},
  author = {Svozil, Karl},
  date = {2011-02-16},
  eprint = {0811.3130},
  eprinttype = {arxiv},
  eprintclass = {physics, q-fin},
  url = {http://arxiv.org/abs/0811.3130},
  urldate = {2020-08-05},
  abstract = {This review is about the convenience, the benefits, as well as the destructive capacities of money. It deals with various aspects of money creation, with its value, and its appropriation. All sorts of money tend to get corrupted by eventually creating too much of them. In the long run, this renders money worthless and deprives people holding it. This misuse of money creation is inevitable and should come as no surprise. Abusive money creation comes in various forms. In the present fiat money system "suspended in free thought" and sustained merely by our belief in and our conditioning to it, money is conveniently created out of "thin air" by excessive government spending and speculative credit creation. Alas, any too tight money supply could ruin an economy by inviting all sorts of unfriendly takeovers, including wars or competition. Therefore the ambivalence of money as benefactor and destroyer should be accepted as destiny.},
  keywords = {Physics - Physics and Society,Quantitative Finance - General Finance},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\5IRNT6GT\\Svozil_2011_An Apology for Money.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QTY8WZ2R\\0811.html}
}

@article{swamidassBoundsAlgorithmsFast2007,
  title = {Bounds and {{Algorithms}} for {{Fast Exact Searches}} of {{Chemical Fingerprints}} in {{Linear}} and {{Sublinear Time}}},
  author = {Swamidass, S. Joshua and Baldi, Pierre},
  date = {2007-03-01},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {47},
  number = {2},
  pages = {302--317},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/ci600358f},
  url = {https://doi.org/10.1021/ci600358f},
  urldate = {2021-05-12},
  abstract = {Chemical fingerprints are used to represent chemical molecules by recording the presence or absence, or by counting the number of occurrences, of particular features or substructures, such as labeled paths in the 2D graph of bonds, of the corresponding molecule. These fingerprint vectors are used to search large databases of small molecules, currently containing millions of entries, using various similarity measures, such as the Tanimoto or Tversky's measures and their variants. Here, we derive simple bounds on these similarity measures and show how these bounds can be used to considerably reduce the subset of molecules that need to be searched. We consider both the case of single-molecule and multiple-molecule queries, as well as queries based on fixed similarity thresholds or aimed at retrieving the top K hits. We study the speedup as a function of query size and distribution, fingerprint length, similarity threshold, and database size |D| and derive analytical formulas that are in excellent agreement with empirical values. The theoretical considerations and experiments show that this approach can provide linear speedups of one or more orders of magnitude in the case of searches with a fixed threshold, and achieve sublinear speedups in the range of O(|D|0.6) for the top K hits in current large databases. This pruning approach yields subsecond search times across the 5 million compounds in the ChemDB database, without any loss of accuracy.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NBVTKJD4\\Swamidass and Baldi - 2007 - Bounds and Algorithms for Fast Exact Searches of C.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8FKICTVM\\ci600358f.html}
}

@article{swinneyHowWereNew2011,
  title = {How Were New Medicines Discovered?},
  author = {Swinney, David C. and Anthony, Jason},
  date = {2011-07},
  journaltitle = {Nature Reviews Drug Discovery},
  volume = {10},
  number = {7},
  pages = {507--519},
  issn = {1474-1784},
  doi = {10.1038/nrd3480},
  url = {https://www.nature.com/articles/nrd3480},
  urldate = {2018-05-20},
  abstract = {Preclinical strategies that are used to identify potential drug candidates include target-based screening, phenotypic screening, modification of natural substances and biologic-based approaches. To investigate whether some strategies have been more successful than others in the discovery of new drugs, we analysed the discovery strategies and the molecular mechanism of action (MMOA) for new molecular entities and new biologics that were approved by the US Food and Drug Administration between 1999 and 2008. Out of the 259 agents that were approved, 75 were first-in-class drugs with new MMOAs, and out of these, 50 (67\%) were small molecules and 25 (33\%) were biologics. The results also show that the contribution of phenotypic screening to the discovery of first-in-class small-molecule drugs exceeded that of target-based approaches — with 28 and 17 of these drugs coming from the two approaches, respectively — in an era in which the major focus was on target-based approaches. We postulate that a target-centric approach for first-in-class drugs, without consideration of an optimal MMOA, may contribute to the current high attrition rates and low productivity in pharmaceutical research and development.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\8A5BGRPK\\Swinney_Anthony_2011_How were new medicines discovered.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SHRP5FZM\\nrd3480.html}
}

@article{sydowAdvancesChallengesComputational2019,
  title = {Advances and {{Challenges}} in {{Computational Target Prediction}}},
  author = {Sydow, Dominique and Burggraaff, Lindsey and Szengel, Angelika and family=Vlijmen, given=Herman W. T., prefix=van, useprefix=true and IJzerman, Adriaan P. and family=Westen, given=Gerard J. P., prefix=van, useprefix=true and Volkamer, Andrea},
  date = {2019-02-28},
  journaltitle = {J. Chem. Inf. Model.},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.8b00832},
  url = {https://doi.org/10.1021/acs.jcim.8b00832},
  urldate = {2019-03-21},
  abstract = {Target deconvolution is a vital initial step in preclinical drug development to determine research focus and strategy. In this respect, computational target prediction is used to identify the most probable targets of an orphan ligand or the most similar targets to a protein under investigation. Applications range from the fundamental analysis of the mode-of-action over polypharmacology or adverse effect predictions to drug repositioning. Here, we provide a review on published ligand- and target-based as well as hybrid approaches for computational target prediction, together with current limitations and future directions.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\KDAQMEYC\\Sydow et al_2019_Advances and Challenges in Computational Target Prediction.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\9SPPDM6Z\\acs.jcim.html}
}

@online{SynthesizabilityMoleculesProposed,
  title = {The {{Synthesizability}} of {{Molecules Proposed}} by {{Generative Models}} | {{Journal}} of {{Chemical Information}} and {{Modeling}}},
  url = {https://pubs.acs.org/doi/10.1021/acs.jcim.0c00174},
  urldate = {2023-10-10},
  file = {C:\Users\USEBPERP\Zotero\storage\DZ36W6LR\acs.jcim.html}
}

@unpublished{szegedyIntriguingPropertiesNeural2014,
  ids = {bruna2013intriguing},
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  date = {2014-02-19},
  eprint = {1312.6199},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1312.6199},
  urldate = {2020-04-09},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\HMQRTTWE\\Szegedy et al_2014_Intriguing properties of neural networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\M2RNLZPH\\forum.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZBH5WIYI\\1312.html}
}

@article{szymkucComputerAssistedSyntheticPlanning2016,
  ids = {szymkucComputerAssistedSyntheticPlanning2016a},
  title = {Computer-{{Assisted Synthetic Planning}}: {{The End}} of the {{Beginning}}},
  shorttitle = {Computer-{{Assisted Synthetic Planning}}},
  author = {Szymkuć, Sara and Gajewska, Ewa P. and Klucznik, Tomasz and Molga, Karol and Dittwald, Piotr and Startek, Michał and Bajczyk, Michał and Grzybowski, Bartosz A.},
  date = {2016},
  journaltitle = {Angewandte Chemie International Edition},
  volume = {55},
  number = {20},
  pages = {5904--5937},
  issn = {1521-3773},
  doi = {10.1002/anie.201506101},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/anie.201506101},
  urldate = {2019-10-01},
  abstract = {Exactly half a century has passed since the launch of the first documented research project (1965 Dendral) on computer-assisted organic synthesis. Many more programs were created in the 1970s and 1980s but the enthusiasm of these pioneering days had largely dissipated by the 2000s, and the challenge of teaching the computer how to plan organic syntheses earned itself the reputation of a “mission impossible”. This is quite curious given that, in the meantime, computers have “learned” many other skills that had been considered exclusive domains of human intellect and creativity—for example, machines can nowadays play chess better than human world champions and they can compose classical music pleasant to the human ear. Although there have been no similar feats in organic synthesis, this Review argues that to concede defeat would be premature. Indeed, bringing together the combination of modern computational power and algorithms from graph/network theory, chemical rules (with full stereo- and regiochemistry) coded in appropriate formats, and the elements of quantum mechanics, the machine can finally be “taught” how to plan syntheses of non-trivial organic molecules in a matter of seconds to minutes. The Review begins with an overview of some basic theoretical concepts essential for the big-data analysis of chemical syntheses. It progresses to the problem of optimizing pathways involving known reactions. It culminates with discussion of algorithms that allow for a completely de novo and fully automated design of syntheses leading to relatively complex targets, including those that have not been made before. Of course, there are still things to be improved, but computers are finally becoming relevant and helpful to the practice of organic-synthetic planning. Paraphrasing Churchill's famous words after the Allies’ first major victory over the Axis forces in Africa, it is not the end, it is not even the beginning of the end, but it is the end of the beginning for the computer-assisted synthesis planning. The machine is here to stay.},
  langid = {english},
  keywords = {algorithms,Chematica,computers,networks,organic synthesis},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\WLE565HF\\Szymkuć et al. - 2016 - Computer-Assisted Synthetic Planning The End of t.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\82TTSPB9\\anie.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\AS62KPIC\\anie.html}
}

@unpublished{tagasovskaFrequentistUncertaintyEstimates2018,
  title = {Frequentist Uncertainty Estimates for Deep Learning},
  author = {Tagasovska, Natasa and Lopez-Paz, David},
  date = {2018-11-02},
  eprint = {1811.00908},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1811.00908},
  urldate = {2019-04-15},
  abstract = {We provide frequentist estimates of aleatoric and epistemic uncertainty for deep neural networks. To estimate aleatoric uncertainty we propose simultaneous quantile regression, a loss function to learn all the conditional quantiles of a given target variable. These quantiles can be used to compute well-calibrated prediction intervals. To estimate epistemic uncertainty we propose orthonormal certificates, a collection of diverse non-constant functions that map all training samples to zero. These certificates map out-of-distribution examples to non-zero values, signaling high epistemic uncertainty. Our uncertainty estimators are computationally attractive, since they do not require training an ensemble of deep models. Throughout a variety of real-world datasets and tasks, we show the state-of-the-art performance of our uncertainty estimators.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\TT5HNFBM\\Tagasovska_Lopez-Paz_2018_Frequentist uncertainty estimates for deep learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\43WLMPUD\\1811.html}
}

@article{TakeItTrust2009,
  title = {Take It on Trust?},
  date = {2009-09},
  journaltitle = {Nature Phys},
  volume = {5},
  number = {9},
  pages = {613--613},
  publisher = {Nature Publishing Group},
  issn = {1745-2481},
  doi = {10.1038/nphys1378},
  url = {https://www.nature.com/articles/nphys1378},
  urldate = {2022-07-15},
  abstract = {Public trust in science is vital. But how do we ensure trust without imposing authority?},
  issue = {9},
  langid = {english},
  keywords = {Atomic,Classical and Continuum Physics,Complex Systems,Condensed Matter Physics,general,Mathematical and Computational Physics,Molecular,Optical and Plasma Physics,Physics,Theoretical},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\46J5CMV2\\2009 - Take it on trust.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HYNTL5WH\\nphys1378.html}
}

@unpublished{tanEfficientNetRethinkingModel2019,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  date = {2019-05-28},
  eprint = {1905.11946},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.11946},
  urldate = {2019-06-03},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NQ3MSL6F\\Tan_Le_2019_EfficientNet.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8ICJPPFB\\1905.html}
}

@inproceedings{tatbulPrecisionRecallTime2018,
  title = {Precision and {{Recall}} for {{Time Series}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tatbul, Nesime and Lee, Tae Jun and Zdonik, Stan and Alam, Mejbah and Gottschlich, Justin},
  date = {2018},
  volume = {31},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper/2018/hash/8f468c873a32bb0619eaeb2050ba45d1-Abstract.html},
  urldate = {2023-03-16},
  abstract = {Classical anomaly detection is principally concerned with point-based anomalies, those anomalies that occur at a single point in time. Yet, many real-world anomalies are range-based, meaning they occur over a period of time. Motivated by this observation, we present a new mathematical model to evaluate the accuracy of time series classification algorithms. Our model expands the well-known Precision and Recall metrics to measure ranges, while simultaneously enabling customization support for domain-specific preferences.},
  file = {C:\Users\USEBPERP\Zotero\storage\EAQ28HBY\Tatbul et al. - 2018 - Precision and Recall for Time Series.pdf}
}

@article{taylorSimulationAnalysisExperimental1995,
  title = {Simulation {{Analysis}} of {{Experimental Design Strategies}} for {{Screening Random Compounds}} as {{Potential New Drugs}} and {{Agrochemicals}}},
  author = {Taylor, Robin},
  date = {1995-01-01},
  journaltitle = {J. Chem. Inf. Comput. Sci.},
  volume = {35},
  number = {1},
  pages = {59--67},
  issn = {0095-2338},
  doi = {10.1021/ci00023a009},
  url = {https://pubs.acs.org/doi/abs/10.1021/ci00023a009},
  urldate = {2022-08-23},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\6BJZYQD2\Taylor - 1995 - Simulation Analysis of Experimental Design Strateg.pdf}
}

@article{tetkoStateoftheartAugmentedNLP2020,
  ids = {tetko2020stateofthearta},
  title = {State-of-the-Art Augmented {{NLP}} Transformer Models for Direct and Single-Step Retrosynthesis},
  author = {Tetko, Igor V. and Karpov, Pavel and Van Deursen, Ruud and Godin, Guillaume},
  date = {2020-11-04},
  journaltitle = {Nature Communications},
  volume = {11},
  number = {1},
  eprint = {2003.02804},
  eprinttype = {arxiv},
  pages = {5575},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-19266-y},
  url = {https://www.nature.com/articles/s41467-020-19266-y},
  urldate = {2021-05-04},
  abstract = {We investigated the effect of different training scenarios on predicting the (retro)synthesis of chemical compounds using text-like representation of chemical reactions (SMILES) and Natural Language Processing (NLP) neural network Transformer architecture. We showed that data augmentation, which is a powerful method used in image processing, eliminated the effect of data memorization by neural networks and improved their performance for prediction of new sequences. This effect was observed when augmentation was used simultaneously for input and the target data simultaneously. The top-5 accuracy was 84.8\% for the prediction of the largest fragment (thus identifying principal transformation for classical retro-synthesis) for the USPTO-50k test dataset, and was achieved by a combination of SMILES augmentation and a beam search algorithm. The same approach provided significantly better results for the prediction of direct reactions from the single-step USPTO-MIT test set. Our model achieved 90.6\% top-1 and 96.1\% top-5 accuracy for its challenging mixed set and 97\% top-5 accuracy for the USPTO-MIT separated set. It also significantly improved results for USPTO-full set single-step retrosynthesis for both top-1 and top-10 accuracies. The appearance frequency of the most abundantly generated SMILES was well correlated with the prediction outcome and can be used as a measure of the quality of reaction prediction.},
  issue = {1},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\67DK6WKB\\Tetko et al. - 2020 - State-of-the-art augmented NLP transformer models .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\J2N2MHXU\\s41467-020-19266-y.html}
}

@article{thakkarArtificialIntelligenceAutomation2021,
  title = {Artificial Intelligence and Automation in Computer Aided Synthesis Planning},
  author = {Thakkar, Amol and Johansson, Simon and Jorner, Kjell and Buttar, David and Reymond, Jean-Louis and Engkvist, Ola},
  date = {2021},
  journaltitle = {Reaction Chemistry \& Engineering},
  volume = {6},
  number = {1},
  pages = {27--51},
  publisher = {Royal Society of Chemistry},
  doi = {10.1039/D0RE00340A},
  url = {https://pubs.rsc.org/en/content/articlelanding/2021/re/d0re00340a},
  urldate = {2021-01-22},
  issue = {1},
  langid = {english}
}

@article{thakkarDatasetsTheirInfluence2019,
  title = {Datasets and {{Their Influence}} on the {{Development}} of {{Computer Assisted Synthesis Planning Tools}} in the {{Pharmaceutical Domain}}},
  author = {Thakkar, Amol and Kogej, Thierry and Reymond, Jean-Louis and Engkvist, Ola and Bjerrum, Esben Jannik},
  date = {2019-09-27},
  doi = {10.26434/chemrxiv.9897692.v1},
  url = {https://chemrxiv.org/articles/Datasets_and_Their_Influence_on_the_Development_of_Computer_Assisted_Synthesis_Planning_Tools_in_the_Pharmaceutical_Domain/9897692},
  urldate = {2019-10-07},
  abstract = {Computer Assisted Synthesis Planning (CASP) has gained considerable interest as of late. Herein we investigate a template-based retrosynthetic planning tool, trained on a variety of datasets consisting of up to 17.5 million reactions. We demonstrate that models trained on datasets such as internal Electronic Laboratory Notebooks (ELN), and the publicly available United States Patent Office (USPTO) extracts, are sufficient for the prediction of full synthetic routes to compounds of interest in medicinal chemistry. As such we have assessed the models on 1,731 compounds from 41 virtual libraries for which experimental results were known. Furthermore, we show that accuracy is a misleading metric for assessment of the ‘filter network’, and propose that the number of successfully applied templates, in conjunction with the overall ability to generate full synthetic routes be examined instead. To this end we found that the specificity of the templates comes at the cost of generalizability, and overall model performance. This is supplemented by a comparison of the underlying datasets and their corresponding models.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\UUUQMWD7\Thakkar et al_2019_Datasets and Their Influence on the Development of Computer Assisted Synthesis.pdf}
}

@unpublished{theisNoteEvaluationGenerative2015,
  title = {A Note on the Evaluation of Generative Models},
  author = {Theis, Lucas and family=Oord, given=Aäron, prefix=van den, useprefix=false and Bethge, Matthias},
  date = {2015-11-05},
  eprint = {1511.01844},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1511.01844},
  urldate = {2018-06-03},
  abstract = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria---average log-likelihood, Parzen window estimates, and visual fidelity of samples---are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
  keywords = {Computer Science - Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\HRG6NQQD\\Theis et al_2015_A note on the evaluation of generative models.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\9WFZ48FL\\1511.html}
}

@video{ThinkingMachineArtificial2010,
  entrysubtype = {video},
  title = {The {{Thinking Machine}} ({{Artificial Intelligence}} in the 1960s)},
  date = {2010-05-31},
  url = {https://www.youtube.com/watch?v=aygSMgK3BEM},
  urldate = {2020-09-29},
  abstract = {Can machines really think? Here is a series of interviews to some of the AI pioneers, Jerome Wiesner, Oliver Selfridge, and Claude Shannon. A view at the future of computer intelligence from back then...}
}

@video{ThinkingMachineMIT2019,
  entrysubtype = {video},
  title = {The {{Thinking Machine}} - {{MIT}} 1961},
  date = {2019-01-21},
  url = {https://www.youtube.com/watch?v=5YBIrc-6G-0},
  urldate = {2020-09-29},
  abstract = {Artificial intelligence began at MIT in the 1950s. This video, filmed in 1961, is a documentary of sorts that explores machine intelligence of the era.}
}

@article{thomasAugmentedHillClimbIncreases2022,
  title = {Augmented {{Hill-Climb}} Increases Reinforcement Learning Efficiency for Language-Based de Novo Molecule Generation},
  author = {Thomas, Morgan and O’Boyle, Noel M. and Bender, Andreas and family=Graaf, given=Chris, prefix=de, useprefix=true},
  date = {2022-10-03},
  journaltitle = {Journal of Cheminformatics},
  volume = {14},
  number = {1},
  pages = {68},
  issn = {1758-2946},
  doi = {10.1186/s13321-022-00646-z},
  url = {https://doi.org/10.1186/s13321-022-00646-z},
  urldate = {2023-09-04},
  abstract = {A plethora of AI-based techniques now exists to conduct de novo molecule generation that can devise molecules conditioned towards a particular endpoint in the context of drug design. One popular approach is using reinforcement learning to update a recurrent neural network or language-based de novo molecule generator. However, reinforcement learning can be inefficient, sometimes requiring up to 105 molecules to be sampled to optimize more complex objectives, which poses a limitation when using computationally expensive scoring functions like docking or computer-aided synthesis planning models. In this work, we propose a reinforcement learning strategy called Augmented Hill-Climb based on a simple, hypothesis-driven hybrid between REINVENT and Hill-Climb that improves sample-efficiency by addressing the limitations of both currently used strategies. We compare its ability to optimize several docking tasks with REINVENT and benchmark this strategy against other commonly used reinforcement learning strategies including REINFORCE, REINVENT (version 1 and 2), Hill-Climb and best agent reminder. We find that optimization ability is improved\,\textasciitilde\,1.5-fold and sample-efficiency is improved\,\textasciitilde\,45-fold compared to REINVENT while still delivering appealing chemistry as output. Diversity filters were used, and their parameters were tuned to overcome observed failure modes that take advantage of certain diversity filter configurations. We find that Augmented Hill-Climb outperforms the other reinforcement learning strategies used on six tasks, especially in the early stages of training or for more difficult objectives. Lastly, we show improved performance not only on recurrent neural networks but also on a reinforcement learning stabilized transformer architecture. Overall, we show that Augmented~Hill-Climb improves sample-efficiency for language-based de novo molecule generation conditioning via reinforcement learning, compared to the current state-of-the-art. This makes more computationally expensive scoring functions, such as docking, more accessible on a relevant timescale.},
  keywords = {AI,Artificial intelligence,De novo design,Deep learning,Generative models,Hill-Climb,Molecular docking,Recurrent neural network,REINFORCE,Reinforcement learning,REINVENT,SBDD,Structure-based drug design},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\IQXCRCSZ\\Thomas et al. - 2022 - Augmented Hill-Climb increases reinforcement learn.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\Z44RXSD5\\s13321-022-00646-z.html}
}

@inproceedings{thomasBiasNaturalActorCritic2014,
  title = {Bias in {{Natural Actor-Critic Algorithms}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Thomas, Philip},
  date = {2014-01-27},
  pages = {441--448},
  url = {http://proceedings.mlr.press/v32/thomas14.html},
  urldate = {2019-03-01},
  abstract = {We show that several popular discounted reward natural actor-critics, including the popular NAC-LSTD and eNAC algorithms, do not generate unbiased estimates of the natural policy gradient as claime...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\5CEJPG8E\\Thomas_2014_Bias in Natural Actor-Critic Algorithms.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\BYE33Q68\\thomas14.html}
}

@article{thomasComparisonStructureLigandbased2021,
  title = {Comparison of Structure- and Ligand-Based Scoring Functions for Deep Generative Models: A {{GPCR}} Case Study},
  shorttitle = {Comparison of Structure- and Ligand-Based Scoring Functions for Deep Generative Models},
  author = {Thomas, Morgan and Smith, Robert T. and O’Boyle, Noel M. and family=Graaf, given=Chris, prefix=de, useprefix=true and Bender, Andreas},
  date = {2021-05-13},
  journaltitle = {Journal of Cheminformatics},
  volume = {13},
  number = {1},
  pages = {39},
  issn = {1758-2946},
  doi = {10.1186/s13321-021-00516-0},
  url = {https://doi.org/10.1186/s13321-021-00516-0},
  urldate = {2022-05-30},
  abstract = {Deep generative models have shown the ability to devise both valid and novel chemistry, which could significantly accelerate the identification of bioactive compounds. Many current models, however, use molecular descriptors or ligand-based predictive methods to guide molecule generation towards a desirable property space. This restricts their application to relatively data-rich targets, neglecting those where little data is available to sufficiently train a predictor. Moreover, ligand-based approaches often bias molecule generation towards previously established chemical space, thereby limiting their ability to identify truly novel chemotypes. In this work, we assess the ability of using molecular docking via Glide—a structure-based approach—as a scoring function to guide the deep generative model REINVENT and compare model performance and behaviour to a ligand-based scoring function. Additionally, we modify the previously published MOSES benchmarking dataset to remove any induced bias towards non-protonatable groups. We also propose a new metric to measure dataset diversity, which is less confounded by the distribution of heavy atom count than the commonly used internal diversity metric. With respect to the main findings, we found that when optimizing the docking score against DRD2, the model improves predicted ligand affinity beyond that of known DRD2 active molecules. In addition, generated molecules occupy complementary chemical and physicochemical space compared to the ligand-based approach, and novel physicochemical space compared to known DRD2 active molecules. Furthermore, the structure-based approach learns to generate molecules that satisfy crucial residue interactions, which is information only available when taking protein structure into account. Overall, this work demonstrates the advantage of using molecular docking to guide de novo molecule generation over ligand-based predictors with respect to predicted affinity, novelty, and the ability to identify key interactions between ligand and protein target. Practically, this approach has applications in early hit generation campaigns to enrich a virtual library towards a particular target, and also in novelty-focused projects, where de novo molecule generation either has no prior ligand knowledge available or should not be biased by it.},
  keywords = {AI,Artificial Intelligence,De novo design,Deep learning,Generative models,LBDD,Ligand-based drug design,Molecular docking,QSAR,Quantitative structure–activity relationship,Recurrent neural network,Reinforcement learning,SBDD,Structure-based drug design},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BCXCXFKB\\Thomas et al. - 2021 - Comparison of structure- and ligand-based scoring .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\45RU4AQP\\s13321-021-00516-0.html}
}

@online{thomasReevaluatingSampleEfficiency2022,
  title = {Re-Evaluating Sample Efficiency in de Novo Molecule Generation},
  author = {Thomas, Morgan and O'Boyle, Noel M. and Bender, Andreas and De Graaf, Chris},
  date = {2022-12-01},
  eprint = {2212.01385},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2212.01385},
  urldate = {2023-09-04},
  abstract = {De novo molecule generation can suffer from data inefficiency; requiring large amounts of training data or many sampled data points to conduct objective optimization. The latter is a particular disadvantage when combining deep generative models with computationally expensive molecule scoring functions (a.k.a. oracles) commonly used in computer-aided drug design. Recent works have therefore focused on methods to improve sample efficiency in the context of de novo molecule drug design, or to benchmark it. In this work, we discuss and adapt a recent sample efficiency benchmark to better reflect realistic goals also with respect to the quality of chemistry generated, which must always be considered in the context of smallmolecule drug design; we then re-evaluate all benchmarked generative models. We find that accounting for molecular weight and LogP with respect to the training data, and the diversity of chemistry proposed, re-orders the ranking of generative models. In addition, we benchmark a recently proposed method to improve sample efficiency (Augmented Hill-Climb) and found it ranked top when considering both sample efficiency and chemistry of molecules generated. Continual improvements in sample efficiency and chemical desirability enable more routine integration of computationally expensive scoring functions on a more realistic timescale.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computational Engineering Finance and Science,Quantitative Biology - Biomolecules},
  file = {C:\Users\USEBPERP\Zotero\storage\Y2PZQV4W\Thomas et al. - 2022 - Re-evaluating sample efficiency in de novo molecul.pdf}
}

@article{thudumuComprehensiveSurveyAnomaly2020,
  title = {A Comprehensive Survey of Anomaly Detection Techniques for High Dimensional Big Data},
  author = {Thudumu, Srikanth and Branch, Philip and Jin, Jiong and Singh, Jugdutt (Jack)},
  date = {2020-07-02},
  journaltitle = {Journal of Big Data},
  volume = {7},
  number = {1},
  pages = {42},
  issn = {2196-1115},
  doi = {10.1186/s40537-020-00320-x},
  url = {https://doi.org/10.1186/s40537-020-00320-x},
  urldate = {2023-01-19},
  abstract = {Anomaly detection in high dimensional data is becoming a fundamental research problem that has various applications in the real world. However, many existing anomaly detection techniques fail to retain sufficient accuracy due to so-called “big data” characterised by high-volume, and high-velocity data generated by variety of sources. This phenomenon of having both problems together can be referred to the “curse of big dimensionality,” that affect existing techniques in terms of both performance and accuracy. To address this gap and to understand the core problem, it is necessary to identify the unique challenges brought by the anomaly detection with both high dimensionality and big data problems. Hence, this survey aims to document the state of anomaly detection in high dimensional big data by representing the unique challenges using a triangular model of vertices: the problem (big dimensionality), techniques/algorithms (anomaly detection), and tools (big data applications/frameworks). Authors’ work that fall directly into any of the vertices or closely related to them are taken into consideration for review. Furthermore, the limitations of traditional approaches and current strategies of high dimensional data are discussed along with recent techniques and applications on big data required for the optimization of anomaly detection.},
  keywords = {Anomaly detection,Big data,Big dimensionality,Big dimensionality tools,High dimensionality,The curse of big dimensionality,The curse of dimensionality},
  file = {C:\Users\USEBPERP\Zotero\storage\NG6FGFQZ\Thudumu et al. - 2020 - A comprehensive survey of anomaly detection techni.pdf}
}

@unpublished{tianContrastiveMultiviewCoding2020,
  title = {Contrastive {{Multiview Coding}}},
  author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  date = {2020-12-18},
  eprint = {1906.05849},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1906.05849},
  urldate = {2021-04-08},
  abstract = {Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a "dog" can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Our approach achieves state-of-the-art results on image and video unsupervised learning benchmarks. Code is released at: http://github.com/HobbitLong/CMC/.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ZAVS3QM8\\Tian et al. - 2020 - Contrastive Multiview Coding.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\5UCR5HKZ\\1906.html}
}

@unpublished{tianLuckMattersUnderstanding2019,
  title = {Luck {{Matters}}: {{Understanding Training Dynamics}} of {{Deep ReLU Networks}}},
  shorttitle = {Luck {{Matters}}},
  author = {Tian, Yuandong and Jiang, Tina and Gong, Qucheng and Morcos, Ari},
  date = {2019-05-31},
  eprint = {1905.13405},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.13405},
  urldate = {2019-06-12},
  abstract = {We analyze the dynamics of training deep ReLU networks and their implications on generalization capability. Using a teacher-student setting, we discovered a novel relationship between the gradient received by hidden student nodes and the activations of teacher nodes for deep ReLU networks. With this relationship and the assumption of small overlapping teacher node activations, we prove that (1) student nodes whose weights are initialized to be close to teacher nodes converge to them at a faster rate, and (2) in over-parameterized regimes and 2-layer case, while a small set of lucky nodes do converge to the teacher nodes, the fan-out weights of other nodes converge to zero. This framework provides insight into multiple puzzling phenomena in deep learning like over-parameterization, implicit regularization, lottery tickets, etc. We verify our assumption by showing that the majority of BatchNorm biases of pre-trained VGG11/16 models are negative. Experiments on (1) random deep teacher networks with Gaussian inputs, (2) teacher network pre-trained on CIFAR-10 and (3) extensive ablation studies validate our multiple theoretical predictions.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\VHLDZJMM\\Tian et al_2019_Luck Matters.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\A69GPTU8\\1905.html}
}

@inreference{TimelineWomenLegal2020,
  title = {Timeline of Women's Legal Rights in the {{United States}} (Other than Voting)},
  booktitle = {Wikipedia},
  date = {2020-12-11T05:40:29Z},
  url = {https://en.wikipedia.org/w/index.php?title=Timeline_of_women%27s_legal_rights_in_the_United_States_(other_than_voting)&oldid=993553620},
  urldate = {2020-12-23},
  abstract = {Timeline of women's legal rights in the United States (other than voting) represents formal legal changes and reforms regarding women's rights in the United States. That includes actual law reforms as well as other formal changes, such as reforms through new interpretations of laws by precedents. For such things outside as well as in the United States, see Timeline of women's legal rights (other than voting). The right to vote is exempted from the timeline: for that right, see Timeline of women's suffrage in the United States. The timeline also excludes ideological changes and events within feminism and antifeminism: for that, see Timeline of feminism in the United States.},
  langid = {english},
  annotation = {Page Version ID: 993553620},
  file = {C:\Users\USEBPERP\Zotero\storage\CVCPV345\index.html}
}

@inproceedings{toledanoRealtimeAnomalyDetection2018,
  title = {Real-Time Anomaly Detection System for Time Series at Scale},
  booktitle = {Proceedings of the {{KDD}} 2017: {{Workshop}} on {{Anomaly Detection}} in {{Finance}}},
  author = {Toledano, Meir and Cohen, Ira and Ben-Simhon, Yonatan and Tadeski, Inbal},
  date = {2018-01-07},
  pages = {56--65},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v71/toledano18a.html},
  urldate = {2023-01-19},
  abstract = {This paper describes the design considerations and general outline of an anomaly detection system used by Anodot. We present results of the system on a large set of metrics collected from multiple companies.},
  eventtitle = {{{KDD}} 2017 {{Workshop}} on {{Anomaly Detection}} in {{Finance}}},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\H3T62FVS\Toledano et al. - 2018 - Real-time anomaly detection system for time series.pdf}
}

@unpublished{torabiBehavioralCloningObservation2018,
  title = {Behavioral {{Cloning}} from {{Observation}}},
  author = {Torabi, Faraz and Warnell, Garrett and Stone, Peter},
  date = {2018-05-04},
  eprint = {1805.01954},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1805.01954},
  urldate = {2019-06-06},
  abstract = {Humans often learn how to perform tasks via imitation: they observe others perform a task, and then very quickly infer the appropriate actions to take based on their observations. While extending this paradigm to autonomous agents is a well-studied problem in general, there are two particular aspects that have largely been overlooked: (1) that the learning is done from observation only (i.e., without explicit action information), and (2) that the learning is typically done very quickly. In this work, we propose a two-phase, autonomous imitation learning technique called behavioral cloning from observation (BCO), that aims to provide improved performance with respect to both of these aspects. First, we allow the agent to acquire experience in a self-supervised fashion. This experience is used to develop a model which is then utilized to learn a particular task by observing an expert perform that task without the knowledge of the specific actions taken. We experimentally compare BCO to imitation learning methods, including the state-of-the-art, generative adversarial imitation learning (GAIL) technique, and we show comparable task performance in several different simulation domains while exhibiting increased learning speed after expert trajectories become available.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\J4EBJV6I\\Torabi et al_2018_Behavioral Cloning from Observation.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\IZF62LEQ\\1805.html}
}

@unpublished{tranBayesianLayersModule2018,
  title = {Bayesian {{Layers}}: {{A Module}} for {{Neural Network Uncertainty}}},
  shorttitle = {Bayesian {{Layers}}},
  author = {Tran, Dustin and Dusenberry, Michael W. and family=Wilk, given=Mark, prefix=van der, useprefix=true and Hafner, Danijar},
  date = {2018-12-10},
  eprint = {1812.03973},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1812.03973},
  urldate = {2019-07-25},
  abstract = {We describe Bayesian Layers, a module designed for fast experimentation with neural network uncertainty. It extends neural network libraries with drop-in replacements for common layers. This enables composition via a unified abstraction over deterministic and stochastic functions and allows for scalability via the underlying system. These layers capture uncertainty over weights (Bayesian neural nets), pre-activation units (dropout), activations ("stochastic output layers"), or the function itself (Gaussian processes). They can also be reversible to propagate uncertainty from input to output. We include code examples for common architectures such as Bayesian LSTMs, deep GPs, and flow-based models. As demonstration, we fit a 5-billion parameter "Bayesian Transformer" on 512 TPUv2 cores for uncertainty in machine translation and a Bayesian dynamics model for model-based planning. Finally, we show how Bayesian Layers can be used within the Edward2 probabilistic programming language for probabilistic programs with stochastic processes.},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\J9LVD8ZI\\Tran et al_2018_Bayesian Layers.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\MB5UVBQ6\\1812.html}
}

@unpublished{tranDiscreteFlowsInvertible2019,
  title = {Discrete {{Flows}}: {{Invertible Generative Models}} of {{Discrete Data}}},
  shorttitle = {Discrete {{Flows}}},
  author = {Tran, Dustin and Vafa, Keyon and Agrawal, Kumar Krishna and Dinh, Laurent and Poole, Ben},
  date = {2019-05-24},
  eprint = {1905.10347},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.10347},
  urldate = {2019-05-27},
  abstract = {While normalizing flows have led to significant advances in modeling high-dimensional continuous distributions, their applicability to discrete distributions remains unknown. In this paper, we show that flows can in fact be extended to discrete events---and under a simple change-of-variables formula not requiring log-determinant-Jacobian computations. Discrete flows have numerous applications. We consider two flow architectures: discrete autoregressive flows that enable bidirectionality, allowing, for example, tokens in text to depend on both left-to-right and right-to-left contexts in an exact language model; and discrete bipartite flows that enable efficient non-autoregressive generation as in RealNVP. Empirically, we find that discrete autoregressive flows outperform autoregressive baselines on synthetic discrete distributions, an addition task, and Potts models; and bipartite flows can obtain competitive performance with autoregressive baselines on character-level language modeling for Penn Tree Bank and text8.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\YU4PHX63\\Tran et al_2019_Discrete Flows.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7S7N643Z\\1905.html}
}

@article{trippEVALUATIONFRAMEWORKOBJECTIVE2022,
  title = {{{AN EVALUATION FRAMEWORK FOR THE OBJECTIVE}}},
  author = {Tripp, Austin and Chen, Wenlin and Hernández-Lobato, José Miguel},
  date = {2022},
  pages = {9},
  abstract = {De novo drug design has recently received increasing attention from the machine learning community. It is important that the field is aware of the actual goals and challenges of drug design and the roles that de novo molecule design algorithms could play in accelerating the process, so that algorithms can be evaluated in a way that reflects how they would be applied in real drug design scenarios. In this paper, we propose a framework for critically assessing the merits of benchmarks, and argue that most of the existing de novo drug design benchmark functions are either highly unrealistic or depend upon a surrogate model whose performance is not well characterized. In order for the field to achieve its long-term goals, we recommend that poor benchmarks (especially logP and QED) be deprecated in favour of better benchmarks. We hope that our proposed framework can play a part in developing new de novo drug design benchmarks that are more realistic and ideally incorporate the intrinsic goals of drug design.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\HT5PBCM3\Tripp et al. - 2022 - AN EVALUATION FRAMEWORK FOR THE OBJECTIVE.pdf}
}

@article{trippFreshLookNovo,
  title = {A {{Fresh Look}} at {{De Novo Molecular Design Benchmarks}}},
  author = {Tripp, Austin and Simm, Gregor N C and Hernández-Lobato, José Miguel},
  abstract = {De novo molecular design is a thriving research area in machine learning (ML) that lacks ubiquitous, high-quality, standardized benchmark tasks. Many existing benchmark tasks do not precisely specify a training dataset or an evaluation budget, which is problematic as they can significantly affect the performance of ML algorithms. This work elucidates the effect of dataset sizes and experimental budgets on established molecular optimization methods through a comprehensive evaluation with 11 selected benchmark tasks. We observe that the dataset size and budget significantly impact all methods’ performance and relative ranking, suggesting that a meaningful comparison requires more than a single benchmark setup. Our results also highlight the relative difficulty of benchmarks, implying that logP and QED are poor objectives. We end by offering guidance to researchers on their choice of experiments.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\PJJ8H7MX\\Tripp et al. - A Fresh Look at De Novo Molecular Design Benchmark.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\Y72IPM5J\\Tripp et al. - A Fresh Look at De Novo Molecular Design Benchmark.pdf}
}

@online{trompCountingLegalPositions2005,
  title = {Counting {{Legal Positions}} in {{Go}}},
  author = {Tromp, John},
  date = {2005},
  url = {https://tromp.github.io/go/legal.html},
  urldate = {2018-07-02},
  file = {C:\Users\USEBPERP\Zotero\storage\K3UQF4K2\legal.html}
}

@unpublished{tucciIntroductionJudeaPearl2013,
  title = {Introduction to {{Judea Pearl}}'s {{Do-Calculus}}},
  author = {Tucci, Robert R.},
  date = {2013-04-25},
  eprint = {1305.5506},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1305.5506},
  urldate = {2019-06-04},
  abstract = {This is a purely pedagogical paper with no new results. The goal of the paper is to give a fairly self-contained introduction to Judea Pearl's do-calculus, including proofs of his 3 rules.},
  keywords = {Computer Science - Artificial Intelligence},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NEN2JTUP\\Tucci_2013_Introduction to Judea Pearl's Do-Calculus.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7IMWDBL7\\1305.html}
}

@report{turkMolecularAssaysSimulator2022,
  type = {preprint},
  title = {A {{Molecular Assays Simulator}} to {{Unravel Predictors Hacking}} in Goal-Directed Molecular Generations},
  author = {Turk, Joseph-André and Gendreau, Philippe and Drizard, Nicolas and Gaston-Mathé, Yann},
  date = {2022-06-13},
  institution = {Chemistry},
  doi = {10.26434/chemrxiv-2022-dl347},
  url = {https://chemrxiv.org/engage/chemrxiv/article-details/62a338aabb75190ef7492fba},
  urldate = {2022-08-31},
  abstract = {Generative models are being increasingly used in drug discovery campaigns, very often coupled with ADME or bio-assays Quantitative Structure-Activity Relationship (QSAR) models to optimize a given set of properties. The molecules proposed by these algorithms are often revealed to be false positives, i.e. outside the true candidate drug target profile (CDTP), because the predictors are being hacked by the generative model during the optimization. By hacking we mean an over-optimization of the predicted score leading to an actual decrease or stagnation of the real score. This issue is reminiscent of adversarial examples in Machine Learning and it can be seen as evidence of the Goodhart’s law - “when a measure becomes a target, it ceases to be a good measure”. This issue is even more apparent in a multi-objective setting where the models need to extrapolate outside the train set distribution, because there are no known molecules satisfying all the objectives simultaneously in the initial train set. However, analysing this problem is very difficult and expensive since it requires synthesis and tests of the generated molecules. Consequently, efforts have been made to develop various kinds of in silico oracles - real-valued functions used as proxies for molecular properties, to help with the evaluation of these generative model-based pipelines. However, these oracles have had a limited value so far, as they are often too easy to model in comparison to biological assays, and are usually limited to mono-objective cases. In this work, we introduce a simulator of multi-target assays using a smartly initialized neural network (NN) which returns continuous values for any input molecule. We use this oracle to replicate a real-world prospective lead optimization scenario. First, we train predictive models on an initial small sample of molecules aimed at predicting their real oracle values. Second, we generate new optimized molecules using the open-source GuacaMol package, coupled with the previously built models. Finally, we select compounds which match the CDTP according to the predicted values and evaluate them by computing the true oracle values. We observe that even when the predictive models have excellent estimated performance metrics, the final selection still contains many false positives according to the NN-based oracle. We evaluate the optimization behavior in mono and bi-objective scenarios using either a logistic regression or a random forest predictive model. We also propose and evaluate several methods to help mitigate the hacking issue.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\SEIA4FL7\Turk et al. - 2022 - A Molecular Assays Simulator to Unravel Predictors.pdf}
}

@article{turnerRapidQuantificationMolecular1997,
  title = {Rapid {{Quantification}} of {{Molecular Diversity}} for {{Selective Database Acquisition}}},
  author = {Turner, David B. and Tyrrell, Simon M. and Willett, Peter},
  date = {1997-01-01},
  journaltitle = {J. Chem. Inf. Comput. Sci.},
  volume = {37},
  number = {1},
  pages = {18--22},
  publisher = {American Chemical Society},
  issn = {0095-2338},
  doi = {10.1021/ci960463h},
  url = {https://doi.org/10.1021/ci960463h},
  urldate = {2022-06-09},
  abstract = {There is an increasing need to expand the structural diversity of the molecules investigated in lead-discovery programs. One way in which this can be achieved is by acquiring external datasets that will enhance an existing database. This paper describes a rapid procedure for the selection of external datasets using a measure of structural diversity that is calculated from sums of pairwise intermolecular structural similarities.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\77N6BGUT\\Turner et al. - 1997 - Rapid Quantification of Molecular Diversity for Se.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AUSE5WKH\\ci960463h.html}
}

@online{UnsupervisedLearningPCFGs,
  title = {Unsupervised Learning of {{PCFGs}} with Normalizing Flow. - {{Google Search}}},
  url = {https://www.google.com/search?q=Unsupervised+learning+of+PCFGs+with+normalizing+flow.&oq=Unsupervised+learning+of+PCFGs+with+normalizing+flow.&aqs=chrome..69i57&sourceid=chrome&ie=UTF-8},
  urldate = {2019-12-19},
  file = {C:\Users\USEBPERP\Zotero\storage\8E64NXRQ\search.html}
}

@online{UnsupervisedRealtimeAnomaly,
  title = {Unsupervised Real-Time Anomaly Detection for Streaming Data | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.neucom.2017.04.070},
  url = {https://reader.elsevier.com/reader/sd/pii/S0925231217309864?token=3018A7606CAB160A7DBD80D8BCEA8EF6432FB8BB9812093A50929D08BDCFD2E01DEA9B8CADE1EB21F9696DB1681CC1A7&originRegion=eu-west-1&originCreation=20220817092103},
  urldate = {2022-08-17},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\G8I5QI7T\\Unsupervised real-time anomaly detection for strea.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\X2RYYDSH\\S0925231217309864.html}
}

@article{unterthinerDeepLearningOpportunity,
  title = {Deep {{Learning}} as an {{Opportunity}} in {{Virtual Screening}}},
  author = {Unterthiner, Thomas and Mayr, Andreas},
  abstract = {Deep learning excels in vision and speech applications where it pushed the stateof-the-art to a new level. However its impact on other fields remains to be shown. The Merck Kaggle challenge on chemical compound activity was won by Hinton’s group with deep networks. This indicates the high potential of deep learning in drug design and attracted the attention of big pharma. However, the unrealistically small scale of the Kaggle dataset does not allow to assess the value of deep learning in drug target prediction if applied to in-house data of pharmaceutical companies. Even a publicly available drug activity data base like ChEMBL is magnitudes larger than the Kaggle dataset. ChEMBL has 13 M compound descriptors, 1.3 M compounds, and 5 k drug targets, compared to the Kaggle dataset with 11 k descriptors, 164 k compounds, and 15 drug targets.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\RB863DA8\Unterthiner and Mayr - Deep Learning as an Opportunity in Virtual Screeni.pdf}
}

@inproceedings{validationchrisNoveltyDetectionNeural1994,
  title = {Novelty {{Detection}} and {{Neural Network}}},
  author = {{ValidationChris} and {M..} and {BishopNeural} and {ScienceAston} and {UniversityBirmingham}},
  date = {1994},
  abstract = {One of the key factors limiting the use of neural networks in many industrial applications has been the diiculty of demonstrating that a trained network will continue to generate reliable outputs once it is in routine use. An important potential source of errors arises from novel input data, that is input data which diier signiicantly from the data used to train the network. In this paper we investigate the relationship between the degree of novelty of input data and the corresponding reliability of the outputs from the network. We describe a quantitative procedure for assessing novelty, and we demonstrate its performance using an application involving the monitoring of oil ow in multi-phase pipelines.},
  keywords = {Artificial neural network,Neural Network Simulation,Novelty detection,Numerous,Pipeline (computing)},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\CWLKEC4B\\ValidationChris et al_1994_Novelty Detection and Neural Network.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HZJUNPM6\\ValidationChris et al_1994_Novelty Detection and Neural Network2.pdf}
}

@article{vandamDiversityItsDecomposition,
  title = {Diversity and Its Decomposition into Variety, Balance and Disparity},
  author = {family=Dam, given=Alje, prefix=van, useprefix=true},
  journaltitle = {Royal Society Open Science},
  volume = {6},
  number = {7},
  pages = {190452},
  publisher = {Royal Society},
  doi = {10.1098/rsos.190452},
  url = {https://royalsocietypublishing.org/doi/10.1098/rsos.190452},
  urldate = {2022-09-01},
  abstract = {Diversity is a central concept in many fields. Despite its importance, there is no unified methodological framework to measure diversity and its three components of variety, balance and disparity. Current approaches take into account disparity of the types by considering their pairwise similarities. Pairwise similarities between types may not adequately capture total disparity, since they do not take into account in which way pairs are similar. Hence, pairwise similarities do not discriminate between similarities of types in terms of the same feature and similarities in which all pairs share different features. This paper presents an alternative approach which is based on the overlap of features over the whole set of types. This results in a measure of diversity that takes into account the aspects of variety, balance and disparity. Based on this measure, the ‘ABC decomposition’ is introduced, which provides separate measures for the variety, balance and disparity, allowing them to enter analysis separately. The method is illustrated by analysing the industrial diversity from 1850 to present while taking into account the overlap in occupations they employ. Finally, the framework is extended to take into account disparity considering multiple features, providing a helpful tool in analysis of high-dimensional data.},
  keywords = {aggregation,entropy,Hill numbers,mutual information,α-diversity,β-diversity},
  file = {C:\Users\USEBPERP\Zotero\storage\KGQVF4ZH\van Dam - Diversity and its decomposition into variety, bala.pdf}
}

@incollection{vandenoordNeuralDiscreteRepresentation2017,
  title = {Neural {{Discrete Representation Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {family=Oord, given=Aaron, prefix=van den, useprefix=true and Vinyals, Oriol and {kavukcuoglu}, koray},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  date = {2017},
  pages = {6306--6315},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/7210-neural-discrete-representation-learning.pdf},
  urldate = {2019-12-23},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\PGNUP9TS\\van den Oord et al_2017_Neural Discrete Representation Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HFGAJ6ZC\\7210-neural-discrete-representation-learning.html}
}

@unpublished{vanderwesthuizenUnreasonableEffectivenessForget2018,
  title = {The Unreasonable Effectiveness of the Forget Gate},
  author = {family=Westhuizen, given=Jos, prefix=van der, useprefix=true and Lasenby, Joan},
  date = {2018-04-13},
  eprint = {1804.04849},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.04849},
  urldate = {2018-09-21},
  abstract = {Given the success of the gated recurrent unit, a natural question is whether all the gates of the long short-term memory (LSTM) network are necessary. Previous research has shown that the forget gate is one of the most important gates in the LSTM. Here we show that a forget-gate-only version of the LSTM with chrono-initialized biases, not only provides computational savings but outperforms the standard LSTM on multiple benchmark datasets and competes with some of the best contemporary models. Our proposed network, the JANET, achieves accuracies of 99\% and 92.5\% on the MNIST and pMNIST datasets, outperforming the standard LSTM which yields accuracies of 98.5\% and 91\%.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\JE7NIQQK\\van der Westhuizen_Lasenby_2018_The unreasonable effectiveness of the forget gate.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DA5GA8DJ\\1804.html}
}

@article{vandeursenGENHighlyEfficient2020,
  title = {{{GEN}}: Highly Efficient {{SMILES}} Explorer Using Autodidactic Generative Examination Networks},
  shorttitle = {{{GEN}}},
  author = {family=Deursen, given=Ruud, prefix=van, useprefix=true and Ertl, Peter and Tetko, Igor V. and Godin, Guillaume},
  date = {2020-12},
  journaltitle = {J Cheminform},
  volume = {12},
  number = {1},
  pages = {22},
  issn = {1758-2946},
  doi = {10.1186/s13321-020-00425-8},
  url = {https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00425-8},
  urldate = {2022-08-29},
  abstract = {Recurrent neural networks have been widely used to generate millions of de novo molecules in defined chemical spaces. Reported deep generative models are exclusively based on LSTM and/or GRU units and frequently trained using canonical SMILES. In this study, we introduce Generative Examination Networks (GEN) as a new approach to train deep generative networks for SMILES generation. In our GENs, we have used an architecture based on multiple concatenated bidirectional RNN units to enhance the validity of generated SMILES. GENs autonomously learn the target space in a few epochs and are stopped early using an independent online examination mechanism, measuring the quality of the generated set. Herein we have used online statistical quality control (SQC) on the percentage of valid molecular SMILES as examination measure to select the earliest available stable model weights. Very high levels of valid SMILES (95–98\%) can be generated using multiple parallel encoding layers in combination with SMILES augmentation using unrestricted SMILES randomization. Our trained models combine an excellent novelty rate (85–90\%) while generating SMILES with strong conservation of the property space (95–99\%). In GENs, both the generative network and the examination mechanism are open to other architectures and quality criteria.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\FHAKM8FY\van Deursen et al. - 2020 - GEN highly efficient SMILES explorer using autodi.pdf}
}

@article{vanhiltenVirtualCompoundLibraries2019,
  title = {Virtual {{Compound Libraries}} in {{Computer-Assisted Drug Discovery}}},
  author = {family=Hilten, given=Niek, prefix=van, useprefix=true and Chevillard, Florent and Kolb, Peter},
  date = {2019-02-25},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {59},
  number = {2},
  pages = {644--651},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.8b00737},
  url = {https://doi.org/10.1021/acs.jcim.8b00737},
  urldate = {2019-05-20},
  abstract = {The use of virtual compound libraries in computer-assisted drug discovery has gained in popularity and has already lead to numerous successes. Here, we examine key static and dynamic virtual library concepts that have been developed over the past decade. To facilitate the search for new drugs in the vastness of chemical space, there are still several hurdles to overcome, including the current difficulties in screening and parsing efficiency and the need for more reliable vendors and accurate synthesis prediction tools. These challenges should be tackled by both the developers of virtual libraries and by their users, in order for the exploration of chemical space to live up to its potential.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\68XIKIFW\\van Hilten et al_2019_Virtual Compound Libraries in Computer-Assisted Drug Discovery.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\FZ5HSW2P\\acs.jcim.html}
}

@article{vanrossenbergTolkienExceptionalVisit,
  title = {Tolkien's {{Exceptional Visit}} to {{Holland}}: {{A Reconstruction}}},
  author = {family=Rossenberg, given=René, prefix=van, useprefix=true},
  pages = {11},
  abstract = {In March 1958 Tolkien was the guest of honour at a “Hobbit Meal” in Rotterdam, Holland. He had never before accepted such an invitation and never did again. By interviewing the organisers and many people who met Tolkien, the visit has been reconstructed, and many, often funny anecdotes have come to light.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\9DFC2TF8\van Rossenberg - Tolkien's Exceptional Visit to Holland A Reconstr.pdf}
}

@unpublished{vasquezMelNetGenerativeModel2019,
  title = {{{MelNet}}: {{A Generative Model}} for {{Audio}} in the {{Frequency Domain}}},
  shorttitle = {{{MelNet}}},
  author = {Vasquez, Sean and Lewis, Mike},
  date = {2019-06-04},
  eprint = {1906.01083},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/1906.01083},
  urldate = {2019-10-21},
  abstract = {Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps. While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms. By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales that time-domain models have yet to achieve. We apply our model to a variety of audio generation tasks, including unconditional speech generation, music generation, and text-to-speech synthesis---showing improvements over previous approaches in both density estimates and human judgments.},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4AXN6A33\\Vasquez_Lewis_2019_MelNet.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\524ZZI89\\1906.html}
}

@unpublished{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-06-12},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2018-10-25},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DGML24JB\\Vaswani et al_2017_Attention Is All You Need.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SM5DN4I4\\Vaswani et al_2017_Attention Is All You Need.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\UVQIP8VQ\\Vaswani et al. - 2017 - Attention Is All You Need.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\5PPY2289\\1706.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\7RQ7X5XA\\1706.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\LY4IVZTV\\1706.html}
}

@article{venkatasubramanianComputeraidedMolecularDesign1994,
  title = {Computer-Aided Molecular Design Using Genetic Algorithms},
  author = {Venkatasubramanian, V. and Chan, K. and Caruthers, J. M.},
  date = {1994-09-01},
  journaltitle = {Computers \& Chemical Engineering},
  series = {An {{International Journal}} of {{Computer Applications}} in {{Chemical Engineering}}},
  volume = {18},
  number = {9},
  pages = {833--844},
  issn = {0098-1354},
  doi = {10.1016/0098-1354(93)E0023-3},
  url = {http://www.sciencedirect.com/science/article/pii/0098135493E00233},
  urldate = {2020-04-08},
  abstract = {Designing new molecules possessing desired properties is an important activity in the chemical and pharmaceutical industries. Much of this design involves an elaborate and expensive trial-and-error process that is difficult to automate. The present study describes a new computer-aided molecular design approach using genetic algorithms. Unlike traditional search and optimization techniques, genetic algorithms perform a guided stochastic search where improved solutions are achieved by sampling areas of the parameter space that have a higher probability for good solutions. Moreover, genetic algorithms allow for the direct incorporation of higher level chemical knowledge and reasoning strategies to make the search more efficient. The utility of genetic algorithms for molecular design is demonstrated with some case studies in polymer design. The merits and potential deficiencies of this approach are also discussed.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\8APPFQA2\\Venkatasubramanian et al_1994_Computer-aided molecular design using genetic algorithms.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TSZDH3AP\\0098135493E00233.html}
}

@article{vinkersSYNOPSISSYNthesizeOPtimize2003,
  title = {{{SYNOPSIS}}: {{SYNthesize}} and {{OPtimize System}} in {{Silico}}},
  shorttitle = {{{SYNOPSIS}}},
  author = {Vinkers, H. Maarten and family=Jonge, given=Marc R., prefix=de, useprefix=true and Daeyaert, Frederik F. D. and Heeres, Jan and Koymans, Lucien M. H. and family=Lenthe, given=Joop H., prefix=van, useprefix=true and Lewi, Paul J. and Timmerman, Henk and Van Aken, Koen and Janssen, Paul A. J.},
  date = {2003-06},
  journaltitle = {J. Med. Chem.},
  volume = {46},
  number = {13},
  pages = {2765--2773},
  issn = {0022-2623, 1520-4804},
  doi = {10.1021/jm030809x},
  url = {https://pubs.acs.org/doi/10.1021/jm030809x},
  urldate = {2020-12-01},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\BVI8VCVQ\Vinkers et al. - 2003 - SYNOPSIS SYNthesize and OPtimize System in Silico.pdf}
}

@incollection{vinyalsPointerNetworks2015,
  title = {Pointer {{Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  date = {2015},
  pages = {2692--2700},
  publisher = {Curran Associates, Inc.},
  url = {http://papers.nips.cc/paper/5866-pointer-networks.pdf},
  urldate = {2019-09-23},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4LIZG8A2\\Vinyals et al_2015_Pointer Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\PBXBIEAE\\5866-pointer-networks.html}
}

@online{VirtualCompoundScreening,
  title = {Virtual {{Compound Screening}}: {{The State}} of the {{Art}}},
  shorttitle = {Virtual {{Compound Screening}}},
  url = {https://www.science.org/content/blog-post/virtual-compound-screening-state-art},
  urldate = {2022-06-02},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\6Y6GPX97\virtual-compound-screening-state-art.html}
}

@article{w.coleyGraphconvolutionalNeuralNetwork2019,
  ids = {coley2019graphconvolutional,w.coleyGraphconvolutionalNeuralNetwork2019a},
  title = {A Graph-Convolutional Neural Network Model for the Prediction of Chemical Reactivity},
  author = {W.~Coley, Connor and Jin, Wengong and Rogers, Luke and F.~Jamison, Timothy and S.~Jaakkola, Tommi and H.~Green, William and Barzilay, Regina and F.~Jensen, Klavs},
  date = {2019},
  journaltitle = {Chemical Science},
  volume = {10},
  number = {2},
  pages = {370--377},
  publisher = {Royal Society of Chemistry},
  doi = {10.1039/C8SC04228D},
  url = {https://pubs.rsc.org/en/content/articlelanding/2019/sc/c8sc04228d},
  urldate = {2019-10-02},
  issue = {2},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NPFLDUUP\\W. Coley et al_2019_A graph-convolutional neural network model for the prediction of chemical.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\NUT8T7NZ\\W. Coley et al. - 2019 - A graph-convolutional neural network model for the.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\U45A37W7\\Coley et al_2019_A graph-convolutional neural network model for the prediction of chemical.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\64P4YL96\\c8sc04228d.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\WLKPB7YU\\c8sc04228d.html}
}

@article{waldmanNovelAlgorithmsOptimization2000,
  title = {Novel Algorithms for the Optimization of Molecular Diversity of Combinatorial {{libraries11Color Plates}} for This Article Are on Pages 533–536.},
  author = {Waldman, Marvin and Li, Hong and Hassan, Moises},
  date = {2000-01-01},
  journaltitle = {Journal of Molecular Graphics and Modelling},
  volume = {18},
  number = {4},
  pages = {412--426},
  issn = {1093-3263},
  doi = {10.1016/S1093-3263(00)00071-1},
  url = {https://www.sciencedirect.com/science/article/pii/S1093326300000711},
  urldate = {2022-06-09},
  abstract = {Various approaches to measuring and optimizing molecular diversity of combinatorial libraries are presented. The need for different diversity metrics for libraries consisting of discrete molecules (“cherry picking”) vs libraries formed from combinatorial R-group enumeration (array-based selection) is discussed. Ideal requirements for diversity metrics applied to array-based selection are proposed, focusing, in particular, on the concept of incremental diversity, i.e., the change in diversity as redundant or nonredundant molecules are added to a compound collection or combinatorial library. Several distance and cell-based diversity functions are presented and analyzed in terms of their ability to satisfy these requirements. These diversity functions are applied to designing diverse libraries for two test cases, and the performance of the diversity functions is assessed. Issues associated with redundant molecules in the virtual library are discussed and analyzed using one of the test examples. The results are compared to reagent-based diversity optimizations, and it is shown that a product-based diversity protocol can result in significant improvements over a reagent-based scheme based on the diversity obtained for the resulting libraries.},
  langid = {english},
  keywords = {combinatorial library algorithm,molecular diversity},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\I399LANC\\Waldman et al. - 2000 - Novel algorithms for the optimization of molecular.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GQR5HCDY\\S1093326300000711.html}
}

@article{wallachMostLigandBasedClassification2018,
  title = {Most {{Ligand-Based Classification Benchmarks Reward Memorization Rather}} than {{Generalization}}},
  author = {Wallach, Izhar and Heifets, Abraham},
  date = {2018-05-29},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {58},
  number = {5},
  pages = {916--932},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.7b00403},
  url = {https://doi.org/10.1021/acs.jcim.7b00403},
  urldate = {2019-05-20},
  abstract = {Undetected overfitting can occur when there are significant redundancies between training and validation data. We describe AVE, a new measure of training–validation redundancy for ligand-based classification problems, that accounts for the similarity among inactive molecules as well as active ones. We investigated seven widely used benchmarks for virtual screening and classification, and we show that the amount of AVE bias strongly correlates with the performance of ligand-based predictive methods irrespective of the predicted property, chemical fingerprint, similarity measure, or previously applied unbiasing techniques. Therefore, it may be the case that the previously reported performance of most ligand-based methods can be explained by overfitting to benchmarks rather than good prospective accuracy.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\94SKLGYM\\Wallach_Heifets_2018_Most Ligand-Based Classification Benchmarks Reward Memorization Rather than.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QV5N6B64\\acs.jcim.html}
}

@article{waltersAssessingImpactGenerative2020,
  title = {Assessing the Impact of Generative {{AI}} on Medicinal Chemistry},
  author = {Walters, W. Patrick and Murcko, Mark},
  date = {2020-02},
  journaltitle = {Nat Biotechnol},
  volume = {38},
  number = {2},
  pages = {143--145},
  publisher = {Nature Publishing Group},
  issn = {1546-1696},
  doi = {10.1038/s41587-020-0418-2},
  url = {https://www.nature.com/articles/s41587-020-0418-2},
  urldate = {2022-07-29},
  issue = {2},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Drug discovery},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\2HZPFHPR\\Walters and Murcko - 2020 - Assessing the impact of generative AI on medicinal.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\L2EZNR9A\\s41587-020-0418-2.html}
}

@article{waltersCriticalAssessmentAI2021,
  title = {Critical Assessment of {{AI}} in Drug Discovery},
  author = {Walters, W. Patrick and Barzilay, Regina},
  date = {2021-09-02},
  journaltitle = {Expert Opinion on Drug Discovery},
  volume = {16},
  number = {9},
  eprint = {33870801},
  eprinttype = {pmid},
  pages = {937--947},
  publisher = {Taylor \& Francis},
  issn = {1746-0441},
  doi = {10.1080/17460441.2021.1915982},
  url = {https://doi.org/10.1080/17460441.2021.1915982},
  urldate = {2022-09-15},
  abstract = {Introduction: Artificial Intelligence (AI) has become a component of our everyday lives, with applications ranging from recommendations on what to buy to the analysis of radiology images. Many of the techniques originally developed for other fields such as language translation and computer vision are now being applied in drug discovery. AI has enabled multiple aspects of drug discovery including the analysis of high content screening data, and the design and synthesis of new molecules.Areas covered: This perspective provides an overview of the application of AI in several areas relevant to drug discovery including property prediction, molecule generation, image analysis, and organic synthesis planning.Expert opinion: While a variety of machine learning methods are now being routinely used to predict biological activity and ADME properties, methods of representing molecules continue to evolve. Molecule generation methods are relatively new and unproven but hold the potential to access new, unexplored areas of chemical space. The application of AI in drug discovery will continue to benefit from dedicated research, as well as AI developments in other fields. With this pairing algorithmic advancements and high-quality data, the impact of AI in drug discovery will continue to grow in the coming years.},
  keywords = {Artificial intelligence,drug discovery,machine learning,QSAR; generative models; image analysis}
}

@article{waltersVirtualChemicalLibraries2019,
  ids = {walters2019virtuala},
  title = {Virtual {{Chemical Libraries}}},
  author = {Walters, W. Patrick},
  date = {2019-02-14},
  journaltitle = {J. Med. Chem.},
  volume = {62},
  number = {3},
  pages = {1116--1124},
  issn = {0022-2623},
  doi = {10.1021/acs.jmedchem.8b01048},
  url = {https://doi.org/10.1021/acs.jmedchem.8b01048},
  urldate = {2019-10-14},
  abstract = {Advances in computer processing speed and storage capacity have enabled researchers to generate virtual chemical libraries containing billions of molecules. While these numbers appear large, they are only a small fraction of the number of organic molecules that could potentially be synthesized. This review provides an overview of recent advances in the generation and use of virtual chemical libraries in medicinal chemistry. We also consider the practical implications of these libraries in drug discovery programs and highlight a number of current and future challenges.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\CL94PLQN\\Walters_2019_Virtual Chemical Libraries.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\MFN55PL3\\Walters_2019_Virtual Chemical Libraries.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\P3ISJHMM\\Walters - 2019 - Virtual Chemical Libraries Miniperspective.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\T7P5HL4E\\Walters - 2019 - Virtual Chemical Libraries Miniperspective.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\4YFV5RZG\\acs.jmedchem.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\Z87QXIIF\\acs.jmedchem.html}
}

@article{waltersVirtualScreeningOverview1998,
  title = {Virtual Screening—an Overview},
  author = {Walters, W.Patrick and Stahl, Matthew T and Murcko, Mark A},
  date = {1998-04},
  journaltitle = {Drug Discovery Today},
  volume = {3},
  number = {4},
  pages = {160--178},
  issn = {13596446},
  doi = {10.1016/S1359-6446(97)01163-X},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S135964469701163X},
  urldate = {2022-09-15},
  langid = {english},
  keywords = {combinatorial chemistry,docking,high-throughput screening,molecular diversity,prediction of ligand binding free energy},
  file = {C:\Users\USEBPERP\Zotero\storage\VF8EW9RI\S135964469701163X.html}
}

@article{waltersWhatMedicinalChemists2011,
  title = {What {{Do Medicinal Chemists Actually Make}}? {{A}} 50-{{Year Retrospective}}},
  shorttitle = {What {{Do Medicinal Chemists Actually Make}}?},
  author = {Walters, W. Patrick and Green, Jeremy and Weiss, Jonathan R. and Murcko, Mark A.},
  date = {2011-10-13},
  journaltitle = {J. Med. Chem.},
  volume = {54},
  number = {19},
  pages = {6405--6416},
  issn = {0022-2623},
  doi = {10.1021/jm200504p},
  url = {https://doi.org/10.1021/jm200504p},
  urldate = {2019-10-24},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LQRGLQFL\\Walters et al_2011_What Do Medicinal Chemists Actually Make.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\E2KWKXRW\\jm200504p.html}
}

@online{wangForecastbasedMultiaspectFramework2022,
  title = {Forecast-Based {{Multi-aspect Framework}} for {{Multivariate Time-series Anomaly Detection}}},
  author = {Wang, Lan and Lin, Yusan and Wu, Yuhang and Chen, Huiyuan and Wang, Fei and Yang, Hao},
  date = {2022-01-13},
  eprint = {2201.04792},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2201.04792},
  urldate = {2023-01-06},
  abstract = {Today's cyber-world is vastly multivariate. Metrics collected at extreme varieties demand multivariate algorithms to properly detect anomalies. However, forecast-based algorithms, as widely proven approaches, often perform sub-optimally or inconsistently across datasets. A key common issue is they strive to be one-size-fits-all but anomalies are distinctive in nature. We propose a method that tailors to such distinction. Presenting FMUAD - a Forecast-based, Multi-aspect, Unsupervised Anomaly Detection framework. FMUAD explicitly and separately captures the signature traits of anomaly types - spatial change, temporal change and correlation change - with independent modules. The modules then jointly learn an optimal feature representation, which is highly flexible and intuitive, unlike most other models in the category. Extensive experiments show our FMUAD framework consistently outperforms other state-of-the-art forecast-based anomaly detectors.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\IRXYSBWG\\Wang et al. - 2022 - Forecast-based Multi-aspect Framework for Multivar.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\PYUJGCYV\\2201.html}
}

@unpublished{wangMolCLRMolecularContrastive2021,
  title = {{{MolCLR}}: {{Molecular Contrastive Learning}} of {{Representations}} via {{Graph Neural Networks}}},
  shorttitle = {{{MolCLR}}},
  author = {Wang, Yuyang and Wang, Jianren and Cao, Zhonglin and Farimani, Amir Barati},
  date = {2021-02-19},
  eprint = {2102.10056},
  eprinttype = {arxiv},
  eprintclass = {physics},
  url = {http://arxiv.org/abs/2102.10056},
  urldate = {2021-04-08},
  abstract = {Molecular machine learning bears promise for efficient molecule property prediction and drug discovery. However, due to the limited labeled data and the giant chemical space, machine learning models trained via supervised learning perform poorly in generalization. This greatly limits the applications of machine learning methods for molecular design and discovery. In this work, we present MolCLR: Molecular Contrastive Learning of Representations via Graph Neural Networks (GNNs), a self-supervised learning framework for large unlabeled molecule datasets. Specifically, we first build a molecular graph, where each node represents an atom and each edge represents a chemical bond. A GNN is then used to encode the molecule graph. We propose three novel molecule graph augmentations: atom masking, bond deletion, and subgraph removal. A contrastive estimator is utilized to maximize the agreement of different graph augmentations from the same molecule. Experiments show that molecule representations learned by MolCLR can be transferred to multiple downstream molecular property prediction tasks. Our method thus achieves state-of-the-art performance on many challenging datasets. We also prove the efficiency of our proposed molecule graph augmentations on supervised molecular classification tasks.},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4T5IBEJB\\Wang et al. - 2021 - MolCLR Molecular Contrastive Learning of Represen.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZLUY5ZBQ\\2102.html}
}

@article{wangRetroPrimeChemistryInspiredTransformerbased2020,
  title = {{{RetroPrime}}: {{A Chemistry-Inspired}} and {{Transformer-based Method}} for {{Retrosynthesis Predictions}}},
  shorttitle = {{{RetroPrime}}},
  author = {Wang, Xiaorui and Qiu, Jiezhong and Li, Yuquan and Chen, Guangyong and Liu, Huanxiang and Liao, Ben and Hsieh, Chang-Yu and Yao, Xiaojun},
  date = {2020-11-26},
  publisher = {ChemRxiv},
  doi = {10.26434/chemrxiv.12971942.v2},
  url = {/articles/preprint/RetroPrime_A_Chemistry-Inspired_and_Transformer-based_Method_for_Retrosynthesis_Predictions/12971942/2},
  urldate = {2021-04-28},
  abstract = {Retrosynthesis prediction is a crucial task for organic synthesis. In this work, we propose a template-free and Transformer-based method dubbed RetroPrime, integrating chemists’ retrosynthetic strategy of (1) decomposing a molecule into synthons then (2) generating reactants by attaching leaving groups. These two steps are accomplished with versatile Transformer models, respectively. While RetroPrime performs competitively against all state-of-the art models on the standard USPTO-50K dataset, it manifests remarkable generalizability and outperforms the only published result by a non-trivial margin of 4.8\% for the Top-1 accuracy on the large-scale USPTO-full dataset. It is known that outputs of Transformer-based retrosynthesis model tend to suffer from insufficient diversity and high invalidity. These problems may limit the potential of Transformer-based methods in real practice, yet no prior works address both issues simultaneously. RetroPrime is designed to tackle these challenges. Finally, we provide convincing results to support the claim that RetromPrime can more effectively generalize across chemical space.},
  langid = {english}
}

@article{wangRetroPrimeDiversePlausible2021,
  title = {{{RetroPrime}}: {{A Diverse}}, Plausible and {{Transformer-based}} Method for {{Single-Step}} Retrosynthesis Predictions},
  shorttitle = {{{RetroPrime}}},
  author = {Wang, Xiaorui and Li, Yuquan and Qiu, Jiezhong and Chen, Guangyong and Liu, Huanxiang and Liao, Benben and Hsieh, Chang-Yu and Yao, Xiaojun},
  date = {2021-09-15},
  journaltitle = {Chemical Engineering Journal},
  volume = {420},
  pages = {129845},
  issn = {1385-8947},
  doi = {10.1016/j.cej.2021.129845},
  url = {https://www.sciencedirect.com/science/article/pii/S1385894721014303},
  urldate = {2021-05-18},
  abstract = {Retrosynthesis prediction is a crucial task for organic synthesis. In this work, we propose a single-step template-free and Transformer-based method dubbed RetroPrime, integrating chemists’ retrosynthetic strategy of (1) decomposing a molecule into synthons then (2) generating reactants by attaching leaving groups. These two stages are accomplished with versatile Transformer models, respectively. RetroPrime achieves the Top-1 accuracy of 64.8\% and 51.4\%, when the reaction type is known and unknown, respectively, in the USPTO-50~K dataset. And the Top-1 accuracy is close to the state-of-the-art transformer-based method in the large dataset USPTO-full. It is known that outputs of the Transformer-based retrosynthesis model tend to suffer from insufficient diversity and high chemical implausibility. These problems may limit the potential of Transformer-based methods in real practice, yet few works address both issues simultaneously. RetroPrime is designed to tackle these challenges.},
  langid = {english},
  keywords = {Deep Learning,Natural Language Processing,Template-free Single-Step Retrosynthesis},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6E69TICA\\Wang et al. - 2021 - RetroPrime A Diverse, plausible and Transformer-b.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SJVMY352\\S1385894721014303.html}
}

@unpublished{wangSATNetBridgingDeep2019,
  title = {{{SATNet}}: {{Bridging}} Deep Learning and Logical Reasoning Using a Differentiable Satisfiability Solver},
  shorttitle = {{{SATNet}}},
  author = {Wang, Po-Wei and Donti, Priya L. and Wilder, Bryan and Kolter, Zico},
  date = {2019-05-28},
  eprint = {1905.12149},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.12149},
  urldate = {2019-06-04},
  abstract = {Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9x9 Sudoku solely from examples. We also solve a "visual Sudok" problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\C7SXWLDV\\Wang et al_2019_SATNet.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\29LCA2CF\\1905.html}
}

@article{wangStudyTwoGeometric1988,
  title = {A Study on Two Geometric Location Problems},
  author = {Wang, D.W. and Kuo, Yue-Sun},
  date = {1988-08},
  journaltitle = {Information Processing Letters},
  volume = {28},
  number = {6},
  pages = {281--286},
  issn = {00200190},
  doi = {10.1016/0020-0190(88)90174-3},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0020019088901743},
  urldate = {2020-10-02},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\ZPKIA3GF\Wang_Kuo_1988_A study on two geometric location problems.pdf}
}

@article{wangStudyWaveletNeural2019,
  title = {Study on Wavelet Neural Network Based Anomaly Detection in Ocean Observing Data Series},
  author = {Wang, Yi and Han, Linsheng and Liu, Wei and Yang, Shujia and Gao, Yanbo},
  date = {2019-08-15},
  journaltitle = {Ocean Engineering},
  volume = {186},
  pages = {106129},
  issn = {0029-8018},
  doi = {10.1016/j.oceaneng.2019.106129},
  url = {https://www.sciencedirect.com/science/article/pii/S002980181930318X},
  urldate = {2023-01-27},
  abstract = {In this paper, a novel method is presented for detecting anomalies in ocean fixed-point observing time series, which combines wavelet neural network (WNN), classifying threshold and two detecting strategies. The WNN was developed without any labeled training data to simulate the non-anomalous behaviors for next-step prediction. The classifying threshold was constructed according to the estimated distribution of long-term historical residual errors. The observation strategy (OS) and prediction strategy (PS) were designed to detect new unknown anomalies. Two types of marine observing time series from a buoy, deployed at the National Ocean Test Site of China, were selected for verifying the method. The results show that 99\% of classifying confidence level is adequate to provide a reasonable trade-off between the false negative and false positive. By using the two detecting strategies and selecting proper estimated distribution of the threshold, the method is efficient for identifying the anomalous points and patterns which were caused by the natural factors or equipment failures. Compared with traditional ANN and wavelet-ANN, the WNN-based method is more tolerant to noise and more sensitive to anomalies with temporal dependencies. Furthermore, this approach introduced here can work in a real-time way and will help ocean engineering managers to obtain informed decisions.},
  langid = {english},
  keywords = {Anomaly detection,Ocean engineering,Ocean observing series,Wavelet neural network(WNN)},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\FXYXGSLR\\Wang et al. - 2019 - Study on wavelet neural network based anomaly dete.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SK837IUT\\S002980181930318X.html}
}

@article{watsonRetrosyntheticAnalysisAlgorithm2019,
  title = {A Retrosynthetic Analysis Algorithm Implementation},
  author = {Watson, Ian A. and Wang, Jibo and Nicolaou, Christos A.},
  date = {2019-12},
  journaltitle = {J Cheminform},
  volume = {11},
  number = {1},
  pages = {1},
  issn = {1758-2946},
  doi = {10.1186/s13321-018-0323-6},
  url = {https://jcheminf.biomedcentral.com/articles/10.1186/s13321-018-0323-6},
  urldate = {2019-10-02},
  abstract = {The need for synthetic route design arises frequently in discovery-oriented chemistry organizations. While traditionally finding solutions to this problem has been the domain of human experts, several computational approaches, aided by the algorithmic advances and the availability of large reaction collections, have recently been reported. Herein we present our own implementation of a retrosynthetic analysis method and demonstrate its capabilities in an attempt to identify synthetic routes for a collection of approved drugs. Our results indicate that the method, leveraging on reaction transformation rules learned from a large patent reaction dataset, can identify multiple theoretically feasible synthetic routes and, thus, support research chemist everyday efforts.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\6J3ZW83X\Watson et al. - 2019 - A retrosynthetic analysis algorithm implementation.pdf}
}

@inproceedings{waxmanNeuralAnalogDiffusionEnhancement1989,
  title = {Neural {{Analog Diffusion-Enhancement Layer}} and {{Spatio-Temporal Grouping}} in {{Early Vision}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Waxman, Allen and Seibert, Michael and Cunningham, Robert and Wu, Jian},
  editor = {Touretzky, D.},
  date = {1989},
  volume = {1},
  publisher = {Morgan-Kaufmann},
  url = {https://proceedings.neurips.cc/paper/1988/file/e2ef524fbf3d9fe611d5a8e90fefdc9c-Paper.pdf},
  urldate = {2021-04-08},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DV5PKWFV\\Waxman et al. - 1989 - Neural Analog Diffusion-Enhancement Layer and Spat.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\66LIFC3B\\6351bf9dce654515bf1ddbd6426dfa97-Paper.html}
}

@online{WaybackMachine2005,
  title = {Wayback {{Machine}}},
  date = {2005-03-07},
  url = {https://web.archive.org/web/20050307024938/http://fragrieu.free.fr/SearchingForSolutions.pdf},
  urldate = {2020-09-10},
  file = {C:\Users\USEBPERP\Zotero\storage\9G67832H\2005_Wayback Machine.pdf}
}

@online{WebLoginService,
  title = {Web {{Login Service}}},
  url = {https://shibboleth.im.jku.at/idp/profile/SAML2/Redirect/SSO?execution=e1s5},
  urldate = {2023-03-10},
  file = {C:\Users\USEBPERP\Zotero\storage\G4EW4CYS\SSO.html}
}

@book{weckertComputerEthics2017,
  title = {Computer {{Ethics}}},
  editor = {Weckert, John},
  date = {2017-05-15},
  edition = {1},
  publisher = {Routledge},
  doi = {10.4324/9781315259697},
  url = {https://www.taylorfrancis.com/books/9781351949828},
  urldate = {2020-06-24},
  isbn = {978-1-315-25969-7},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\2BX3ITSG\Weckert - 2017 - Computer Ethics.pdf}
}

@article{weiMultipleobjectiveReinforcementLearning,
  title = {Multiple-Objective {{Reinforcement Learning}} for {{Inverse Design}} and {{Identiﬁcation}}},
  author = {Wei, Haoran and Olarte, Mariefel and Goh, Garrett B},
  pages = {11},
  abstract = {Inverse chemical design using generative deep learning (DL) models is an emerging tool, which has early success in generating molecular structures with desired properties. We enhance this tool by developing heuristics for curriculum-learning based multiple-objective (20+ objectives) reinforcement learning, and apply it to the context of chemical identification. We develop a generative DL framework that utilizes constraints pertaining to the unknown molecule’s mass and functional group composition, and show that multiple-objective RL-based generative DL models can correctly identify unknown molecules with a 80\% success rate, compared to the baseline approach of 0\%. Lastly, the heuristics developed are not limited to just chemistry research challenges; we anticipate any problem that utilizes reinforcement learning with multiple-objectives will benefit from it.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\ZIE8WY6G\Wei et al_Multiple-objective Reinforcement Learning for Inverse Design and Identiﬁcation.pdf}
}

@article{weiNeuralNetworksPrediction2016,
  ids = {wei2016neurala,wei2016neuralb,weiNeuralNetworksPrediction2016a,weiNeuralNetworksPrediction2016b},
  title = {Neural {{Networks}} for the {{Prediction}} of {{Organic Chemistry Reactions}}},
  author = {Wei, Jennifer N. and Duvenaud, David and Aspuru-Guzik, Alán},
  date = {2016-10-26},
  journaltitle = {ACS Cent. Sci.},
  volume = {2},
  number = {10},
  pages = {725--732},
  publisher = {American Chemical Society},
  issn = {2374-7943},
  doi = {10.1021/acscentsci.6b00219},
  url = {https://doi.org/10.1021/acscentsci.6b00219},
  urldate = {2019-10-01},
  abstract = {Reaction prediction remains one of the major challenges for organic chemistry and is a prerequisite for efficient synthetic planning. It is desirable to develop algorithms that, like humans, “learn” from being exposed to examples of the application of the rules of organic chemistry. We explore the use of neural networks for predicting reaction types, using a new reaction fingerprinting method. We combine this predictor with SMARTS transformations to build a system which, given a set of reagents and reactants, predicts the likely products. We test this method on problems from a popular organic chemistry textbook.},
  issue = {10},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6QIDCP9K\\Wei et al. - 2016 - Neural Networks for the Prediction of Organic Chem.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JGNGMYWJ\\Wei et al. - 2016 - Neural Networks for the Prediction of Organic Chem.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QV4MBYQA\\Wei et al_2016_Neural Networks for the Prediction of Organic Chemistry Reactions.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\W73ZWPJ7\\Wei et al. - 2016 - Neural Networks for the Prediction of Organic Chem.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\3H943LEW\\acscentsci.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\3N9DPPMN\\acscentsci.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\B5S29DP2\\acscentsci.html}
}

@article{weiningerSMILESChemicalLanguage1988,
  title = {{{SMILES}}, a Chemical Language and Information System},
  author = {Weininger, David},
  date = {1988-02-01},
  journaltitle = {J. Chem. Inf. Comput. Sci.},
  volume = {28},
  number = {1},
  pages = {31--36},
  publisher = {American Chemical Society},
  issn = {0095-2338},
  doi = {10.1021/ci00057a005},
  url = {https://pubs.acs.org/doi/abs/10.1021/ci00057a005},
  urldate = {2020-04-20},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NKIKPGDC\\Weininger - 1988 - SMILES, a chemical language and information system.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\TLUSWMUD\\Weininger_1988_SMILES, a chemical language and information system.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\IHGAR55Z\\ci00057a005.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\TDR7TRH5\\ci00057a005.html}
}

@unpublished{wenInterplayOptimizationGeneralization2019,
  title = {Interplay {{Between Optimization}} and {{Generalization}} of {{Stochastic Gradient Descent}} with {{Covariance Noise}}},
  author = {Wen, Yeming and Luk, Kevin and Gazeau, Maxime and Zhang, Guodong and Chan, Harris and Ba, Jimmy},
  date = {2019-04-02},
  eprint = {1902.08234},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1902.08234},
  urldate = {2020-02-03},
  abstract = {The choice of batch-size in a stochastic optimization algorithm plays a substantial role for both optimization and generalization. Increasing the batch-size used typically improves optimization but degrades generalization. To address the problem of improving generalization while maintaining optimal convergence in large-batch training, we propose to add covariance noise to the gradients. We demonstrate that the optimization performance of our method is more accurately captured by the structure of the noise covariance matrix rather than by the variance of gradients. Moreover, over the convex-quadratic, we prove in theory that it can be characterized by the Frobenius norm of the noise matrix. Our empirical studies with standard deep learning model-architectures and datasets shows that our method not only improves generalization performance in large-batch training, but furthermore, does so in a way where the optimization performance remains desirable and the training duration is not elongated.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\U2HJ3U2T\\Wen et al_2019_Interplay Between Optimization and Generalization of Stochastic Gradient.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7VP3EZHK\\1902.html}
}

@online{WhatWhatNearly2021,
  title = {What’s {{What}}: {{The}} ({{Nearly}}) {{Definitive Guide}} to {{Reaction Role Assignment}} | {{Journal}} of {{Chemical Information}} and {{Modeling}}},
  date = {2021-01-22},
  url = {https://pubs.acs.org/doi/10.1021/acs.jcim.6b00564},
  urldate = {2021-01-22}
}

@article{whiteGenerativeModelsChemical2010,
  title = {Generative {{Models}} for {{Chemical Structures}}},
  author = {White, David and Wilson, Richard C.},
  date = {2010-07-26},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {50},
  number = {7},
  pages = {1257--1274},
  issn = {1549-9596},
  doi = {10.1021/ci9004089},
  url = {https://doi.org/10.1021/ci9004089},
  urldate = {2018-09-18},
  abstract = {We apply recently developed techniques for pattern recognition to construct a generative model for chemical structure. This approach can be viewed as ligand-based de novo design. We construct a statistical model describing the structural variations present in a set of molecules which may be sampled to generate new structurally similar examples. We prevent the possibility of generating chemically invalid molecules, according to our implicit hydrogen model, by projecting samples onto the nearest chemically valid molecule. By populating the input set with molecules that are active against a target, we show how new molecules may be generated that will likely also be active against the target.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\8ZJC5V2N\\White_Wilson_2010_Generative Models for Chemical Structures.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HJWEWRT2\\ci9004089.html}
}

@article{whiteheadImputationAssayBioactivity2019,
  title = {Imputation of {{Assay Bioactivity Data Using Deep Learning}}},
  author = {Whitehead, T. M. and Irwin, B. W. J. and Hunt, P. and Segall, M. D. and Conduit, G. J.},
  date = {2019-03-25},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {59},
  number = {3},
  pages = {1197--1204},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.8b00768},
  url = {https://doi.org/10.1021/acs.jcim.8b00768},
  urldate = {2019-05-20},
  abstract = {We describe a novel deep learning neural network method and its application to impute assay pIC50 values. Unlike conventional machine learning approaches, this method is trained on sparse bioactivity data as input, typical of that found in public and commercial databases, enabling it to learn directly from correlations between activities measured in different assays. In two case studies on public domain data sets we show that the neural network method outperforms traditional quantitative structure–activity relationship (QSAR) models and other leading approaches. Furthermore, by focusing on only the most confident predictions the accuracy is increased to R2 {$>$} 0.9 using our method, as compared to R2 = 0.44 when reporting all predictions.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NJSKWNT2\\Whitehead et al_2019_Imputation of Assay Bioactivity Data Using Deep Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\J5IEXI69\\acs.jcim.html}
}

@video{WhyGovernmentShouldn2017,
  entrysubtype = {video},
  title = {Why {{The Government Shouldn}}'t {{Break WhatsApp}}},
  date = {2017-07-03},
  url = {https://www.youtube.com/watch?v=CINVwWHlzTY&feature=youtu.be},
  urldate = {2020-11-09},
  abstract = {Encryption backdoors - breaking WhatsApp and iMessage's security to let the government stop Bad Things - sounds like a reasonable idea. Here's why it isn't. A transcript of this video's available here: https://www.facebook.com/notes/tom-sc... CREDITS: Filmed at the Cambridge Centre for Computing History: http://www.computinghistory.org.uk/ Camera by Tomek: https://www.youtube.com/tomek Thanks to everyone who helped proofread my script! REFERENCES: WhatsApp's privacy protections questioned after terror attack: http://www.bbc.co.uk/news/technology-... WhatsApp must be accessible to authorities, says Amber Rudd: https://www.theguardian.com/technolog... UK government renews calls for WhatsApp backdoor after London attack: https://www.theverge.com/2017/3/27/15... Investigatory Powers Act: http://www.legislation.gov.uk/ukpga/2... India is 'ready to use' Blackberry message intercept system: http://www.bbc.co.uk/news/technology-... Revealed: how US and UK spy agencies defeat internet privacy and security: https://www.theguardian.com/world/201... Councils secretly spied on people walking dogs and feeding birds for five years: http://metro.co.uk/2016/12/26/council... [This is basically a rephrase of https://www.theguardian.com/world/201... with a better headline] Poole council spies on family over school claim: http://www.telegraph.co.uk/news/uknew... Security services missed five opportunities to stop the Manchester bomber: http://www.telegraph.co.uk/news/2017/... Reuters reference to "500 active investigations": http://www.reuters.com/article/us-bri... AP: Across US, police officers abuse confidential databases: https://apnews.com/699236946e3140659f... ME: I'm at http://tomscott.com on Twitter at http://twitter.com/tomscott on Facebook at http://facebook.com/tomscott and on Snapchat and Instagram as tomscottgo}
}

@unpublished{widrichModernHopfieldNetworks2020,
  title = {Modern {{Hopfield Networks}} and {{Attention}} for {{Immune Repertoire Classification}}},
  author = {Widrich, Michael and Schäfl, Bernhard and Ramsauer, Hubert and Pavlović, Milena and Gruber, Lukas and Holzleitner, Markus and Brandstetter, Johannes and Sandve, Geir Kjetil and Greiff, Victor and Hochreiter, Sepp and Klambauer, Günter},
  date = {2020-07-16},
  eprint = {2007.13505},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio, stat},
  url = {http://arxiv.org/abs/2007.13505},
  urldate = {2021-04-08},
  abstract = {A central mechanism in machine learning is to identify, store, and recognize patterns. How to learn, access, and retrieve such patterns is crucial in Hopfield networks and the more recent transformer architectures. We show that the attention mechanism of transformer architectures is actually the update rule of modern Hopfield networks that can store exponentially many patterns. We exploit this high storage capacity of modern Hopfield networks to solve a challenging multiple instance learning (MIL) problem in computational biology: immune repertoire classification. Accurate and interpretable machine learning methods solving this problem could pave the way towards new vaccines and therapies, which is currently a very relevant research topic intensified by the COVID-19 crisis. Immune repertoire classification based on the vast number of immunosequences of an individual is a MIL problem with an unprecedentedly massive number of instances, two orders of magnitude larger than currently considered problems, and with an extremely low witness rate. In this work, we present our novel method DeepRC that integrates transformer-like attention, or equivalently modern Hopfield networks, into deep learning architectures for massive MIL such as immune repertoire classification. We demonstrate that DeepRC outperforms all other methods with respect to predictive performance on large-scale experiments, including simulated and real-world virus infection data, and enables the extraction of sequence motifs that are connected to a given disease class. Source code and datasets: https://github.com/ml-jku/DeepRC},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Biomolecules,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\S85S4V5L\\Widrich et al. - 2020 - Modern Hopfield Networks and Attention for Immune .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XLHG2I4K\\2007.html}
}

@online{WieGestalteIch2020,
  title = {Wie gestalte ich Zielvereinbarungen?},
  date = {2020-08-04T12:21:00+02:00},
  url = {https://www.forschung-und-lehre.de/wie-gestalte-ich-zielvereinbarungen-313/},
  urldate = {2020-08-05},
  abstract = {Zielvereinbarungen sind meist sehr individuell. Es lohnt sich daher, die für einen persönlich wichtigen Punkte genau zu definieren.},
  langid = {ngerman},
  file = {C:\Users\USEBPERP\Zotero\storage\MS268A5R\wie-gestalte-ich-zielvereinbarungen-313.html}
}

@article{wildComparison2DFingerprint2000,
  title = {Comparison of {{2D Fingerprint Types}} and {{Hierarchy Level Selection Methods}} for {{Structural Grouping Using Ward}}'s {{Clustering}}},
  author = {Wild, D. J. and Blankley, C. J.},
  date = {2000-01-01},
  journaltitle = {J. Chem. Inf. Comput. Sci.},
  volume = {40},
  number = {1},
  pages = {155--162},
  publisher = {American Chemical Society},
  issn = {0095-2338},
  doi = {10.1021/ci990086j},
  url = {https://doi.org/10.1021/ci990086j},
  urldate = {2022-07-26},
  abstract = {Four different two-dimensional fingerprint types (MACCS, Unity, BCI, and Daylight) and nine methods of selecting optimal cluster levels from the output of a hierarchical clustering algorithm were evaluated for their ability to select clusters that represent chemical series present in some typical examples of chemical compound data sets. The methods were evaluated using a Ward's clustering algorithm on subsets of the publicly available National Cancer Institute HIV data set, as well as with compounds from our corporate data set. We make a number of observations and recommendations about the choice of fingerprint type and cluster level selection methods for use in this type of clustering},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\UEXBA3ES\\Wild and Blankley - 2000 - Comparison of 2D Fingerprint Types and Hierarchy L.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\EPLTHCIB\\ci990086j.html}
}

@article{wildmanPredictionPhysicochemicalParameters1999,
  ids = {wildman1999prediction,wildman1999predictiona},
  title = {Prediction of {{Physicochemical Parameters}} by {{Atomic Contributions}}},
  author = {Wildman, Scott A. and Crippen, Gordon M.},
  date = {1999-09-27},
  journaltitle = {J. Chem. Inf. Comput. Sci.},
  volume = {39},
  number = {5},
  pages = {868--873},
  publisher = {American Chemical Society},
  issn = {0095-2338},
  doi = {10.1021/ci990307l},
  url = {https://doi.org/10.1021/ci990307l},
  urldate = {2019-10-14},
  abstract = {We present a new atom type classification system for use in atom-based calculation of partition coefficient (log P) and molar refractivity (MR) designed in part to address published concerns of previous atomic methods. The 68 atomic contributions to log P have been determined by fitting an extensive training set of 9920 molecules, with r2 = 0.918 and σ = 0.677. A separate set of 3412 molecules was used for the determination of contributions to MR with r2 = 0.997 and σ = 1.43. Both calculations are shown to have high predictive ability.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4NFS7XW7\\Wildman_Crippen_1999_Prediction of Physicochemical Parameters by Atomic Contributions.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YV6IDV9W\\Wildman_Crippen_1999_Prediction of Physicochemical Parameters by Atomic Contributions.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AHSICPF5\\ci990307l.html}
}

@incollection{willettDissimilarityBasedCompoundSelection2001,
  title = {Dissimilarity-{{Based Compound Selection}} for {{Library Design}}},
  booktitle = {Combinatorial {{Library Design}} and {{Evaluation}}},
  author = {Willett, Peter, Valerie J. Gillet},
  date = {2001},
  publisher = {CRC Press},
  abstract = {One of the principal objectives of molecular diversity analysis is to devise computational methods that ensure coverage of the largest possible expanse of chemical space in the search for bioactive molecules. The concept of diversity is normally quantified by using techniques derived from those developed for similarity  searching in chemical databases, which involves measuring the degree of structural similarity (or dissimilarity) between two molecules by a comparison of the  sets of descriptors that characterize those molecules (I). There has thus been much  interest in measures of structural similarity (including both the descriptors that are  employed to characterize molecules, and the coefficients that are employed to  quantify the degree of resemblance between two molecules' sets of associated descriptors) and in ways in which such measures can be used in diversity analyses  (2,3), in particular in methods for selecting compounds to maximize their structural diversity (4).},
  isbn = {978-0-429-18097-2},
  pagetotal = {20}
}

@article{williamsFutureRetrosynthesisSynthetic2021,
  title = {The {{Future}} of {{Retrosynthesis}} and {{Synthetic Planning}}: {{Algorithmic}}, {{Humanistic}} or the {{Interplay}}?},
  shorttitle = {The {{Future}} of {{Retrosynthesis}} and {{Synthetic Planning}}},
  author = {Williams, Craig M. and Dallaston, Madeleine A. and Williams, Craig M. and Dallaston, Madeleine A.},
  date = {2021-05-12},
  journaltitle = {Aust. J. Chem.},
  publisher = {CSIRO PUBLISHING},
  issn = {1445-0038, 1445-0038},
  doi = {10.1071/CH20371},
  url = {https://www.publish.csiro.au/ch/CH20371},
  urldate = {2021-05-17},
  abstract = {The practice of deploying and teaching retrosynthesis is on the cusp of considerable change, which in turn forces practitioners and educators to contemplate whether this impending change will advance or erode the efficiency and elegance of organic synthesis in the future. A short treatise is presented herein that covers the concept of retrosynthesis, along with exemplified methods and theories, and an attempt to comprehend the impact of artificial intelligence in an era when freely and commercially available retrosynthetic and forward synthesis planning programs are increasingly prevalent. Will the computer ever compete with human retrosynthetic design and the art of organic synthesis?},
  langid = {english}
}

@article{williamsSimpleStatisticalGradientfollowing1992,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  date = {1992-05-01},
  journaltitle = {Mach Learn},
  volume = {8},
  number = {3-4},
  pages = {229--256},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00992696},
  url = {https://link.springer.com/article/10.1007/BF00992696},
  urldate = {2018-02-14},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\3F94ER94\\Williams_1992_Simple statistical gradient-following algorithms for connectionist.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8DYX3FVL\\BF00992696.html}
}

@article{wilsonGeneralInefficiencyBatch2003,
  title = {The General Inefficiency of Batch Training for Gradient Descent Learning},
  author = {Wilson, D.Randall and Martinez, Tony R.},
  date = {2003-12},
  journaltitle = {Neural Networks},
  volume = {16},
  number = {10},
  pages = {1429--1451},
  issn = {08936080},
  doi = {10.1016/S0893-6080(03)00138-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608003001382},
  urldate = {2019-08-28},
  abstract = {Gradient descent training of neural networks can be done in either a batch or on-line manner. A widely held myth in the neural network community is that batch training is as fast or faster and/or more ‘correct’ than on-line training because it supposedly uses a better approximation of the true gradient for its weight updates. This paper explains why batch training is almost always slower than on-line training—often orders of magnitude slower—especially on large training sets. The main reason is due to the ability of on-line training to follow curves in the error surface throughout each epoch, which allows it to safely use a larger learning rate and thus converge with less iterations through the training data. Empirical results on a large (20,000-instance) speech recognition task and on 26 other learning tasks demonstrate that convergence can be reached significantly faster using on-line training than batch training, with no apparent difference in accuracy.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\HSU8AW2Q\Wilson and Martinez - 2003 - The general inefficiency of batch training for gra.pdf}
}

@inproceedings{wilsonInefficiencyBatchTraining2000,
  title = {The {{Inefficiency}} of {{Batch Training}} for {{Large Training Sets}}},
  booktitle = {{{IJCNN}}},
  author = {Wilson, D. Randall and Martinez, Tony R.},
  date = {2000},
  doi = {10.1109/IJCNN.2000.10003},
  abstract = {Multilayer perceptrons are often trained using error backpropagation (BP). BP training can be done in either a batch or continuous manner. Claims have frequently been made that batch training is faster and/or more "correct" than continuous training because it uses a better approximation of the true gradient for its weight updates. These claims are often supported by empirical evidence on very small data sets. These claims are untrue, however, for large training sets. This paper explains why batch training is much slower than continuous training for large training sets. Various levels of semi-batch training used on a 20,000-instance speech recognition task show a roughly linear increase in training time required with an increase in batch size.},
  keywords = {Approximation,Backpropagation,Batch processing,Gradient,Multilayer perceptron,Semiconductor industry,Speech recognition},
  file = {C:\Users\USEBPERP\Zotero\storage\AVKB6XMX\Wilson_Martinez_2000_The Inefficiency of Batch Training for Large Training Sets.pdf}
}

@article{winterEfficientMultiobjectiveMolecular2019,
  title = {Efficient Multi-Objective Molecular Optimization in a Continuous Latent Space},
  author = {Winter, Robin and Montanari, Floriane and Steffen, Andreas and Briem, Hans and Noé, Frank and Clevert, Djork-Arné},
  date = {2019-08-28},
  journaltitle = {Chem. Sci.},
  volume = {10},
  number = {34},
  pages = {8016--8024},
  publisher = {The Royal Society of Chemistry},
  issn = {2041-6539},
  doi = {10.1039/C9SC01928F},
  url = {https://pubs.rsc.org/en/content/articlelanding/2019/sc/c9sc01928f},
  urldate = {2020-07-23},
  abstract = {One of the main challenges in small molecule drug discovery is finding novel chemical compounds with desirable properties. In this work, we propose a novel method that combines in silico prediction of molecular properties such as biological activity or pharmacokinetics with an in silico optimization algorithm, namely Particle Swarm Optimization. Our method takes a starting compound as input and proposes new molecules with more desirable (predicted) properties. It navigates a machine-learned continuous representation of a drug-like chemical space guided by a defined objective function. The objective function combines multiple in silico prediction models, defined desirability ranges and substructure constraints. We demonstrate that our proposed method is able to consistently find more desirable molecules for the studied tasks in relatively short time. We hope that our method can support medicinal chemists in accelerating and improving the lead optimization process.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\ETNTC8B2\\Winter et al_2019_Efficient multi-objective molecular optimization in a continuous latent space.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ETDKJ5AN\\c9sc01928f.html}
}

@article{winterLearningContinuousDatadriven2019,
  title = {Learning Continuous and Data-Driven Molecular Descriptors by Translating Equivalent Chemical Representations},
  author = {Winter, Robin and Montanari, Floriane and Noé, Frank and Clevert, Djork-Arné},
  date = {2019-02-06},
  journaltitle = {Chem. Sci.},
  volume = {10},
  number = {6},
  pages = {1692--1701},
  publisher = {The Royal Society of Chemistry},
  issn = {2041-6539},
  doi = {10.1039/C8SC04175J},
  url = {https://pubs.rsc.org/en/content/articlelanding/2019/sc/c8sc04175j},
  urldate = {2020-09-08},
  abstract = {There has been a recent surge of interest in using machine learning across chemical space in order to predict properties of molecules or design molecules and materials with the desired properties. Most of this work relies on defining clever feature representations, in which the chemical graph structure is encoded in a uniform way such that predictions across chemical space can be made. In this work, we propose to exploit the powerful ability of deep neural networks to learn a feature representation from low-level encodings of a huge corpus of chemical structures. Our model borrows ideas from neural machine translation: it translates between two semantically equivalent but syntactically different representations of molecular structures, compressing the meaningful information both representations have in common in a low-dimensional representation vector. Once the model is trained, this representation can be extracted for any new molecule and utilized as a descriptor. In fair benchmarks with respect to various human-engineered molecular fingerprints and graph-convolution models, our method shows competitive performance in modelling quantitative structure–activity relationships in all analysed datasets. Additionally, we show that our descriptor significantly outperforms all baseline molecular fingerprints in two ligand-based virtual screening tasks. Overall, our descriptors show the most consistent performances in all experiments. The continuity of the descriptor space and the existence of the decoder that permits deducing a chemical structure from an embedding vector allow for exploration of the space and open up new opportunities for compound optimization and idea generation.},
  langid = {english},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\468AIVV7\\Winter et al_2019_Learning continuous and data-driven molecular descriptors by translating.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZSJQ537N\\c8sc04175j.html}
}

@article{woodwardCoverageScoreModel2022,
  title = {Coverage {{Score}}: {{A Model Agnostic Method}} to {{Efficiently Explore Chemical Space}}},
  shorttitle = {Coverage {{Score}}},
  author = {Woodward, Daniel J. and Bradley, Anthony R. and family=Hoorn, given=Willem P., prefix=van, useprefix=true},
  date = {2022-07-22},
  journaltitle = {J. Chem. Inf. Model.},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.2c00258},
  url = {https://doi.org/10.1021/acs.jcim.2c00258},
  urldate = {2022-07-26},
  abstract = {Selecting the most appropriate compounds to synthesize and test is a vital aspect of drug discovery. Methods like clustering and diversity present weaknesses in selecting the optimal sets for information gain. Active learning techniques often rely on an initial model and computationally expensive semi-supervised batch selection. Herein, we describe a new subset-based selection method, Coverage Score, that combines Bayesian statistics and information entropy to balance representation and diversity to select a maximally informative subset. Coverage Score can be influenced by prior selections and desirable properties. In this paper, subsets selected through Coverage Score are compared against subsets selected through model-independent and model-dependent techniques for several datasets. In drug-like chemical space, Coverage Score consistently selects subsets that lead to more accurate predictions compared to other selection methods. Subsets selected through Coverage Score produced Random Forest models that have a root-mean-square-error up to 12.8\% lower than subsets selected at random and can retain up to 99\% of the structural dissimilarity of a diversity selection.},
  file = {C:\Users\USEBPERP\Zotero\storage\8DCN3IF9\Woodward et al. - 2022 - Coverage Score A Model Agnostic Method to Efficie.pdf}
}

@article{woottonPhysicochemicalactivityRelationsPractice1975,
  title = {Physicochemical-Activity Relations in Practice. 2. {{Rational}} Selection of Benzenoid Substituents},
  author = {Wootton, Raymond and Cranfield, Robert and Sheppey, G. Clive and Goodford, Peter J.},
  date = {1975-06},
  journaltitle = {J. Med. Chem.},
  volume = {18},
  number = {6},
  pages = {607--613},
  issn = {0022-2623, 1520-4804},
  doi = {10.1021/jm00240a017},
  url = {https://pubs.acs.org/doi/abs/10.1021/jm00240a017},
  urldate = {2022-07-01},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\XHDESCW7\Wootton et al. - 1975 - Physicochemical-activity relations in practice. 2..pdf}
}

@online{wrightNewStateArt2019,
  title = {New {{State}} of the {{Art AI Optimizer}}: {{Rectified Adam}} ({{RAdam}}).},
  shorttitle = {New {{State}} of the {{Art AI Optimizer}}},
  author = {Wright, Less},
  date = {2019-08-20T18:20:03},
  url = {https://medium.com/@lessw/new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b},
  urldate = {2020-09-25},
  abstract = {A new paper by Liu, Jian, He et al introduces RAdam, or “Rectified Adam”. It’s a new variation of the classic Adam optimizer that provides…},
  langid = {english},
  organization = {Medium},
  file = {C:\Users\USEBPERP\Zotero\storage\MTWBIR2W\new-state-of-the-art-ai-optimizer-rectified-adam-radam-5d854730807b.html}
}

@article{wuCurrentTimeSeries2021,
  title = {Current {{Time Series Anomaly Detection Benchmarks}} Are {{Flawed}} and Are {{Creating}} the {{Illusion}} of {{Progress}}},
  author = {Wu, Renjie and Keogh, Eamonn J.},
  date = {2021},
  journaltitle = {IEEE Trans. Knowl. Data Eng.},
  eprint = {2009.13807},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {1--1},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2021.3112126},
  url = {http://arxiv.org/abs/2009.13807},
  urldate = {2022-08-17},
  abstract = {Time series anomaly detection has been a perennially important topic in data science, with papers dating back to the 1950s. However, in recent years there has been an explosion of interest in this topic, much of it driven by the success of deep learning in other domains and for other time series tasks. Most of these papers test on one or more of a handful of popular benchmark datasets, created by Yahoo, Numenta, NASA, etc. In this work we make a surprising claim. The majority of the individual exemplars in these datasets suffer from one or more of four flaws. Because of these four flaws, we believe that many published comparisons of anomaly detection algorithms may be unreliable, and more importantly, much of the apparent progress in recent years may be illusionary. In addition to demonstrating these claims, with this paper we introduce the UCR Time Series Anomaly Archive. We believe that this resource will perform a similar role as the UCR Time Series Classification Archive, by providing the community with a benchmark that allows meaningful comparisons between approaches and a meaningful gauge of overall progress.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\KXGYHGWY\\Wu and Keogh - 2021 - Current Time Series Anomaly Detection Benchmarks a.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\UWFPMNN7\\2009.html}
}

@article{wuDevelopingUnsupervisedRealtime2022,
  title = {Developing an {{Unsupervised Real-time Anomaly Detection Scheme}} for {{Time Series}} with {{Multi-seasonality}}},
  author = {Wu, Wentai and He, Ligang and Lin, Weiwei and Su, Yi and Cui, Yuhua and Maple, Carsten and Jarvis, Stephen},
  date = {2022-09-01},
  journaltitle = {IEEE Trans. Knowl. Data Eng.},
  volume = {34},
  number = {9},
  eprint = {1908.01146},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  pages = {4147--4160},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2020.3035685},
  url = {http://arxiv.org/abs/1908.01146},
  urldate = {2023-01-27},
  abstract = {On-line detection of anomalies in time series is a key technique used in various event-sensitive scenarios such as robotic system monitoring, smart sensor networks and data center security. However, the increasing diversity of data sources and the variety of demands make this task more challenging than ever. Firstly, the rapid increase in unlabeled data means supervised learning is becoming less suitable in many cases. Secondly, a large portion of time series data have complex seasonality features. Thirdly, on-line anomaly detection needs to be fast and reliable. In light of this, we have developed a prediction-driven, unsupervised anomaly detection scheme, which adopts a backbone model combining the decomposition and the inference of time series data. Further, we propose a novel metric, Local Trend Inconsistency (LTI), and an efficient detection algorithm that computes LTI in a real-time manner and scores each data point robustly in terms of its probability of being anomalous. We have conducted extensive experimentation to evaluate our algorithm with several datasets from both public repositories and production environments. The experimental results show that our scheme outperforms existing representative anomaly detection algorithms in terms of the commonly used metric, Area Under Curve (AUC), while achieving the desired efficiency.},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\RKXNFHAX\\Wu et al. - 2022 - Developing an Unsupervised Real-time Anomaly Detec.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VF64GYKE\\1908.html}
}

@unpublished{wuUnderstandingShortHorizonBias2018,
  title = {Understanding {{Short-Horizon Bias}} in {{Stochastic Meta-Optimization}}},
  author = {Wu, Yuhuai and Ren, Mengye and Liao, Renjie and Grosse, Roger},
  date = {2018-03-06},
  eprint = {1803.02021},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1803.02021},
  urldate = {2019-09-27},
  abstract = {Careful tuning of the learning rate, or even schedules thereof, can be crucial to effective neural net training. There has been much recent interest in gradient-based meta-optimization, where one tunes hyperparameters, or even learns an optimizer, in order to minimize the expected loss when the training procedure is unrolled. But because the training procedure must be unrolled thousands of times, the meta-objective must be defined with an orders-of-magnitude shorter time horizon than is typical for neural net training. We show that such short-horizon meta-objectives cause a serious bias towards small step sizes, an effect we term short-horizon bias. We introduce a toy problem, a noisy quadratic cost function, on which we analyze short-horizon bias by deriving and comparing the optimal schedules for short and long time horizons. We then run meta-optimization experiments (both offline and online) on standard benchmark datasets, showing that meta-optimization chooses too small a learning rate by multiple orders of magnitude, even when run with a moderately long time horizon (100 steps) typical of work in the area. We believe short-horizon bias is a fundamental problem that needs to be addressed if meta-optimization is to scale to practical neural net training regimes.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NQBJ6XNM\\Wu et al_2018_Understanding Short-Horizon Bias in Stochastic Meta-Optimization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YUEJXYFE\\1803.html}
}

@inproceedings{wuUnsupervisedFeatureLearning2018,
  ids = {wuUnsupervisedFeatureLearning2018a},
  title = {Unsupervised {{Feature Learning}} via {{Non-Parametric Instance Discrimination}}},
  author = {Wu, Zhirong and Xiong, Yuanjun and Yu, Stella X. and Lin, Dahua},
  date = {2018},
  pages = {3733--3742},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html},
  urldate = {2021-04-08},
  eventtitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\37B95PUX\\Wu et al. - 2018 - Unsupervised Feature Learning via Non-Parametric I.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\65RSJLK8\\Wu et al. - 2018 - Unsupervised Feature Learning via Non-Parametric I.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\2LQCHWEN\\Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\3QH3VJDX\\Wu_Unsupervised_Feature_Learning_CVPR_2018_paper.html}
}

@inproceedings{xieHowMuchSpace2023,
  title = {How {{Much Space Has Been Explored}}? {{Measuring}} the {{Chemical Space Covered}} by {{Databases}} and {{Machine-Generated Molecules}}},
  shorttitle = {How {{Much Space Has Been Explored}}?},
  author = {Xie, Yutong and Xu, Ziqiao and Ma, Jiaqi and Mei, Qiaozhu},
  date = {2023-02-01},
  url = {https://openreview.net/forum?id=Yo06F8kfMa1},
  urldate = {2023-10-29},
  abstract = {Forming a molecular candidate set that contains a wide range of potentially effective compounds is crucial to the success of drug discovery. While most databases and machine-learning-based generation models aim to optimize particular chemical properties, there is limited literature on how to properly measure the coverage of the chemical space by those candidates included or generated. This problem is challenging due to the lack of formal criteria to select good measures of the chemical space. In this paper, we propose a novel evaluation framework for measures of the chemical space based on two analyses: an axiomatic analysis with three intuitive axioms that a good measure should obey, and an empirical analysis on the correlation between a measure and a proxy gold standard. Using this framework, we are able to identify \#Circles, a new measure of chemical space coverage, which is superior to existing measures both analytically and empirically. We further evaluate how well the existing databases and generation models cover the chemical space in terms of \#Circles. The results suggest that many generation models fail to explore a larger space over existing databases, which leads to new opportunities for improving generation models by encouraging exploration.},
  eventtitle = {{{ICLR}}},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\WLVHX8MA\Xie et al. - 2022 - How Much Space Has Been Explored Measuring the Ch.pdf}
}

@unpublished{xieMARSMarkovMolecular2021,
  title = {{{MARS}}: {{Markov Molecular Sampling}} for {{Multi-objective Drug Discovery}}},
  shorttitle = {{{MARS}}},
  author = {Xie, Yutong and Shi, Chence and Zhou, Hao and Yang, Yuwei and Zhang, Weinan and Yu, Yong and Li, Lei},
  date = {2021-03-18},
  eprint = {2103.10432},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2103.10432},
  urldate = {2021-05-10},
  abstract = {Searching for novel molecules with desired chemical properties is crucial in drug discovery. Existing work focuses on developing neural models to generate either molecular sequences or chemical graphs. However, it remains a big challenge to find novel and diverse compounds satisfying several properties. In this paper, we propose MARS, a method for multi-objective drug molecule discovery. MARS is based on the idea of generating the chemical candidates by iteratively editing fragments of molecular graphs. To search for high-quality candidates, it employs Markov chain Monte Carlo sampling (MCMC) on molecules with an annealing scheme and an adaptive proposal. To further improve sample efficiency, MARS uses a graph neural network (GNN) to represent and select candidate edits, where the GNN is trained on-the-fly with samples from MCMC. Experiments show that MARS achieves state-of-the-art performance in various multi-objective settings where molecular bio-activity, drug-likeness, and synthesizability are considered. Remarkably, in the most challenging setting where all four objectives are simultaneously optimized, our approach outperforms previous methods significantly in comprehensive evaluations. The code is available at https://github.com/yutxie/mars.},
  keywords = {Computer Science - Computational Engineering Finance and Science,Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\XTZUUWXP\\32ed9abd-0c89-48e6-819a-2f5ff40b0a9b.js;C\:\\Users\\USEBPERP\\Zotero\\storage\\EDH33DAR\\2103.html}
}

@unpublished{xingWalkSGD2018,
  ids = {xing2018walka},
  title = {A {{Walk}} with {{SGD}}},
  author = {Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
  date = {2018-05-29},
  eprint = {1802.08770},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1802.08770},
  urldate = {2020-09-28},
  abstract = {We present novel empirical observations regarding how stochastic gradient descent (SGD) navigates the loss landscape of over-parametrized deep neural networks (DNNs). These observations expose the qualitatively different roles of learning rate and batch-size in DNN optimization and generalization. Specifically we study the DNN loss surface along the trajectory of SGD by interpolating the loss surface between parameters from consecutive \textbackslash textit\{iterations\} and tracking various metrics during training. We find that the loss interpolation between parameters before and after each training iteration's update is roughly convex with a minimum (\textbackslash textit\{valley floor\}) in between for most of the training. Based on this and other metrics, we deduce that for most of the training update steps, SGD moves in valley like regions of the loss surface by jumping from one valley wall to another at a height above the valley floor. This 'bouncing between walls at a height' mechanism helps SGD traverse larger distance for small batch sizes and large learning rates which we find play qualitatively different roles in the dynamics. While a large learning rate maintains a large height from the valley floor, a small batch size injects noise facilitating exploration. We find this mechanism is crucial for generalization because the valley floor has barriers and this exploration above the valley floor allows SGD to quickly travel far away from the initialization point (without being affected by barriers) and find flatter regions, corresponding to better generalization.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\5BA8CTJY\\Xing et al. - 2018 - A Walk with SGD.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\UIL2A8AC\\Xing et al_2018_A Walk with SGD.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8IR4ISGI\\1802.html}
}

@article{xuDeepLearningMolecular2019,
  title = {Deep Learning for Molecular Generation},
  author = {Xu, Youjun and Lin, Kangjie and Wang, Shiwei and Wang, Lei and Cai, Chenjing and Song, Chen and Lai, Luhua and Pei, Jianfeng},
  date = {2019-01-30},
  journaltitle = {Future Medicinal Chemistry},
  volume = {11},
  number = {6},
  pages = {567--597},
  issn = {1756-8919},
  doi = {10.4155/fmc-2018-0358},
  url = {https://www.future-science.com/doi/abs/10.4155/fmc-2018-0358},
  urldate = {2019-10-02},
  abstract = {De novo drug design aims to generate novel chemical compounds with desirable chemical and pharmacological properties from scratch using computer-based methods. Recently, deep generative neural networks have become a very active research frontier in de novo drug discovery, both in theoretical and in experimental evidence, shedding light on a promising new direction of automatic molecular generation and optimization. In this review, we discussed recent development of deep learning models for molecular generation and summarized them as four different generative architectures with four different optimization strategies. We also discussed future directions of deep generative models for de novo drug design.},
  file = {C:\Users\USEBPERP\Zotero\storage\3N3RDE3I\fmc-2018-0358.html}
}

@article{xuDemystifyingMultitaskDeep2017,
  title = {Demystifying {{Multitask Deep Neural Networks}} for {{Quantitative Structure}}–{{Activity Relationships}}},
  author = {Xu, Yuting and Ma, Junshui and Liaw, Andy and Sheridan, Robert P. and Svetnik, Vladimir},
  date = {2017-10-23},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {57},
  number = {10},
  pages = {2490--2504},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.7b00087},
  url = {https://doi.org/10.1021/acs.jcim.7b00087},
  urldate = {2019-11-05},
  abstract = {Deep neural networks (DNNs) are complex computational models that have found great success in many artificial intelligence applications, such as computer vision1,2 and natural language processing.3,4 In the past four years, DNNs have also generated promising results for quantitative structure–activity relationship (QSAR) tasks.5,6 Previous work showed that DNNs can routinely make better predictions than traditional methods, such as random forests, on a diverse collection of QSAR data sets. It was also found that multitask DNN models—those trained on and predicting multiple QSAR properties simultaneously—outperform DNNs trained separately on the individual data sets in many, but not all, tasks. To date there has been no satisfactory explanation of why the QSAR of one task embedded in a multitask DNN can borrow information from other unrelated QSAR tasks. Thus, using multitask DNNs in a way that consistently provides a predictive advantage becomes a challenge. In this work, we explored why multitask DNNs make a difference in predictive performance. Our results show that during prediction a multitask DNN does borrow “signal” from molecules with similar structures in the training sets of the other tasks. However, whether this borrowing leads to better or worse predictive performance depends on whether the activities are correlated. On the basis of this, we have developed a strategy to use multitask DNNs that incorporate prior domain knowledge to select training sets with correlated activities, and we demonstrate its effectiveness on several examples.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\AKJQSRB3\\Xu et al. - 2017 - Demystifying Multitask Deep Neural Networks for Qu.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\N4RHGI6H\\acs.jcim.html}
}

@unpublished{xuShowAttendTell2015,
  title = {Show, {{Attend}} and {{Tell}}: {{Neural Image Caption Generation}} with {{Visual Attention}}},
  shorttitle = {Show, {{Attend}} and {{Tell}}},
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  date = {2015-02-10},
  eprint = {1502.03044},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1502.03044},
  urldate = {2018-12-19},
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-theart performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\USEBPERP\Zotero\storage\9ATWDUDJ\Xu et al_2015_Show, Attend and Tell.pdf}
}

@inproceedings{xuUnsupervisedAnomalyDetection2018,
  title = {Unsupervised {{Anomaly Detection}} via {{Variational Auto-Encoder}} for {{Seasonal KPIs}} in {{Web Applications}}},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}} on {{World Wide Web}} - {{WWW}} '18},
  author = {Xu, Haowen and Feng, Yang and Chen, Jie and Wang, Zhaogang and Qiao, Honglin and Chen, Wenxiao and Zhao, Nengwen and Li, Zeyan and Bu, Jiahao and Li, Zhihan and Liu, Ying and Zhao, Youjian and Pei, Dan},
  date = {2018},
  pages = {187--196},
  publisher = {ACM Press},
  location = {Lyon, France},
  doi = {10.1145/3178876.3185996},
  url = {http://dl.acm.org/citation.cfm?doid=3178876.3185996},
  urldate = {2023-01-19},
  abstract = {To ensure undisrupted business, large Internet companies need to closely monitor various KPIs (e.g., Page Views, number of online users, and number of orders) of its Web applications, to accurately detect anomalies and trigger timely troubleshooting/mitigation. However, anomaly detection for these seasonal KPIs with various patterns and data quality has been a great challenge, especially without labels. In this paper, we proposed Donut, an unsupervised anomaly detection algorithm based on VAE. Thanks to a few of our key techniques, Donut greatly outperforms a state-of-arts supervised ensemble approach and a baseline VAE approach, and its best F-scores range from 0.75 to 0.9 for the studied KPIs from a top global Internet company. We come up with a novel KDE interpretation of reconstruction for Donut, making it the first VAE-based anomaly detection algorithm with solid theoretical explanation.},
  eventtitle = {The 2018 {{World Wide Web Conference}}},
  isbn = {978-1-4503-5639-8},
  langid = {english},
  keywords = {anomaly detection,seasonal KPI,variational auto-encoder},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4EK2ZA29\\Xu et al. - 2018 - Unsupervised Anomaly Detection via Variational Aut.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\57MSN7VA\\Xu et al. - 2018 - Unsupervised Anomaly Detection via Variational Aut.pdf}
}

@article{yairiFaultDetectionMining,
  title = {Fault {{Detection}} by {{Mining Association Rules}} from {{House-keeping Data}}},
  author = {Yairi, Dr Takehisa},
  abstract = {This paper proposes a novel anomaly detection method for spacecraft systems based on data-mining techniques. This method automatically constructs a system behavior model in the form of a set of rules by applying pattern clustering and association rule mining to the time-series data obtained in the learning phase, then detects anomalies by checking the subsequent on-line data with the acquired rules. A major advantage of this approach is that it requires little a priori knowledge on the system.},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\GVIHPCAI\Yairi - Fault Detection by Mining Association Rules from H.pdf}
}

@article{yairiFaultDetectionMining2001,
  title = {Fault {{Detection}} by {{Mining Association Rules}} from {{Housekeeping Data}}},
  author = {Yairi, Takehisa and Kato, Yoshikiyo and Hori, Koichi},
  date = {2001-06-09},
  abstract = {This paper proposes a novel anomaly detection method for spacecraft systems based on data-mining techniques. This method automatically constructs a system behavior model in the form of a set of rules by applying pattern clustering and association rule mining to the time-series data obtained in the learn- ing phase, then detects anomalies by checking the subsequent on-line data with the acquired rules. A major advantage of this approach is that it requires little a priori knowledge on the system.},
  file = {C:\Users\USEBPERP\Zotero\storage\3SFCJWGL\Yairi et al. - 2001 - Fault Detection by Mining Association Rules from H.pdf}
}

@article{yangChemTSEfficientPython2017,
  title = {{{ChemTS}}: An Efficient Python Library for de Novo Molecular Generation},
  shorttitle = {{{ChemTS}}},
  author = {Yang, Xiufeng and Zhang, Jinzhe and Yoshizoe, Kazuki and Terayama, Kei and Tsuda, Koji},
  date = {2017-12-31},
  journaltitle = {Science and Technology of Advanced Materials},
  volume = {18},
  number = {1},
  eprint = {29435094},
  eprinttype = {pmid},
  pages = {972--976},
  issn = {1468-6996},
  doi = {10.1080/14686996.2017.1401424},
  url = {https://doi.org/10.1080/14686996.2017.1401424},
  urldate = {2019-05-06},
  abstract = {Automatic design of organic materials requires black-box optimization in a vast chemical space. In conventional molecular design algorithms, a molecule is built as a combination of predetermined fragments. Recently, deep neural network models such as variational autoencoders and recurrent neural networks (RNNs) are shown to be effective in de novo design of molecules without any predetermined fragments. This paper presents a novel Python library ChemTS that explores the chemical space by combining Monte Carlo tree search and an RNN. In a benchmarking problem of optimizing the octanol-water partition coefficient and synthesizability, our algorithm showed superior efficiency in finding high-scoring molecules. ChemTS is available at https://github.com/tsudalab/ChemTS.},
  keywords = {404 Materials informatics / Genomics,60 New topics/Others,Computer Science - Computational Engineering Finance and Science,Molecular design,Monte Carlo tree search,Physics - Chemical Physics,python library,recurrent neural network},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\LBXR9U3N\\Yang et al_2017_ChemTS3.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\PZG96MGW\\Yang et al_2017_ChemTS.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XA3KU5AD\\Yang et al_2017_ChemTS2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AGA7VUMM\\14686996.2017.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\CRQDFRUV\\1710.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\DF2VSR94\\1710.html}
}

@unpublished{yangMeanFieldTheory2019,
  title = {A {{Mean Field Theory}} of {{Batch Normalization}}},
  author = {Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S.},
  date = {2019-02-21},
  eprint = {1902.08129},
  eprinttype = {arxiv},
  eprintclass = {cond-mat},
  url = {http://arxiv.org/abs/1902.08129},
  urldate = {2019-03-08},
  abstract = {We develop a mean field theory for batch normalization in fully-connected feedforward neural networks. In so doing, we provide a precise characterization of signal propagation and gradient backpropagation in wide batch-normalized networks at initialization. Our theory shows that gradient signals grow exponentially in depth and that these exploding gradients cannot be eliminated by tuning the initial weight variances or by adjusting the nonlinear activation function. Indeed, batch normalization itself is the cause of gradient explosion. As a result, vanilla batch-normalized networks without skip connections are not trainable at large depths for common initialization schemes, a prediction that we verify with a variety of empirical simulations. While gradient explosion cannot be eliminated, it can be reduced by tuning the network close to the linear regime, which improves the trainability of deep batch-normalized networks without residual connections. Finally, we investigate the learning dynamics of batch-normalized networks and observe that after a single step of optimization the networks achieve a relatively stable equilibrium in which gradients have dramatically smaller dynamic range. Our theory leverages Laplace, Fourier, and Gegenbauer transforms and we derive new identities that may be of independent interest.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Mathematics - Dynamical Systems},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\KJVWUE7V\\Yang et al_2019_A Mean Field Theory of Batch Normalization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\DJSNJHSW\\1902.html}
}

@unpublished{yangXLNetGeneralizedAutoregressive2019,
  title = {{{XLNet}}: {{Generalized Autoregressive Pretraining}} for {{Language Understanding}}},
  shorttitle = {{{XLNet}}},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
  date = {2019-06-19},
  eprint = {1906.08237},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1906.08237},
  urldate = {2019-06-26},
  abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\TCX3DZWY\\Yang et al_2019_XLNet.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\VDWPC967\\1906.html}
}

@unpublished{yanRetroXpertDecomposeRetrosynthesis2020,
  title = {{{RetroXpert}}: {{Decompose Retrosynthesis Prediction}} like a {{Chemist}}},
  shorttitle = {{{RetroXpert}}},
  author = {Yan, Chaochao and Ding, Qianggang and Zhao, Peilin and Zheng, Shuangjia and Yang, Jinyu and Yu, Yang and Huang, Junzhou},
  date = {2020-11-03},
  eprint = {2011.02893},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2011.02893},
  urldate = {2021-05-05},
  abstract = {Retrosynthesis is the process of recursively decomposing target molecules into available building blocks. It plays an important role in solving problems in organic synthesis planning. To automate or assist in the retrosynthesis analysis, various retrosynthesis prediction algorithms have been proposed. However, most of them are cumbersome and lack interpretability about their predictions. In this paper, we devise a novel template-free algorithm for automatic retrosynthetic expansion inspired by how chemists approach retrosynthesis prediction. Our method disassembles retrosynthesis into two steps: i) identify the potential reaction center of the target molecule through a novel graph neural network and generate intermediate synthons, and ii) generate the reactants associated with synthons via a robust reactant generation model. While outperforming the state-of-the-art baselines by a significant margin, our model also provides chemically reasonable interpretation.},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods}
}

@video{ycombinatorHowFutureBillionaires2022,
  entrysubtype = {video},
  title = {How {{Future Billionaires Get Sh}}*t {{Done}}},
  editor = {{Y Combinator}},
  editortype = {director},
  date = {2022-04-01},
  url = {https://www.youtube.com/watch?v=ephzgxgOjR0},
  urldate = {2023-12-07},
  abstract = {Dalton Caldwell and Michael Seibel take a look at Paul Graham's essay "Maker's Schedule, Manager's Schedule" and share tips on how to be more effective and productive on the journey to creating a billion dollar business. Read PG's essay here: http://www.paulgraham.com/makerssched... Apply to Y Combinator: https://yc.link/DandM-apply Work at a Startup: https://yc.link/DandM-jobs Chapters (Powered by https://bit.ly/chapterme-yc) -  00:00 - How Future Billionaires Get Sh*t Done 00:38 - PG Essay 01:18 - Maker Schedule 03:59 - The Right Time 05:01 - Structure of YC 05:59 - Manager Schedule 06:36 - Meetings 07:38 - Visible KPIs 09:38 - Your Main Focus 10:51 - Great founders not do 👇 11:01 - Social Media 13:33 - Tools for Time 14:10 - Startup Mentorship 16:00 - Hedging Bets}
}

@inproceedings{yehMatrixProfileAll2016,
  title = {Matrix {{Profile I}}: {{All Pairs Similarity Joins}} for {{Time Series}}: {{A Unifying View That Includes Motifs}}, {{Discords}} and {{Shapelets}}},
  shorttitle = {Matrix {{Profile I}}},
  booktitle = {2016 {{IEEE}} 16th {{International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Yeh, Chin-Chia Michael and Zhu, Yan and Ulanova, Liudmila and Begum, Nurjahan and Ding, Yifei and Dau, Hoang Anh and Silva, Diego Furtado and Mueen, Abdullah and Keogh, Eamonn},
  date = {2016-12},
  pages = {1317--1322},
  publisher = {IEEE},
  location = {Barcelona, Spain},
  doi = {10.1109/ICDM.2016.0179},
  url = {http://ieeexplore.ieee.org/document/7837992/},
  urldate = {2023-04-11},
  abstract = {The all-pairs-similarity-search (or similarity join) problem has been extensively studied for text and a handful of other datatypes. However, surprisingly little progress has been made on similarity joins for time series subsequences. The lack of progress probably stems from the daunting nature of the problem. For even modest sized datasets the obvious nested-loop algorithm can take months, and the typical speed-up techniques in this domain (i.e., indexing, lower-bounding, triangularinequality pruning and early abandoning) at best produce one or two orders of magnitude speedup. In this work we introduce a novel scalable algorithm for time series subsequence all-pairssimilarity-search. For exceptionally large datasets, the algorithm can be trivially cast as an anytime algorithm and produce highquality approximate solutions in reasonable time. The exact similarity join algorithm computes the answer to the time series motif and time series discord problem as a side-effect, and our algorithm incidentally provides the fastest known algorithm for both these extensively-studied problems. We demonstrate the utility of our ideas for many time series data mining problems, including motif discovery, novelty discovery, shapelet discovery, semantic segmentation, density estimation, and contrast set mining.},
  eventtitle = {2016 {{IEEE}} 16th {{International Conference}} on {{Data Mining}} ({{ICDM}})},
  isbn = {978-1-5090-5473-2},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\JTK3QKEX\Yeh et al. - 2016 - Matrix Profile I All Pairs Similarity Joins for T.pdf}
}

@unpublished{yinGradientDiversityKey2017,
  title = {Gradient {{Diversity}}: A {{Key Ingredient}} for {{Scalable Distributed Learning}}},
  shorttitle = {Gradient {{Diversity}}},
  author = {Yin, Dong and Pananjady, Ashwin and Lam, Max and Papailiopoulos, Dimitris and Ramchandran, Kannan and Bartlett, Peter},
  date = {2017-06-18},
  eprint = {1706.05699},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.05699},
  urldate = {2019-09-02},
  abstract = {It has been experimentally observed that distributed implementations of mini-batch stochastic gradient descent (SGD) algorithms exhibit speedup saturation and decaying generalization ability beyond a particular batch-size. In this work, we present an analysis hinting that high similarity between concurrently processed gradients may be a cause of this performance degradation. We introduce the notion of gradient diversity that measures the dissimilarity between concurrent gradient updates, and show its key role in the performance of mini-batch SGD. We prove that on problems with high gradient diversity, mini-batch SGD is amenable to better speedups, while maintaining the generalization performance of serial (one sample) SGD. We further establish lower bounds on convergence where mini-batch SGD slows down beyond a particular batch-size, solely due to the lack of gradient diversity. We provide experimental evidence indicating the key role of gradient diversity in distributed learning, and discuss how heuristics like dropout, Langevin dynamics, and quantization can improve it.},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\97U43K8I\\Yin et al. - 2017 - Gradient Diversity a Key Ingredient for Scalable .pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\AAJERFTK\\1706.html}
}

@unpublished{yooLearningLossActive2019,
  title = {Learning {{Loss}} for {{Active Learning}}},
  author = {Yoo, Donggeun and Kweon, In So},
  date = {2019-05-09},
  eprint = {1905.03677},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1905.03677},
  urldate = {2019-06-04},
  abstract = {The performance of deep neural networks improves with more annotated data. The problem is that the budget for annotation is limited. One solution to this is active learning, where a model asks human to annotate data that it perceived as uncertain. A variety of recent methods have been proposed to apply active learning to deep networks but most of them are either designed specific for their target tasks or computationally inefficient for large networks. In this paper, we propose a novel active learning method that is simple but task-agnostic, and works efficiently with the deep networks. We attach a small parametric module, named "loss prediction module," to a target network, and learn it to predict target losses of unlabeled inputs. Then, this module can suggest data that the target model is likely to produce a wrong prediction. This method is task-agnostic as networks are learned from a single loss regardless of target tasks. We rigorously validate our method through image classification, object detection, and human pose estimation, with the recent network architectures. The results demonstrate that our method consistently outperforms the previous methods over the tasks.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6IECSL66\\Yoo_Kweon_2019_Learning Loss for Active Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\K2YFNC9B\\1905.html}
}

@unpublished{yoshikawaPopulationbasedNovoMolecule2018,
  title = {Population-Based de Novo Molecule Generation, Using Grammatical Evolution},
  author = {Yoshikawa, Naruki and Terayama, Kei and Honma, Teruki and Oono, Kenta and Tsuda, Koji},
  date = {2018-04-06},
  eprint = {1804.02134},
  eprinttype = {arxiv},
  eprintclass = {physics, q-bio},
  url = {http://arxiv.org/abs/1804.02134},
  urldate = {2018-09-24},
  abstract = {Automatic design with machine learning and molecular simulations has shown a remarkable ability to generate new and promising drug candidates. Current models, however, still have problems in simulation concurrency and molecular diversity. Most methods generate one molecule at a time and do not allow multiple simulators to run simultaneously. Additionally, better molecular diversity could boost the success rate in the subsequent drug discovery process. We propose a new population-based approach using grammatical evolution named ChemGE. In our method, a large population of molecules are updated concurrently and evaluated by multiple simulators in parallel. In docking experiments with thymidine kinase, ChemGE succeeded in generating hundreds of high-affinity molecules whose diversity is better than that of known inding molecules in DUD-E.},
  keywords = {Physics - Chemical Physics,Quantitative Biology - Biomolecules},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\3YA3ELLR\\Yoshikawa et al_2018_Population-based de novo molecule generation, using grammatical evolution3.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\79FX2HKI\\Yoshikawa et al. - 2018 - Population-based de novo molecule generation, usin.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\M7VR78YN\\Yoshikawa et al_2018_Population-based de novo molecule generation, using grammatical evolution2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\XQWZLJYK\\Yoshikawa et al_2018_Population-based de novo molecule generation, using grammatical evolution.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\YT3R9G5F\\Yoshikawa et al. - 2018 - Population-based de novo molecule generation, usin.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7QS84L6U\\1804.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\JAXREIR9\\1804.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\TTJH72I4\\1804.html}
}

@unpublished{youGraphRNNGeneratingRealistic2018,
  title = {{{GraphRNN}}: {{Generating Realistic Graphs}} with {{Deep Auto-regressive Models}}},
  shorttitle = {{{GraphRNN}}},
  author = {You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William L. and Leskovec, Jure},
  date = {2018-02-23},
  eprint = {1802.08773},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1802.08773},
  urldate = {2019-05-06},
  abstract = {Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. In order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,I.2.6},
  file = {C:\Users\USEBPERP\Zotero\storage\M4SBCF78\You et al_2018_GraphRNN.pdf}
}

@unpublished{youLargeBatchTraining2017,
  title = {Large {{Batch Training}} of {{Convolutional Networks}}},
  author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
  date = {2017-09-13},
  eprint = {1708.03888},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1708.03888},
  urldate = {2020-02-03},
  abstract = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\VJD9SYMC\\You et al_2017_Large Batch Training of Convolutional Networks.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7SL77HPX\\1708.html}
}

@unpublished{youLimitBatchSize2020,
  title = {The {{Limit}} of the {{Batch Size}}},
  author = {You, Yang and Wang, Yuhui and Zhang, Huan and Zhang, Zhao and Demmel, James and Hsieh, Cho-Jui},
  date = {2020-06-15},
  eprint = {2006.08517},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.08517},
  urldate = {2020-06-19},
  abstract = {Large-batch training is an efficient approach for current distributed deep learning systems. It has enabled researchers to reduce the ImageNet/ResNet-50 training from 29 hours to around 1 minute. In this paper, we focus on studying the limit of the batch size. We think it may provide a guidance to AI supercomputer and algorithm designers. We provide detailed numerical optimization instructions for step-by-step comparison. Moreover, it is important to understand the generalization and optimization performance of huge batch training. Hoffer et al. introduced "ultra-slow diffusion" theory to large-batch training. However, our experiments show contradictory results with the conclusion of Hoffer et al. We provide comprehensive experimental results and detailed analysis to study the limitations of batch size scaling and "ultra-slow diffusion" theory. For the first time we scale the batch size on ImageNet to at least a magnitude larger than all previous work, and provide detailed studies on the performance of many state-of-the-art optimization schemes under this setting. We propose an optimization recipe that is able to improve the top-1 test accuracy by 18\% compared to the baseline.},
  version = {1},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\NSJMP956\\You et al_2020_The Limit of the Batch Size.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\B6HQAFFH\\2006.html}
}

@online{yueRandomPonderingsBrief2015,
  title = {Random {{Ponderings}}: {{A Brief Overview}} of {{Deep Learning}}},
  shorttitle = {Random {{Ponderings}}},
  author = {Yue, Yisong},
  date = {2015-01-13},
  url = {http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html},
  urldate = {2019-03-08},
  organization = {Random Ponderings},
  keywords = {computer science,machine learning,science / technology},
  file = {C:\Users\USEBPERP\Zotero\storage\6U8AAUEQ\a-brief-overview-of-deep-learning.html}
}

@online{yueTS2VecUniversalRepresentation2022,
  title = {{{TS2Vec}}: {{Towards Universal Representation}} of {{Time Series}}},
  shorttitle = {{{TS2Vec}}},
  author = {Yue, Zhihan and Wang, Yujing and Duan, Juanyong and Yang, Tianmeng and Huang, Congrui and Tong, Yunhai and Xu, Bixiong},
  date = {2022-02-03},
  eprint = {2106.10466},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.10466},
  urldate = {2022-08-16},
  abstract = {This paper presents TS2Vec, a universal framework for learning representations of time series in an arbitrary semantic level. Unlike existing methods, TS2Vec performs contrastive learning in a hierarchical way over augmented context views, which enables a robust contextual representation for each timestamp. Furthermore, to obtain the representation of an arbitrary sub-sequence in the time series, we can apply a simple aggregation over the representations of corresponding timestamps. We conduct extensive experiments on time series classification tasks to evaluate the quality of time series representations. As a result, TS2Vec achieves significant improvement over existing SOTAs of unsupervised time series representation on 125 UCR datasets and 29 UEA datasets. The learned timestamp-level representations also achieve superior results in time series forecasting and anomaly detection tasks. A linear regression trained on top of the learned representations outperforms previous SOTAs of time series forecasting. Furthermore, we present a simple way to apply the learned representations for unsupervised anomaly detection, which establishes SOTA results in the literature. The source code is publicly available at https://github.com/yuezhihan/ts2vec.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DFHLRFWU\\Yue et al. - 2022 - TS2Vec Towards Universal Representation of Time S.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\RQXYAUHV\\2106.html}
}

@online{zandercuttWeReOptimizing2019,
  title = {We’re {{Optimizing Ourselves}} to {{Death}}},
  author = {Zandercutt},
  date = {2019-02-18T23:40:12+00:00},
  url = {https://zandercutt.com/2019/02/18/were-optimizing-ourselves-to-death/},
  urldate = {2020-11-23},
  abstract = {Burnout is the inevitable result of our endlessly accelerating pace of~life Illustration: Jutta Kuss/Getty Images Author’s Note: I’ve recently partnered with Project DigInThere, an onli…},
  langid = {english},
  organization = {Zander Nethercutt},
  file = {C:\Users\USEBPERP\Zotero\storage\YRGI3EXS\were-optimizing-ourselves-to-death.html}
}

@unpublished{zeilerADADELTAAdaptiveLearning2012,
  title = {{{ADADELTA}}: {{An Adaptive Learning Rate Method}}},
  shorttitle = {{{ADADELTA}}},
  author = {Zeiler, Matthew D.},
  date = {2012-12-22},
  eprint = {1212.5701},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1212.5701},
  urldate = {2018-05-20},
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  keywords = {Computer Science - Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\3K4K8V2J\\Zeiler_2012_ADADELTA.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\RZATM2GJ\\1212.html}
}

@online{zengAreTransformersEffective2022,
  title = {Are {{Transformers Effective}} for {{Time Series Forecasting}}?},
  author = {Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},
  date = {2022-08-17},
  eprint = {2205.13504},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.13504},
  urldate = {2023-02-09},
  abstract = {Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the \textbackslash emph\{permutation-invariant\} self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future. Code is available at: \textbackslash url\{https://github.com/cure-lab/LTSF-Linear\}.},
  pubstate = {preprint},
  version = {3},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\4RYJDVAT\\Zeng et al. - 2022 - Are Transformers Effective for Time Series Forecas.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\U5GTKJC7\\2205.html}
}

@article{zhangDiveDeepLearning,
  title = {Dive into {{Deep Learning}}},
  author = {Zhang, Aston and Lipton, Zack C and Li, Mu and Smola, Alex J},
  pages = {658},
  file = {C:\Users\USEBPERP\Zotero\storage\HVCVYYUL\Zhang et al. - Dive into Deep Learning.pdf}
}

@online{zhangFalsehoodsThatML2022,
  title = {Falsehoods That {{ML}} Researchers Believe about {{OOD}} Detection},
  author = {Zhang, Andi and Wischik, Damon},
  date = {2022-11-01},
  eprint = {2210.12767},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2210.12767},
  url = {http://arxiv.org/abs/2210.12767},
  urldate = {2023-01-06},
  abstract = {An intuitive way to detect out-of-distribution (OOD) data is via the density function of a fitted probabilistic generative model: points with low density may be classed as OOD. But this approach has been found to fail, in deep learning settings. In this paper, we list some falsehoods that machine learning researchers believe about density-based OOD detection. Many recent works have proposed likelihood-ratio-based methods to `fix' the problem. We propose a framework, the OOD proxy framework, to unify these methods, and we argue that likelihood ratio is a principled method for OOD detection and not a mere `fix'. Finally, we discuss the relationship between domain discrimination and semantics.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\VEMXDT4T\\Zhang and Wischik - 2022 - Falsehoods that ML researchers believe about OOD d.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\N3D39UD2\\2210.html}
}

@unpublished{zhangFixupInitializationResidual2019,
  title = {Fixup {{Initialization}}: {{Residual Learning Without Normalization}}},
  shorttitle = {Fixup {{Initialization}}},
  author = {Zhang, Hongyi and Dauphin, Yann N. and Ma, Tengyu},
  date = {2019-01-27},
  eprint = {1901.09321},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1901.09321},
  urldate = {2019-05-27},
  abstract = {Normalization layers are a staple in state-of-the-art deep neural network architectures. They are widely believed to stabilize training, enable higher learning rate, accelerate convergence and improve generalization, though the reason for their effectiveness is still an active research topic. In this work, we challenge the commonly-held beliefs by showing that none of the perceived benefits is unique to normalization. Specifically, we propose fixed-update initialization (Fixup), an initialization motivated by solving the exploding and vanishing gradient problem at the beginning of training via properly rescaling a standard initialization. We find training residual networks with Fixup to be as stable as training with normalization -- even for networks with 10,000 layers. Furthermore, with proper regularization, Fixup enables residual networks without normalization to achieve state-of-the-art performance in image classification and machine translation.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\S35MVTGV\\Zhang et al_2019_Fixup Initialization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\RGNGGS7F\\1901.html}
}

@unpublished{zhangMixupEmpiricalRisk2017,
  title = {Mixup: {{Beyond Empirical Risk Minimization}}},
  shorttitle = {Mixup},
  author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
  date = {2017-10-25},
  eprint = {1710.09412},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1710.09412},
  urldate = {2019-05-27},
  abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\QPWPS92L\\Zhang et al_2017_mixup2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SGNVZL3T\\Zhang et al_2017_mixup.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\EZIWW423\\1710.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZHLIZPX2\\1710.html}
}

@inproceedings{zhangMolecularGraphGeneration2019,
  ids = {zhangMolecularGraphGeneration2019a},
  title = {Molecular {{Graph Generation}} with {{Deep Reinforced Multitask Network}} and {{Adversarial Imitation Learning}}},
  author = {Zhang, Chenrui and Lyu, Xiaoqing and Huang, Yifeng and Tang, Zhi and Liu, Zhenming},
  date = {2019-11-18},
  doi = {10.1109/BIBM47256.2019.8983277},
  abstract = {Molecular graph generation aims to design molecules with desired chemical and biological properties, which is promising in drug discovery. State-of-the-art methods in this field typically combine deep reinforcement models with adversarial training, for tackling the discreteness and vastness of molecule space. However, the reinforced rewards in molecule generation are delayed and sparse while adversarial training suffers mode collapse issue. Moreover, they optimize multiple properties with a weighted linear combination manner, in which the models can get distracted by potentially conflicting objectives. To tackle the above challenges, we propose a Deep Reinforced architecture with Adversarial Imitation learning and Multitask learning (DRAIM).  First, a deep Q-learning architecture is designed, which contains reinforced module and adversarial imitation learning (AIL) module. The agent of reinforced module generates discrete molecular graphs via deep Q-learning, where the trajectories of high rewards are cached. The AIL module directly extracts policy from past good trajectories by adversarial imitation learning, in which the discriminator delivers behavior distribution signals to the agent as dense rewards, alleviating the delayed and sparse reward issue in reinforcement learning. Second, we propose to realize multi-goal molecule generation as a multitask learning process, where different property optimizations are treated as different tasks to be trained jointly. Domain relatedness is fully utilized via graph representations sharing, while objective competition is mitigated with separated Q-value estimation. Extensive experiments demonstrate the effectiveness of our proposal in molecule generation and optimization.},
  eventtitle = {{{IEEE International Conference}} on {{Bioinformatics}} and {{Biomedicine}}},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\J857CHD7\\Zhang et al_2019_Molecular Graph Generation with Deep Reinforced Multitask Network and.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\QCWQKJQA\\Zhang et al_2019_Molecular Graph Generation with Deep Reinforced Multitask Network and.pdf}
}

@inproceedings{zhangSeq3seqFingerprintEndtoend2018,
  title = {Seq3seq {{Fingerprint}}: {{Towards End-to-end Semi-supervised Deep Drug Discovery}}},
  shorttitle = {Seq3seq {{Fingerprint}}},
  booktitle = {Proceedings of the 2018 {{ACM International Conference}} on {{Bioinformatics}}, {{Computational Biology}}, and {{Health Informatics}}  - {{BCB}} '18},
  author = {Zhang, Xiaoyu and Wang, Sheng and Zhu, Feiyun and Xu, Zheng and Wang, Yuhong and Huang, Junzhou},
  date = {2018},
  pages = {404--413},
  publisher = {ACM Press},
  location = {Washington, DC, USA},
  doi = {10.1145/3233547.3233548},
  url = {http://dl.acm.org/citation.cfm?doid=3233547.3233548},
  urldate = {2018-09-14},
  abstract = {Observing the recent progress in Deep Learning, the employment of AI is surging to accelerate drug discovery and cut R\&D costs in the last few years. However, the success of deep learning is attributed to large-scale clean high-quality labeled data, which is generally unavailable in drug discovery practices.},
  eventtitle = {The 2018 {{ACM International Conference}}},
  isbn = {978-1-4503-5794-4},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\7TX2RU7N\Zhang et al_2018_Seq3seq Fingerprint.pdf}
}

@article{zhangTheoryDeepLearning,
  title = {Theory of {{Deep Learning III}}: {{Generalization Properties}} of {{SGD}}},
  author = {Zhang, Chiyuan and Liao, Qianli and Rakhlin, Alexander and Miranda, Brando and Golowich, Noah and Poggio, Tomaso},
  pages = {38},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\FV4YT9HY\Zhang et al. - Theory of Deep Learning III Generalization Proper.pdf}
}

@unpublished{zhangUnderstandingDeepLearning2016,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  date = {2016-11-10},
  eprint = {1611.03530},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1611.03530},
  urldate = {2018-09-04},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\5D69A9BW\\Zhang et al_2016_Understanding deep learning requires rethinking generalization2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JBRW248Q\\Zhang et al_2016_Understanding deep learning requires rethinking generalization.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\S2EHNL8Y\\1611.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\X43Q6ZR6\\1611.html}
}

@unpublished{zhangWhichAlgorithmicChoices2019,
  ids = {zhang2019which,zhangWhichAlgorithmicChoices2019a},
  title = {Which {{Algorithmic Choices Matter}} at {{Which Batch Sizes}}? {{Insights From}} a {{Noisy Quadratic Model}}},
  shorttitle = {Which {{Algorithmic Choices Matter}} at {{Which Batch Sizes}}?},
  author = {Zhang, Guodong and Li, Lala and Nado, Zachary and Martens, James and Sachdeva, Sushant and Dahl, George E. and Shallue, Christopher J. and Grosse, Roger},
  date = {2019-07-09},
  eprint = {1907.04164},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1907.04164},
  urldate = {2019-09-02},
  abstract = {Increasing the batch size is a popular way to speed up neural network training, but beyond some critical batch size, larger batch sizes yield diminishing returns. In this work, we study how the critical batch size changes based on properties of the optimization algorithm, including acceleration and preconditioning, through two different lenses: large scale experiments, and analysis of a simple noisy quadratic model (NQM). We experimentally demonstrate that optimization algorithms that employ preconditioning, specifically Adam and K-FAC, result in much larger critical batch sizes than stochastic gradient descent with momentum. We also demonstrate that the NQM captures many of the essential features of real neural network training, despite being drastically simpler to work with. The NQM predicts our results with preconditioned optimizers, previous results with accelerated gradient descent, and other results around optimal learning rates and large batch training, making it a useful tool to generate testable predictions about neural network optimization.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\6MU5R54J\\Zhang et al. - 2019 - Which Algorithmic Choices Matter at Which Batch Si.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\8KBTBFQF\\Zhang et al. - 2019 - Which Algorithmic Choices Matter at Which Batch Si.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\GRURUZC6\\Zhang et al_2019_Which Algorithmic Choices Matter at Which Batch Sizes.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\I2QR9LGG\\Zhang et al_2019_Which Algorithmic Choices Matter at Which Batch Sizes.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\3KSP73QB\\1907.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\4RSR9WHF\\1907.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\5RSUJREP\\1907.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\8469N6WQ\\1907.html}
}

@online{zhaoMultivariateTimeseriesAnomaly2020,
  title = {Multivariate {{Time-series Anomaly Detection}} via {{Graph Attention Network}}},
  author = {Zhao, Hang and Wang, Yujing and Duan, Juanyong and Huang, Congrui and Cao, Defu and Tong, Yunhai and Xu, Bixiong and Bai, Jing and Tong, Jie and Zhang, Qi},
  date = {2020-09-04},
  eprint = {2009.02040},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2009.02040},
  urldate = {2023-01-27},
  abstract = {Anomaly detection on multivariate time-series is of great importance in both data mining research and industrial applications. Recent approaches have achieved significant progress in this topic, but there is remaining limitations. One major limitation is that they do not capture the relationships between different time-series explicitly, resulting in inevitable false alarms. In this paper, we propose a novel self-supervised framework for multivariate time-series anomaly detection to address this issue. Our framework considers each univariate time-series as an individual feature and includes two graph attention layers in parallel to learn the complex dependencies of multivariate time-series in both temporal and feature dimensions. In addition, our approach jointly optimizes a forecasting-based model and a reconstruction-based model, obtaining better time-series representations through a combination of single-timestamp prediction and reconstruction of the entire time-series. We demonstrate the efficacy of our model through extensive experiments. The proposed method outperforms other state-of-the-art models on three real-world datasets. Further analysis shows that our method has good interpretability and is useful for anomaly diagnosis.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\USEBPERP\Zotero\storage\YFYYESEX\Zhao et al. - 2020 - Multivariate Time-series Anomaly Detection via Gra.pdf}
}

@article{zhaoPyODPythonToolbox2019,
  title = {{{PyOD}}: {{A Python Toolbox}} for {{Scalable Outlier Detection}}},
  shorttitle = {{{PyOD}}},
  author = {Zhao, Yue and Nasrullah, Zain and Li, Zheng},
  date = {2019},
  journaltitle = {Journal of Machine Learning Research},
  volume = {20},
  number = {96},
  pages = {1--7},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v20/19-011.html},
  urldate = {2023-04-11},
  abstract = {PyOD is an open-source Python toolbox for performing scalable outlier detection on multivariate data. Uniquely, it provides access to a wide range of outlier detection algorithms, including established outlier ensembles and more recent neural network-based approaches, under a single, well-documented API designed for use by both practitioners and researchers. With robustness and scalability in mind, best practices such as unit testing, continuous integration, code coverage, maintainability checks, interactive examples and parallelization are emphasized as core components in the toolbox's development. PyOD is compatible with both Python 2 and 3 and can be installed through Python Package Index (PyPI) or https://github.com/yzhao062/pyod.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\YEKN9TGD\\Zhao et al. - 2019 - PyOD A Python Toolbox for Scalable Outlier Detect.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\ZKYJSZ2Y\\pyod.html}
}

@article{zhavoronkovArtificialIntelligenceDrug2018,
  title = {Artificial {{Intelligence}} for {{Drug Discovery}}, {{Biomarker Development}}, and {{Generation}} of {{Novel Chemistry}}},
  author = {Zhavoronkov, Alex},
  date = {2018-10-01},
  journaltitle = {Mol. Pharmaceutics},
  volume = {15},
  number = {10},
  pages = {4311--4313},
  publisher = {American Chemical Society},
  issn = {1543-8384},
  doi = {10.1021/acs.molpharmaceut.8b00930},
  url = {https://doi.org/10.1021/acs.molpharmaceut.8b00930},
  urldate = {2020-03-17},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\7V4CIYVZ\\Zhavoronkov_2018_Artificial Intelligence for Drug Discovery, Biomarker Development, and.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\Q6ULWGN5\\Zhavoronkov_2018_Artificial Intelligence for Drug Discovery, Biomarker Development, and2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\SLKJ8DF3\\acs.molpharmaceut.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\TSBZXNCX\\acs.molpharmaceut.html}
}

@article{zhavoronkovDeepLearningEnables2019,
  title = {Deep Learning Enables Rapid Identification of Potent {{DDR1}} Kinase Inhibitors},
  author = {Zhavoronkov, Alex and Ivanenkov, Yan A. and Aliper, Alex and Veselov, Mark S. and Aladinskiy, Vladimir A. and Aladinskaya, Anastasiya V. and Terentiev, Victor A. and Polykovskiy, Daniil A. and Kuznetsov, Maksim D. and Asadulaev, Arip and Volkov, Yury and Zholus, Artem and Shayakhmetov, Rim R. and Zhebrak, Alexander and Minaeva, Lidiya I. and Zagribelnyy, Bogdan A. and Lee, Lennart H. and Soll, Richard and Madge, David and Xing, Li and Guo, Tao and Aspuru-Guzik, Alán},
  date = {2019-09},
  journaltitle = {Nat Biotechnol},
  volume = {37},
  number = {9},
  pages = {1038--1040},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/s41587-019-0224-x},
  url = {http://www.nature.com/articles/s41587-019-0224-x},
  urldate = {2020-03-13},
  langid = {english},
  file = {C:\Users\USEBPERP\Zotero\storage\Z6BJP9UY\Zhavoronkov et al. - 2019 - Deep learning enables rapid identification of pote.pdf}
}

@article{zhengPredictingRetrosyntheticReactions2020,
  ids = {zheng2020predictinga},
  title = {Predicting {{Retrosynthetic Reactions Using Self-Corrected Transformer Neural Networks}}},
  author = {Zheng, Shuangjia and Rao, Jiahua and Zhang, Zhongyue and Xu, Jun and Yang, Yuedong},
  date = {2020-01-27},
  journaltitle = {J. Chem. Inf. Model.},
  volume = {60},
  number = {1},
  pages = {47--55},
  publisher = {American Chemical Society},
  issn = {1549-9596},
  doi = {10.1021/acs.jcim.9b00949},
  url = {https://doi.org/10.1021/acs.jcim.9b00949},
  urldate = {2021-05-19},
  abstract = {Synthesis planning is the process of recursively decomposing target molecules into available precursors. Computer-aided retrosynthesis can potentially assist chemists in designing synthetic routes; however, at present, it is cumbersome and cannot provide satisfactory results. In this study, we have developed a template-free self-corrected retrosynthesis predictor (SCROP) to predict retrosynthesis using transformer neural networks. In the method, the retrosynthesis planning was converted to a machine translation problem from the products to molecular linear notations of the reactants. By coupling with a neural network-based syntax corrector, our method achieved an accuracy of 59.0\% on a standard benchmark data set, which outperformed other deep learning methods by {$>$}21\% and template-based methods by {$>$}6\%. More importantly, our method was 1.7 times more accurate than other state-of-the-art methods for compounds not appearing in the training set.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\EBVWNULE\\Zheng et al. - 2020 - Predicting Retrosynthetic Reactions Using Self-Cor.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WQUUWPXJ\\Zheng et al. - 2020 - Predicting Retrosynthetic Reactions Using Self-Cor.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\HQ753XLX\\acs.jcim.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\JHYH5JLG\\acs.jcim.html}
}

@article{zhengQBMGQuasibiogenicMolecule2019,
  title = {{{QBMG}}: Quasi-Biogenic Molecule Generator with Deep Recurrent Neural Network},
  shorttitle = {{{QBMG}}},
  author = {Zheng, Shuangjia and Yan, Xin and Gu, Qiong and Yang, Yuedong and Du, Yunfei and Lu, Yutong and Xu, Jun},
  date = {2019-01-17},
  journaltitle = {Journal of Cheminformatics},
  volume = {11},
  number = {1},
  pages = {5},
  issn = {1758-2946},
  doi = {10.1186/s13321-019-0328-9},
  url = {https://doi.org/10.1186/s13321-019-0328-9},
  urldate = {2019-01-22},
  abstract = {Biogenic compounds are important materials for drug discovery and chemical biology. In this work, we report a quasi-biogenic molecule generator (QBMG) to compose virtual quasi-biogenic compound libraries by means of gated recurrent unit recurrent neural networks. The library includes stereo-chemical properties, which are crucial features of natural products. QMBG can reproduce the property distribution of the underlying training set, while being able to generate realistic, novel molecules outside of the training set. Furthermore, these compounds are associated with known bioactivities. A focused compound library based on a given chemotype/scaffold can also be generated by this approach combining transfer learning technology. This approach can be used to generate virtual compound libraries for pharmaceutical lead identification and optimization.},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\DVVEBD2P\\Zheng et al_2019_QBMG.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\PITUSL73\\s13321-019-0328-9.html}
}

@unpublished{zhouOptimizationMoleculesDeep2018,
  title = {Optimization of {{Molecules}} via {{Deep Reinforcement Learning}}},
  author = {Zhou, Zhenpeng and Kearnes, Steven and Li, Li and Zare, Richard N. and Riley, Patrick},
  date = {2018-10-19},
  eprint = {1810.08678},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.08678},
  urldate = {2019-05-06},
  abstract = {We present a framework, which we call Molecule Deep \$Q\$-Networks (MolDQN), for molecule optimization by combining domain knowledge of chemistry and state-of-the-art reinforcement learning techniques (double \$Q\$-learning and randomized value functions). We directly define modifications on molecules, thereby ensuring 100\textbackslash\% chemical validity. Further, we operate without pre-training on any dataset to avoid possible bias from the choice of that set. Inspired by problems faced during medicinal chemistry lead optimization, we extend our model with multi-objective reinforcement learning, which maximizes drug-likeness while maintaining similarity to the original molecule. We further show the path through chemical space to achieve optimization for a molecule to understand how the model works.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\72ZC8JMF\\Zhou et al_2018_Optimization of Molecules via Deep Reinforcement Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\9QA9CATY\\Zhou et al_2018_Optimization of Molecules via Deep Reinforcement Learning2.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\EM9SBSC3\\Zhou et al_2019_Optimization of Molecules via Deep Reinforcement Learning.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\937DQSFM\\1810.html;C\:\\Users\\USEBPERP\\Zotero\\storage\\LQBQPBM4\\1810.html}
}

@article{zhuLearningLabeledUnlabeled2002,
  title = {Learning from Labeled and Unlabeled Data with Label Propagation},
  author = {Zhu, Xiaojin and Ghahramani, Zoubin},
  date = {2002},
  file = {C:\Users\USEBPERP\Zotero\storage\Z8PHHZU4\Zhu_Ghahramani_2002_Learning from labeled and unlabeled data with label propagation.pdf}
}

@unpublished{zieglerLatentNormalizingFlows2019,
  title = {Latent {{Normalizing Flows}} for {{Discrete Sequences}}},
  author = {Ziegler, Zachary M. and Rush, Alexander M.},
  date = {2019-06-04},
  eprint = {1901.10548},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1901.10548},
  urldate = {2019-12-19},
  abstract = {Normalizing flows are a powerful class of generative models for continuous random variables, showing both strong model flexibility and the potential for non-autoregressive generation. These benefits are also desired when modeling discrete random variables such as text, but directly applying normalizing flows to discrete sequences poses significant additional challenges. We propose a VAE-based generative model which jointly learns a normalizing flow-based distribution in the latent space and a stochastic mapping to an observed discrete space. In this setting, we find that it is crucial for the flow-based distribution to be highly multimodal. To capture this property, we propose several normalizing flow architectures to maximize model flexibility. Experiments consider common discrete sequence tasks of character-level language modeling and polyphonic music generation. Our results indicate that an autoregressive flow-based model can match the performance of a comparable autoregressive baseline, and a non-autoregressive flow-based model can improve generation speed with a penalty to performance.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\3N7SYAPT\\Ziegler_Rush_2019_Latent Normalizing Flows for Discrete Sequences.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\JRZH5Y2K\\Ziegler and Rush - Latent Normalizing Flows for Discrete Sequences.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\7GEH3H7V\\1901.html}
}

@online{zohrevandShouldRaiseRed2020,
  title = {Should {{I Raise The Red Flag}}? {{A}} Comprehensive Survey of Anomaly Scoring Methods toward Mitigating False Alarms},
  shorttitle = {Should {{I Raise The Red Flag}}?},
  author = {Zohrevand, Zahra and Glässer, Uwe},
  date = {2020-08-30},
  eprint = {1904.06646},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1904.06646},
  urldate = {2023-02-20},
  abstract = {Nowadays, advanced intrusion detection systems (IDSs) rely on a combination of anomaly detection and signature-based methods. An IDS gathers observations, analyzes behavioral patterns, and reports suspicious events for further investigation. A notorious issue anomaly detection systems (ADSs) and IDSs face is the possibility of high false alarms, which even state-of-the-art systems have not overcome. This is especially a problem with large and complex systems. The number of non-critical alarms can easily overwhelm administrators and increase the likelihood of ignoring future alerts. Mitigation strategies thus aim to avoid raising `too many' false alarms without missing potentially dangerous situations. There are two major categories of false alarm-mitigation strategies: (1) methods that are customized to enhance the quality of anomaly scoring; (2) approaches acting as filtering methods in contexts that aim to decrease false alarm rates. These methods have been widely utilized by many scholars. Herein, we review and compare the existing techniques for false alarm mitigation in ADSs. We also examine the use of promising techniques in signature-based IDS and other relevant contexts, such as commercial security information and event management tools, which are promising for ADSs. We conclude by highlighting promising directions for future research.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\BNY8AGJ7\\Zohrevand and Glässer - 2020 - Should I Raise The Red Flag A comprehensive surve.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\88FJZJED\\1904.html}
}

@article{zotero-693,
  type = {article}
}

@online{ZoteroDownloads,
  title = {Zotero | {{Downloads}}},
  url = {https://www.zotero.org/download/},
  urldate = {2024-04-03},
  file = {C:\Users\USEBPERP\Zotero\storage\SN9W6RTB\download.html}
}

@online{ZoteroYourPersonal,
  title = {Zotero | {{Your}} Personal Research Assistant},
  url = {https://www.zotero.org/},
  urldate = {2020-02-20},
  file = {C:\Users\USEBPERP\Zotero\storage\74HVUC98\www.zotero.org.html}
}

@unpublished{zouObjectDetection202019,
  title = {Object {{Detection}} in 20 {{Years}}: {{A Survey}}},
  shorttitle = {Object {{Detection}} in 20 {{Years}}},
  author = {Zou, Zhengxia and Shi, Zhenwei and Guo, Yuhong and Ye, Jieping},
  date = {2019-05-13},
  eprint = {1905.05055},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1905.05055},
  urldate = {2019-09-05},
  abstract = {Object detection, as of one the most fundamental and challenging problems in computer vision, has received great attention in recent years. Its development in the past two decades can be regarded as an epitome of computer vision history. If we think of today's object detection as a technical aesthetics under the power of deep learning, then turning back the clock 20 years we would witness the wisdom of cold weapon era. This paper extensively reviews 400+ papers of object detection in the light of its technical evolution, spanning over a quarter-century's time (from the 1990s to 2019). A number of topics have been covered in this paper, including the milestone detectors in history, detection datasets, metrics, fundamental building blocks of the detection system, speed up techniques, and the recent state of the art detection methods. This paper also reviews some important detection applications, such as pedestrian detection, face detection, text detection, etc, and makes an in-deep analysis of their challenges as well as technical improvements in recent years.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\USEBPERP\\Zotero\\storage\\3CQTGIQM\\Zou et al_2019_Object Detection in 20 Years.pdf;C\:\\Users\\USEBPERP\\Zotero\\storage\\WMJVYTIJ\\1905.html}
}
